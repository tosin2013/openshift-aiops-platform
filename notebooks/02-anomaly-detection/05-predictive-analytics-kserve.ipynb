{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics Model for KServe\n",
    "\n",
    "## Overview\n",
    "This notebook trains and saves a **Predictive Analytics model** in KServe-compatible format for the `predictive-analytics` InferenceService.\n",
    "\n",
    "**Model Type**: Time series forecasting with Random Forest  \n",
    "**Purpose**: Predict future resource usage (CPU, memory, disk, network)  \n",
    "**Deployment**: KServe InferenceService with sklearn runtime\n",
    "\n",
    "## KServe Integration (Issue #13 Fix)\n",
    "\n",
    "This notebook implements the fix for [Issue #13](https://github.com/tosin2013/openshift-aiops-platform/issues/13) where models were registering as `\"model\"` instead of `\"predictive-analytics\"`.\n",
    "\n",
    "### Problem Solved\n",
    "- **Before**: Models saved to `/mnt/models/cpu_usage_step_0_model.pkl` (flat structure)\n",
    "- **After**: Models saved to `/mnt/models/predictive-analytics/model.pkl` (KServe structure)\n",
    "- **Result**: Model registers correctly as `\"predictive-analytics\"` ‚úÖ\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Notebook Training ‚Üí /mnt/models/predictive-analytics/model.pkl\n",
    "                    ‚Üì\n",
    "KServe InferenceService (storageUri: pvc://model-storage-pvc/predictive-analytics)\n",
    "                    ‚Üì\n",
    "Model registered as: \"predictive-analytics\"\n",
    "                    ‚Üì\n",
    "Endpoint: /v1/models/predictive-analytics:predict\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "- Model storage PVC mounted at `/mnt/models`\n",
    "- Python environment with sklearn, pandas, numpy\n",
    "- Access to `src/models/predictive_analytics.py` module\n",
    "\n",
    "## What This Notebook Does\n",
    "1. ‚úÖ Imports the `PredictiveAnalytics` module\n",
    "2. ‚úÖ Generates synthetic time series training data\n",
    "3. ‚úÖ Trains multi-metric forecasting models (CPU, memory, disk, network)\n",
    "4. ‚úÖ Saves in KServe-compatible format: `/mnt/models/predictive-analytics/model.pkl`\n",
    "5. ‚úÖ Validates the model works correctly\n",
    "6. ‚úÖ Tests prediction endpoint format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section\n",
    "\n",
    "### Import Libraries and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path for src/models module - works from any directory\n",
    "def find_models_path():\n",
    "    \"\"\"Find src/models path regardless of current working directory\"\"\"\n",
    "    possible_paths = [\n",
    "        Path(__file__).parent.parent.parent / 'src' / 'models' if '__file__' in dir() else None,\n",
    "        Path.cwd().parent.parent / 'src' / 'models',\n",
    "        Path('/workspace/repo/src/models'),\n",
    "        Path('/opt/app-root/src/openshift-aiops-platform/src/models'),\n",
    "    ]\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists() and (p / 'predictive_analytics.py').exists():\n",
    "            return str(p)\n",
    "    # Try relative path search\n",
    "    current = Path.cwd()\n",
    "    for _ in range(5):\n",
    "        models_path = current / 'src' / 'models'\n",
    "        if models_path.exists() and (models_path / 'predictive_analytics.py').exists():\n",
    "            return str(models_path)\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "models_path = find_models_path()\n",
    "if models_path:\n",
    "    sys.path.insert(0, models_path)\n",
    "    print(f\"‚úÖ Models path found: {models_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Models path not found - using fallback implementation\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PredictiveAnalytics module\n",
    "try:\n",
    "    from predictive_analytics import PredictiveAnalytics, generate_sample_timeseries_data\n",
    "    print(\"‚úÖ PredictiveAnalytics module imported successfully\")\n",
    "    USING_MODULE = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import PredictiveAnalytics module: {e}\")\n",
    "    print(\"   Please ensure src/models/predictive_analytics.py is available\")\n",
    "    USING_MODULE = False\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model Storage Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure storage paths\n",
    "# Use /mnt/models for persistent storage (model-storage-pvc)\n",
    "# Fallback to local for development outside cluster\n",
    "MODELS_DIR = Path('/mnt/models') if Path('/mnt/models').exists() else Path('/opt/app-root/src/models')\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model name must match InferenceService name\n",
    "MODEL_NAME = 'predictive-analytics'\n",
    "MODEL_DIR = MODELS_DIR / MODEL_NAME  # Will be created by save_models()\n",
    "\n",
    "print(f\"üìÅ Model Storage Configuration:\")\n",
    "print(f\"   Base directory: {MODELS_DIR}\")\n",
    "print(f\"   Model name: {MODEL_NAME}\")\n",
    "print(f\"   Expected KServe path: {MODEL_DIR}/model.pkl\")\n",
    "print(f\"   PVC available: {'‚úÖ Yes' if MODELS_DIR == Path('/mnt/models') else '‚ö†Ô∏è No (using local)'}\")\n",
    "\n",
    "if not USING_MODULE:\n",
    "    print(\"\\n‚ùå Cannot proceed without PredictiveAnalytics module\")\n",
    "    raise ImportError(\"PredictiveAnalytics module required for this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# Data Source Configuration (ADR-050, ADR-052)\n",
    "# ====================\n",
    "import requests\n",
    "\n",
    "DATA_SOURCE = os.getenv('DATA_SOURCE', 'synthetic')  # synthetic|prometheus|hybrid\n",
    "PROMETHEUS_URL = os.getenv('PROMETHEUS_URL', 'http://prometheus-k8s.openshift-monitoring.svc:9090')\n",
    "TRAINING_DAYS = int(os.getenv('TRAINING_DAYS', '30'))  # 30-day lookback for predictive analytics\n",
    "TRAINING_HOURS = int(os.getenv('TRAINING_HOURS', str(TRAINING_DAYS * 24)))\n",
    "PROMETHEUS_AVAILABLE = False\n",
    "\n",
    "# Check Prometheus availability\n",
    "if DATA_SOURCE in ['prometheus', 'hybrid']:\n",
    "    try:\n",
    "        response = requests.get(f\"{PROMETHEUS_URL}/api/v1/status/config\", timeout=5)\n",
    "        PROMETHEUS_AVAILABLE = response.status_code == 200\n",
    "        print(f\"‚úÖ Prometheus available at {PROMETHEUS_URL}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Prometheus not available: {e}\")\n",
    "        print(f\"   Falling back to synthetic data\")\n",
    "        DATA_SOURCE = 'synthetic'\n",
    "\n",
    "print(f\"\\nüìä Data Source Configuration:\")\n",
    "print(f\"   Mode: {DATA_SOURCE}\")\n",
    "print(f\"   Training hours: {TRAINING_HOURS}h ({TRAINING_HOURS / 24:.1f} days)\")\n",
    "print(f\"   Prometheus: {'‚úÖ Available' if PROMETHEUS_AVAILABLE else '‚ùå Unavailable'}\")\n",
    "\n",
    "\n",
    "# ====================\n",
    "# Prometheus Data Fetching Functions\n",
    "# ====================\n",
    "\n",
    "def fetch_prometheus_timeseries(metric_query, lookback_hours=720):\n",
    "    \"\"\"\n",
    "    Fetch time series data from Prometheus\n",
    "    \n",
    "    Args:\n",
    "        metric_query: PromQL query string\n",
    "        lookback_hours: Time window in hours (default: 720 = 30 days)\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with timestamp and value columns\n",
    "    \"\"\"\n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=lookback_hours)\n",
    "    \n",
    "    params = {\n",
    "        'query': metric_query,\n",
    "        'start': int(start_time.timestamp()),\n",
    "        'end': int(end_time.timestamp()),\n",
    "        'step': '5m'  # 5-minute intervals\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f'{PROMETHEUS_URL}/api/v1/query_range', \n",
    "            params=params,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        if result['status'] != 'success':\n",
    "            raise ValueError(f\"Prometheus query failed: {result}\")\n",
    "        \n",
    "        # Parse results\n",
    "        timestamps = []\n",
    "        values = []\n",
    "        for series in result['data']['result']:\n",
    "            for timestamp, value in series['values']:\n",
    "                timestamps.append(pd.to_datetime(timestamp, unit='s'))\n",
    "                values.append(float(value))\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'timestamp': timestamps,\n",
    "            'value': values\n",
    "        })\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to fetch Prometheus data: {e}\")\n",
    "        return pd.DataFrame(columns=['timestamp', 'value'])\n",
    "\n",
    "\n",
    "def fetch_prometheus_metrics_for_prediction(lookback_hours=720):\n",
    "    \"\"\"\n",
    "    Fetch all metrics needed for predictive analytics from Prometheus\n",
    "    \n",
    "    Args:\n",
    "        lookback_hours: Time window in hours\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with timestamp, cpu_usage, memory_usage, disk_usage, network_in, network_out\n",
    "    \"\"\"\n",
    "    print(f\"üîç Fetching metrics from Prometheus (lookback: {lookback_hours}h)...\")\n",
    "    \n",
    "    # Prometheus query mappings\n",
    "    metric_queries = {\n",
    "        'cpu_usage': 'instance:node_cpu:ratio',\n",
    "        'memory_usage': 'instance:node_memory_utilisation:ratio',\n",
    "        'disk_usage': 'instance:node_filesystem_usage:ratio',\n",
    "        'network_in': 'instance:node_network_receive_bytes:rate1m',\n",
    "        'network_out': 'instance:node_network_transmit_bytes:rate1m'\n",
    "    }\n",
    "    \n",
    "    # Fetch each metric\n",
    "    metric_dfs = {}\n",
    "    for metric_name, query in metric_queries.items():\n",
    "        print(f\"  üìä Fetching {metric_name}...\")\n",
    "        df = fetch_prometheus_timeseries(query, lookback_hours)\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            metric_dfs[metric_name] = df\n",
    "            print(f\"    ‚úÖ {len(df)} data points\")\n",
    "        else:\n",
    "            print(f\"    ‚ö†Ô∏è  No data available\")\n",
    "    \n",
    "    # If no metrics fetched, return empty DataFrame\n",
    "    if not metric_dfs:\n",
    "        print(f\"‚ùå No metrics fetched from Prometheus\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Merge all metrics on timestamp\n",
    "    print(f\"\\nüîß Merging metrics...\")\n",
    "    combined_df = None\n",
    "    \n",
    "    for metric_name, df in metric_dfs.items():\n",
    "        df = df.rename(columns={'value': metric_name})\n",
    "        if combined_df is None:\n",
    "            combined_df = df\n",
    "        else:\n",
    "            combined_df = combined_df.merge(df, on='timestamp', how='outer')\n",
    "    \n",
    "    # Sort by timestamp and forward-fill missing values\n",
    "    combined_df = combined_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    combined_df = combined_df.ffill().bfill()\n",
    "    \n",
    "    # Fill any remaining NaN with 0\n",
    "    combined_df = combined_df.fillna(0)\n",
    "    \n",
    "    print(f\"‚úÖ Combined dataset: {len(combined_df)} samples with {len(metric_dfs)} metrics\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "print(\"‚úÖ Data fetching functions configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data based on data source configuration\n",
    "print(\"üìä Generating training data...\")\n",
    "print(f\"   Mode: {DATA_SOURCE}\")\n",
    "print(f\"   Time window: {TRAINING_HOURS}h ({TRAINING_HOURS / 24:.1f} days)\")\n",
    "\n",
    "if DATA_SOURCE == 'prometheus' and PROMETHEUS_AVAILABLE:\n",
    "    # Fetch data from Prometheus\n",
    "    print(\"\\nüîç Fetching data from Prometheus...\")\n",
    "    sample_data = fetch_prometheus_metrics_for_prediction(lookback_hours=TRAINING_HOURS)\n",
    "    \n",
    "    if len(sample_data) == 0:\n",
    "        print(\"‚ö†Ô∏è  No Prometheus data available, falling back to synthetic\")\n",
    "        n_samples = int(TRAINING_HOURS * 12)  # 5-min intervals\n",
    "        sample_data = generate_sample_timeseries_data(n_samples=n_samples)\n",
    "    else:\n",
    "        print(f\"‚úÖ Using Prometheus data: {len(sample_data)} samples\")\n",
    "\n",
    "elif DATA_SOURCE == 'hybrid' and PROMETHEUS_AVAILABLE:\n",
    "    # Mix Prometheus and synthetic data\n",
    "    print(\"\\nüîç Creating hybrid dataset (50% Prometheus, 50% synthetic)...\")\n",
    "    prom_data = fetch_prometheus_metrics_for_prediction(lookback_hours=TRAINING_HOURS)\n",
    "    \n",
    "    if len(prom_data) > 0:\n",
    "        # Generate synthetic data to match Prometheus size\n",
    "        synthetic_data = generate_sample_timeseries_data(n_samples=len(prom_data))\n",
    "        \n",
    "        # Combine datasets\n",
    "        sample_data = pd.concat([prom_data, synthetic_data], ignore_index=True)\n",
    "        sample_data = sample_data.sort_values('timestamp').reset_index(drop=True)\n",
    "        print(f\"‚úÖ Combined: {len(prom_data)} Prometheus + {len(synthetic_data)} synthetic = {len(sample_data)} total\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No Prometheus data available, using 100% synthetic\")\n",
    "        n_samples = int(TRAINING_HOURS * 12)\n",
    "        sample_data = generate_sample_timeseries_data(n_samples=n_samples)\n",
    "\n",
    "else:\n",
    "    # Pure synthetic data\n",
    "    print(\"\\nüìä Generating synthetic time series data...\")\n",
    "    print(\"   This simulates realistic infrastructure metrics with patterns:\")\n",
    "    print(\"   - Daily cycles (higher during business hours)\")\n",
    "    print(\"   - Weekly patterns (weekday vs weekend)\")\n",
    "    print(\"   - Trends (gradual growth over time)\")\n",
    "    print(\"   - Noise (random variations)\")\n",
    "    \n",
    "    # Calculate sample count based on training hours (5-minute intervals)\n",
    "    n_samples = int(TRAINING_HOURS * 12)\n",
    "    sample_data = generate_sample_timeseries_data(n_samples=n_samples)\n",
    "    print(f\"‚úÖ Generated {len(sample_data)} synthetic samples\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training data prepared:\")\n",
    "print(f\"   Samples: {len(sample_data)}\")\n",
    "print(f\"   Columns: {', '.join(sample_data.columns)}\")\n",
    "print(f\"   Shape: {sample_data.shape}\")\n",
    "print(f\"   Date range: {sample_data['timestamp'].min()} to {sample_data['timestamp'].max()}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìã Sample data (first 5 rows):\")\n",
    "display(sample_data.head())\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nüìà Data Statistics:\")\n",
    "display(sample_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time series data for training\n",
    "print(\"üìä Generating synthetic time series data...\")\n",
    "print(\"   This simulates realistic infrastructure metrics with patterns:\")\n",
    "print(\"   - Daily cycles (higher during business hours)\")\n",
    "print(\"   - Weekly patterns (weekday vs weekend)\")\n",
    "print(\"   - Trends (gradual growth over time)\")\n",
    "print(\"   - Noise (random variations)\")\n",
    "\n",
    "# Generate 2000 samples (about 7 days at 5-minute intervals)\n",
    "sample_data = generate_sample_timeseries_data(n_samples=2000)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(sample_data)} samples\")\n",
    "print(f\"   Columns: {', '.join(sample_data.columns)}\")\n",
    "print(f\"   Shape: {sample_data.shape}\")\n",
    "print(f\"   Date range: {sample_data['timestamp'].min()} to {sample_data['timestamp'].max()}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìã Sample data (first 5 rows):\")\n",
    "display(sample_data.head())\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nüìà Data Statistics:\")\n",
    "display(sample_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the generated time series data\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "fig.suptitle('Synthetic Time Series Training Data', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['cpu_usage', 'memory_usage', 'disk_usage', 'network_in', 'network_out']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.plot(sample_data['timestamp'], sample_data[metric], color=color, alpha=0.7, linewidth=1)\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide the last subplot (we have 5 metrics in a 3x2 grid)\n",
    "axes[2, 1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% validation\n",
    "split_point = int(len(sample_data) * 0.8)\n",
    "\n",
    "train_data = sample_data.iloc[:split_point].copy()\n",
    "val_data = sample_data.iloc[split_point:].copy()\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Training samples: {len(train_data)} ({len(train_data)/len(sample_data)*100:.1f}%)\")\n",
    "print(f\"   Validation samples: {len(val_data)} ({len(val_data)/len(sample_data)*100:.1f}%)\")\n",
    "print(f\"   Training period: {train_data['timestamp'].min()} to {train_data['timestamp'].max()}\")\n",
    "print(f\"   Validation period: {val_data['timestamp'].min()} to {val_data['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Section\n",
    "\n",
    "### Initialize and Train PredictiveAnalytics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PredictiveAnalytics model\n",
    "print(\"üî¨ Initializing PredictiveAnalytics model...\")\n",
    "\n",
    "# Configure model parameters\n",
    "FORECAST_HORIZON = 12  # Predict 12 time steps ahead\n",
    "LOOKBACK_WINDOW = 24   # Use 24 historical time steps\n",
    "\n",
    "predictor = PredictiveAnalytics(\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    lookback_window=LOOKBACK_WINDOW\n",
    ")\n",
    "\n",
    "print(f\"   Forecast horizon: {FORECAST_HORIZON} time steps\")\n",
    "print(f\"   Lookback window: {LOOKBACK_WINDOW} time steps\")\n",
    "print(f\"   Target metrics: {', '.join(predictor.target_metrics)}\")\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\nüéØ Training models on {len(train_data)} samples...\")\n",
    "print(\"   This will train separate models for each metric:\")\n",
    "print(\"   - CPU usage\")\n",
    "print(\"   - Memory usage\")\n",
    "print(\"   - Disk usage\")\n",
    "print(\"   - Network in\")\n",
    "print(\"   - Network out\")\n",
    "\n",
    "training_results = predictor.train(train_data)\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"   Models trained: {training_results['models_trained']}\")\n",
    "print(f\"   Feature count: {training_results['feature_count']}\")\n",
    "print(f\"   Forecast horizon: {training_results['forecast_horizon']}\")\n",
    "print(f\"   Lookback window: {training_results['lookback_window']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed metrics for each model\n",
    "print(\"üìä Model Performance Metrics:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric_name, results in training_results['metrics'].items():\n",
    "    print(f\"\\n{metric_name.upper().replace('_', ' ')}:\")\n",
    "    print(f\"  Mean Absolute Error (MAE):  {results['mae']:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {results['rmse']:.4f}\")\n",
    "    print(f\"  R¬≤ Score: {results['r2']:.4f}\")\n",
    "    print(f\"  Training samples: {results['training_samples']}\")\n",
    "    print(f\"  Test samples: {results['test_samples']}\")\n",
    "    print(\"  \" + \"-\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Calculate average R¬≤ across all metrics\n",
    "avg_r2 = np.mean([r['r2'] for r in training_results['metrics'].values()])\n",
    "print(f\"\\nüìà Average R¬≤ Score: {avg_r2:.4f}\")\n",
    "\n",
    "if avg_r2 > 0.8:\n",
    "    print(\"‚úÖ Excellent model performance!\")\n",
    "elif avg_r2 > 0.6:\n",
    "    print(\"‚úÖ Good model performance\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model performance could be improved - consider more training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation Section\n",
    "\n",
    "### Test Predictions on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation data\n",
    "print(\"üîÆ Making predictions on validation data...\")\n",
    "\n",
    "predictions = predictor.predict(val_data.head(50))\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions generated:\")\n",
    "print(f\"   Timestamp: {predictions['timestamp']}\")\n",
    "print(f\"   Metrics predicted: {len(predictions['predictions'])}\")\n",
    "print(f\"   Lookback window used: {predictions['lookback_window']}\")\n",
    "\n",
    "# Display predictions for each metric\n",
    "print(\"\\nüìä Prediction Results:\\n\")\n",
    "for metric_name, pred_data in predictions['predictions'].items():\n",
    "    forecast = pred_data['forecast']\n",
    "    confidence = pred_data.get('confidence', [0.5] * len(forecast))\n",
    "    \n",
    "    print(f\"{metric_name.upper().replace('_', ' ')}:\")\n",
    "    print(f\"  Forecast values (first 5): {[f'{v:.2f}' for v in forecast[:5]]}\")\n",
    "    print(f\"  Confidence (avg): {np.mean(confidence):.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Predictions vs Actual Values (Validation Set)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Select metrics to visualize\n",
    "vis_metrics = ['cpu_usage', 'memory_usage', 'disk_usage', 'network_in']\n",
    "\n",
    "for idx, metric in enumerate(vis_metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Plot actual values\n",
    "    actual_vals = val_data[metric].head(50).values\n",
    "    ax.plot(range(len(actual_vals)), actual_vals, label='Actual', color='blue', alpha=0.6, linewidth=2)\n",
    "    \n",
    "    # Plot predictions (if available)\n",
    "    if metric in predictions['predictions']:\n",
    "        forecast = predictions['predictions'][metric]['forecast']\n",
    "        # Start prediction from lookback_window position\n",
    "        pred_start = predictions['lookback_window']\n",
    "        ax.plot(range(pred_start, pred_start + len(forecast)), \n",
    "               forecast, label='Predicted', color='red', alpha=0.6, linewidth=2, linestyle='--')\n",
    "    \n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving Section\n",
    "\n",
    "### Save Model in KServe-Compatible Format\n",
    "\n",
    "This is the critical step that implements the **Issue #13 fix**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save MULTI-OUTPUT model using sklearn's standard MultiOutputRegressor\n",
    "# This ensures KServe sklearn server can load it without custom class dependencies\n",
    "print(\"üíæ Saving sklearn Pipeline + MultiOutputRegressor for KServe...\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"KSERVE SKLEARN PIPELINE (Standard sklearn classes only)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìÇ Directory Structure:\")\n",
    "print(f\"   Base: {MODELS_DIR}\")\n",
    "print(f\"   Model subdirectory: {MODEL_NAME}/\")\n",
    "print(f\"   Full path: {MODELS_DIR}/{MODEL_NAME}/model.pkl\")\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Check if XGBoost is available\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"\\n‚úÖ XGBoost available - using XGBRegressor\")\n",
    "except ImportError:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"\\n‚ö†Ô∏è XGBoost not available - using RandomForestRegressor\")\n",
    "\n",
    "# ====================\n",
    "# PREPARE TRAINING DATA FOR MULTI-OUTPUT MODEL\n",
    "# ====================\n",
    "print(f\"\\nüîß Preparing training data for multi-output model...\")\n",
    "\n",
    "# Target columns in specific order (must match coordination engine expectations)\n",
    "target_columns = ['cpu_usage', 'memory_usage', 'disk_usage', 'network_in', 'network_out']\n",
    "\n",
    "# Get feature columns (all numeric columns except targets and timestamp)\n",
    "feature_columns = [col for col in train_data.columns \n",
    "                   if col not in target_columns + ['timestamp'] \n",
    "                   and train_data[col].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "\n",
    "print(f\"   Features: {len(feature_columns)} columns\")\n",
    "print(f\"   Targets: {target_columns}\")\n",
    "\n",
    "# Prepare X (features) and y (all 5 targets)\n",
    "X_train_all = train_data[feature_columns].values\n",
    "y_train_all = train_data[target_columns].values\n",
    "\n",
    "print(f\"   X_train shape: {X_train_all.shape}\")\n",
    "print(f\"   y_train shape: {y_train_all.shape}\")\n",
    "\n",
    "# ====================\n",
    "# CREATE SKLEARN PIPELINE WITH MULTIOUTPUTREGRESSOR\n",
    "# ====================\n",
    "print(f\"\\nüîß Creating sklearn Pipeline with MultiOutputRegressor...\")\n",
    "\n",
    "# Create the base estimator\n",
    "if XGBOOST_AVAILABLE:\n",
    "    base_estimator = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        tree_method='hist',  # Fast histogram method for CPU\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0  # Suppress XGBoost output\n",
    "    )\n",
    "    print(\"   Base estimator: XGBRegressor (tree_method='hist')\")\n",
    "else:\n",
    "    base_estimator = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    print(\"   Base estimator: RandomForestRegressor\")\n",
    "\n",
    "# Create Pipeline: StandardScaler -> MultiOutputRegressor\n",
    "# This is fully serializable and KServe sklearn server can load it!\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', MultiOutputRegressor(base_estimator))\n",
    "])\n",
    "\n",
    "print(\"   Pipeline steps:\")\n",
    "print(\"     1. StandardScaler (normalizes features)\")\n",
    "print(\"     2. MultiOutputRegressor (wraps base estimator for 5 outputs)\")\n",
    "\n",
    "# ====================\n",
    "# TRAIN THE PIPELINE\n",
    "# ====================\n",
    "print(f\"\\nüéØ Training multi-output pipeline on {len(X_train_all)} samples...\")\n",
    "print(\"   This will train 5 separate regressors (one per target metric)\")\n",
    "\n",
    "pipeline.fit(X_train_all, y_train_all)\n",
    "\n",
    "print(\"   ‚úÖ Pipeline trained successfully!\")\n",
    "\n",
    "# Test predictions\n",
    "test_input = X_train_all[:1]  # Use first sample for testing\n",
    "test_output = pipeline.predict(test_input)\n",
    "print(f\"\\nüß™ Test prediction:\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "print(f\"   Output shape: {test_output.shape}\")\n",
    "print(f\"   Output values: {test_output[0]}\")\n",
    "print(f\"   Output order: [cpu, memory, disk, network_in, network_out]\")\n",
    "\n",
    "# ====================\n",
    "# SAVE THE PIPELINE\n",
    "# ====================\n",
    "kserve_dir = MODELS_DIR / MODEL_NAME\n",
    "kserve_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = kserve_dir / 'model.pkl'\n",
    "joblib.dump(pipeline, model_path)\n",
    "print(f\"\\n‚úÖ sklearn Pipeline saved to: {model_path}\")\n",
    "\n",
    "# Verify the saved model\n",
    "expected_path = MODELS_DIR / MODEL_NAME / 'model.pkl'\n",
    "print(f\"\\nüîç Verifying saved model...\")\n",
    "\n",
    "if expected_path.exists():\n",
    "    # Load and verify\n",
    "    loaded_pipeline = joblib.load(expected_path)\n",
    "    size_kb = expected_path.stat().st_size / 1024\n",
    "    \n",
    "    # Verify predict() works\n",
    "    verify_output = loaded_pipeline.predict(test_input)\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCCESS! sklearn Pipeline saved and verified:\")\n",
    "    print(f\"   Location: {expected_path}\")\n",
    "    print(f\"   Size: {size_kb:.2f} KB\")\n",
    "    print(f\"   Type: {type(loaded_pipeline).__name__}\")\n",
    "    print(f\"   Pipeline steps: {list(loaded_pipeline.named_steps.keys())}\")\n",
    "    print(f\"   Has predict(): {hasattr(loaded_pipeline, 'predict')}\")\n",
    "    print(f\"   Output shape: {verify_output.shape}\")\n",
    "    \n",
    "    print(f\"\\nüì° KServe Response Format:\")\n",
    "    print(f\"   Request: {{\\\"instances\\\": [[... {len(feature_columns)} features ...]]}}\")\n",
    "    print(f\"   Response: {{\\\"predictions\\\": [[cpu, mem, disk, net_in, net_out]]}}\")\n",
    "    \n",
    "    print(f\"\\nüîó Coordination Engine Integration:\")\n",
    "    print(f\"   predictions[0] = cpu_usage\")\n",
    "    print(f\"   predictions[1] = memory_usage\")\n",
    "    print(f\"   predictions[2] = disk_usage\")\n",
    "    print(f\"   predictions[3] = network_in\")\n",
    "    print(f\"   predictions[4] = network_out\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  IMPORTANT: Feature count = {len(feature_columns)}\")\n",
    "    print(f\"   Coordination engine must send exactly {len(feature_columns)} features!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ùå ERROR: Model not found at expected location: {expected_path}\")\n",
    "\n",
    "# Store feature count for later reference\n",
    "FEATURE_COUNT = len(feature_columns)\n",
    "print(f\"\\nüìä Model expects {FEATURE_COUNT} input features\")\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model Loading (Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the sklearn Pipeline can be loaded back (as KServe sklearn server would)\n",
    "print(\"üß™ Testing sklearn Pipeline loading (simulating KServe sklearn server)...\")\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the saved Pipeline (exactly as KServe sklearn server will do)\n",
    "model_path = MODELS_DIR / MODEL_NAME / 'model.pkl'\n",
    "loaded_pipeline = joblib.load(model_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline loaded successfully!\")\n",
    "print(f\"   Type: {type(loaded_pipeline).__name__}\")\n",
    "print(f\"   Steps: {list(loaded_pipeline.named_steps.keys())}\")\n",
    "\n",
    "# Verify it's a standard sklearn Pipeline\n",
    "assert type(loaded_pipeline).__name__ == 'Pipeline', \"Model must be sklearn Pipeline!\"\n",
    "print(f\"   ‚úÖ Model is a standard sklearn Pipeline (KServe compatible)\")\n",
    "\n",
    "# Test prediction with validation data\n",
    "X_val = val_data[feature_columns].values\n",
    "y_val_pred = loaded_pipeline.predict(X_val[:10])\n",
    "\n",
    "print(f\"\\nüß™ Test prediction on validation data:\")\n",
    "print(f\"   Input shape: {X_val[:10].shape}\")\n",
    "print(f\"   Output shape: {y_val_pred.shape}\")\n",
    "print(f\"   First prediction: {y_val_pred[0]}\")\n",
    "print(f\"   Output columns: [cpu, memory, disk, network_in, network_out]\")\n",
    "\n",
    "# Verify output shape is correct\n",
    "assert y_val_pred.shape[1] == 5, f\"Expected 5 outputs, got {y_val_pred.shape[1]}\"\n",
    "print(f\"   ‚úÖ Output has 5 columns (all metrics)\")\n",
    "\n",
    "print(f\"\\nüéâ sklearn Pipeline is ready for KServe deployment!\")\n",
    "print(f\"   - No custom classes\")\n",
    "print(f\"   - Standard sklearn serialization\")\n",
    "print(f\"   - KServe sklearn server can load directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Verification Section\n",
    "\n",
    "### Generate KServe Test Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate commands for testing the deployed model\n",
    "print(\"üìã KServe Deployment Test Commands:\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"After deploying the InferenceService, run these commands to verify:\\n\")\n",
    "\n",
    "print(\"# 1. Get the predictor pod IP\")\n",
    "print(f\"PREDICTOR_IP=$(oc get pod -l serving.kserve.io/inferenceservice={MODEL_NAME} -o jsonpath='{{.items[0].status.podIP}}')\")\n",
    "print(f\"echo \\\"Predictor IP: $PREDICTOR_IP\\\"\\n\")\n",
    "\n",
    "print(\"# 2. List available models (should return 'predictive-analytics', not 'model')\")\n",
    "print(\"curl http://${PREDICTOR_IP}:8080/v1/models\")\n",
    "print(f\"# Expected: {{\\\"models\\\":[\\\"{MODEL_NAME}\\\"]}}  ‚úÖ\\n\")\n",
    "\n",
    "print(\"# 3. Check model status\")\n",
    "print(f\"curl http://${{PREDICTOR_IP}}:8080/v1/models/{MODEL_NAME}\")\n",
    "print(f\"# Expected: {{\\\"name\\\":\\\"{MODEL_NAME}\\\",\\\"ready\\\":true}}  ‚úÖ\\n\")\n",
    "\n",
    "print(\"# 4. Test prediction endpoint\")\n",
    "print(f\"curl -X POST http://${{PREDICTOR_IP}}:8080/v1/models/{MODEL_NAME}:predict \\\\\")\n",
    "print(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "print(\"  -d '{\\\"instances\\\": [[0.5, 0.6, 0.4, 100, 80]]}'\")\n",
    "print(\"# Expected: Prediction response with forecast values  ‚úÖ\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ If all commands work, Issue #13 is fixed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What Was Accomplished\n",
    "\n",
    "‚úÖ **Model trained** with multi-metric forecasting (CPU, memory, disk, network)  \n",
    "‚úÖ **Saved in KServe format**: `/mnt/models/predictive-analytics/model.pkl`  \n",
    "‚úÖ **Issue #13 fixed**: Model will register as `\"predictive-analytics\"` not `\"model\"`  \n",
    "‚úÖ **Validated**: Model can be loaded and makes predictions  \n",
    "‚úÖ **Ready for deployment**: Compatible with KServe InferenceService\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Deploy InferenceService** (if not already deployed):\n",
    "   ```yaml\n",
    "   apiVersion: serving.kserve.io/v1beta1\n",
    "   kind: InferenceService\n",
    "   metadata:\n",
    "     name: predictive-analytics\n",
    "   spec:\n",
    "     predictor:\n",
    "       model:\n",
    "         name: predictive-analytics\n",
    "         runtime: sklearn-pvc-runtime\n",
    "         storageUri: \"pvc://model-storage-pvc/predictive-analytics\"\n",
    "   ```\n",
    "\n",
    "2. **Verify deployment** using the commands above\n",
    "\n",
    "3. **Test from coordination engine**:\n",
    "   - Ensure coordination engine can call `/v1/models/predictive-analytics:predict`\n",
    "   - Verify predictions work end-to-end\n",
    "\n",
    "4. **Monitor predictions**:\n",
    "   - Check prediction accuracy over time\n",
    "   - Retrain periodically with new data\n",
    "\n",
    "### References\n",
    "\n",
    "- **Issue**: [#13 - KServe model registration fix](https://github.com/tosin2013/openshift-aiops-platform/issues/13)\n",
    "- **Module**: `src/models/predictive_analytics.py`\n",
    "- **Training script**: `src/models/train_predictive_analytics.py`\n",
    "- **Documentation**: `src/models/KSERVE_FIX_README.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics Model for KServe\n",
    "\n",
    "## Overview\n",
    "This notebook trains and saves a **Predictive Analytics model** in KServe-compatible format for the `predictive-analytics` InferenceService.\n",
    "\n",
    "**Model Type**: Time series forecasting with Random Forest  \n",
    "**Purpose**: Predict future resource usage (CPU, memory, disk, network)  \n",
    "**Deployment**: KServe InferenceService with sklearn runtime\n",
    "\n",
    "## KServe Integration (Issue #13 Fix)\n",
    "\n",
    "This notebook implements the fix for [Issue #13](https://github.com/tosin2013/openshift-aiops-platform/issues/13) where models were registering as `\"model\"` instead of `\"predictive-analytics\"`.\n",
    "\n",
    "### Problem Solved\n",
    "- **Before**: Models saved to `/mnt/models/cpu_usage_step_0_model.pkl` (flat structure)\n",
    "- **After**: Models saved to `/mnt/models/predictive-analytics/model.pkl` (KServe structure)\n",
    "- **Result**: Model registers correctly as `\"predictive-analytics\"` ‚úÖ\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Notebook Training ‚Üí /mnt/models/predictive-analytics/model.pkl\n",
    "                    ‚Üì\n",
    "KServe InferenceService (storageUri: pvc://model-storage-pvc/predictive-analytics)\n",
    "                    ‚Üì\n",
    "Model registered as: \"predictive-analytics\"\n",
    "                    ‚Üì\n",
    "Endpoint: /v1/models/predictive-analytics:predict\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "- Model storage PVC mounted at `/mnt/models`\n",
    "- Python environment with sklearn, pandas, numpy\n",
    "- Access to `src/models/predictive_analytics.py` module\n",
    "\n",
    "## What This Notebook Does\n",
    "1. ‚úÖ Imports the `PredictiveAnalytics` module\n",
    "2. ‚úÖ Generates synthetic time series training data\n",
    "3. ‚úÖ Trains multi-metric forecasting models (CPU, memory, disk, network)\n",
    "4. ‚úÖ Saves in KServe-compatible format: `/mnt/models/predictive-analytics/model.pkl`\n",
    "5. ‚úÖ Validates the model works correctly\n",
    "6. ‚úÖ Tests prediction endpoint format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section\n",
    "\n",
    "### Import Libraries and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path for src/models module - works from any directory\n",
    "def find_models_path():\n",
    "    \"\"\"Find src/models path regardless of current working directory\"\"\"\n",
    "    possible_paths = [\n",
    "        Path(__file__).parent.parent.parent / 'src' / 'models' if '__file__' in dir() else None,\n",
    "        Path.cwd().parent.parent / 'src' / 'models',\n",
    "        Path('/workspace/repo/src/models'),\n",
    "        Path('/opt/app-root/src/openshift-aiops-platform/src/models'),\n",
    "    ]\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists() and (p / 'predictive_analytics.py').exists():\n",
    "            return str(p)\n",
    "    # Try relative path search\n",
    "    current = Path.cwd()\n",
    "    for _ in range(5):\n",
    "        models_path = current / 'src' / 'models'\n",
    "        if models_path.exists() and (models_path / 'predictive_analytics.py').exists():\n",
    "            return str(models_path)\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "models_path = find_models_path()\n",
    "if models_path:\n",
    "    sys.path.insert(0, models_path)\n",
    "    print(f\"‚úÖ Models path found: {models_path}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è Models path not found - using fallback implementation\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PredictiveAnalytics module\n",
    "try:\n",
    "    from predictive_analytics import PredictiveAnalytics, generate_sample_timeseries_data\n",
    "    print(\"‚úÖ PredictiveAnalytics module imported successfully\")\n",
    "    USING_MODULE = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import PredictiveAnalytics module: {e}\")\n",
    "    print(\"   Please ensure src/models/predictive_analytics.py is available\")\n",
    "    USING_MODULE = False\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model Storage Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure storage paths\n",
    "# Use /mnt/models for persistent storage (model-storage-pvc)\n",
    "# Fallback to local for development outside cluster\n",
    "MODELS_DIR = Path('/mnt/models') if Path('/mnt/models').exists() else Path('/opt/app-root/src/models')\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model name must match InferenceService name\n",
    "MODEL_NAME = 'predictive-analytics'\n",
    "MODEL_DIR = MODELS_DIR / MODEL_NAME  # Will be created by save_models()\n",
    "\n",
    "print(f\"üìÅ Model Storage Configuration:\")\n",
    "print(f\"   Base directory: {MODELS_DIR}\")\n",
    "print(f\"   Model name: {MODEL_NAME}\")\n",
    "print(f\"   Expected KServe path: {MODEL_DIR}/model.pkl\")\n",
    "print(f\"   PVC available: {'‚úÖ Yes' if MODELS_DIR == Path('/mnt/models') else '‚ö†Ô∏è No (using local)'}\")\n",
    "\n",
    "if not USING_MODULE:\n",
    "    print(\"\\n‚ùå Cannot proceed without PredictiveAnalytics module\")\n",
    "    raise ImportError(\"PredictiveAnalytics module required for this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation Section\n",
    "\n",
    "### Generate Synthetic Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time series data for training\n",
    "print(\"üìä Generating synthetic time series data...\")\n",
    "print(\"   This simulates realistic infrastructure metrics with patterns:\")\n",
    "print(\"   - Daily cycles (higher during business hours)\")\n",
    "print(\"   - Weekly patterns (weekday vs weekend)\")\n",
    "print(\"   - Trends (gradual growth over time)\")\n",
    "print(\"   - Noise (random variations)\")\n",
    "\n",
    "# Generate 2000 samples (about 7 days at 5-minute intervals)\n",
    "sample_data = generate_sample_timeseries_data(n_samples=2000)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(sample_data)} samples\")\n",
    "print(f\"   Columns: {', '.join(sample_data.columns)}\")\n",
    "print(f\"   Shape: {sample_data.shape}\")\n",
    "print(f\"   Date range: {sample_data['timestamp'].min()} to {sample_data['timestamp'].max()}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìã Sample data (first 5 rows):\")\n",
    "display(sample_data.head())\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nüìà Data Statistics:\")\n",
    "display(sample_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the generated time series data\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "fig.suptitle('Synthetic Time Series Training Data', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['cpu_usage', 'memory_usage', 'disk_usage', 'network_in', 'network_out']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.plot(sample_data['timestamp'], sample_data[metric], color=color, alpha=0.7, linewidth=1)\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide the last subplot (we have 5 metrics in a 3x2 grid)\n",
    "axes[2, 1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% validation\n",
    "split_point = int(len(sample_data) * 0.8)\n",
    "\n",
    "train_data = sample_data.iloc[:split_point].copy()\n",
    "val_data = sample_data.iloc[split_point:].copy()\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Training samples: {len(train_data)} ({len(train_data)/len(sample_data)*100:.1f}%)\")\n",
    "print(f\"   Validation samples: {len(val_data)} ({len(val_data)/len(sample_data)*100:.1f}%)\")\n",
    "print(f\"   Training period: {train_data['timestamp'].min()} to {train_data['timestamp'].max()}\")\n",
    "print(f\"   Validation period: {val_data['timestamp'].min()} to {val_data['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Section\n",
    "\n",
    "### Initialize and Train PredictiveAnalytics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PredictiveAnalytics model\n",
    "print(\"üî¨ Initializing PredictiveAnalytics model...\")\n",
    "\n",
    "# Configure model parameters\n",
    "FORECAST_HORIZON = 12  # Predict 12 time steps ahead\n",
    "LOOKBACK_WINDOW = 24   # Use 24 historical time steps\n",
    "\n",
    "predictor = PredictiveAnalytics(\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    lookback_window=LOOKBACK_WINDOW\n",
    ")\n",
    "\n",
    "print(f\"   Forecast horizon: {FORECAST_HORIZON} time steps\")\n",
    "print(f\"   Lookback window: {LOOKBACK_WINDOW} time steps\")\n",
    "print(f\"   Target metrics: {', '.join(predictor.target_metrics)}\")\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\nüéØ Training models on {len(train_data)} samples...\")\n",
    "print(\"   This will train separate models for each metric:\")\n",
    "print(\"   - CPU usage\")\n",
    "print(\"   - Memory usage\")\n",
    "print(\"   - Disk usage\")\n",
    "print(\"   - Network in\")\n",
    "print(\"   - Network out\")\n",
    "\n",
    "training_results = predictor.train(train_data)\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"   Models trained: {training_results['models_trained']}\")\n",
    "print(f\"   Feature count: {training_results['feature_count']}\")\n",
    "print(f\"   Forecast horizon: {training_results['forecast_horizon']}\")\n",
    "print(f\"   Lookback window: {training_results['lookback_window']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed metrics for each model\n",
    "print(\"üìä Model Performance Metrics:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric_name, results in training_results['metrics'].items():\n",
    "    print(f\"\\n{metric_name.upper().replace('_', ' ')}:\")\n",
    "    print(f\"  Mean Absolute Error (MAE):  {results['mae']:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {results['rmse']:.4f}\")\n",
    "    print(f\"  R¬≤ Score: {results['r2']:.4f}\")\n",
    "    print(f\"  Training samples: {results['training_samples']}\")\n",
    "    print(f\"  Test samples: {results['test_samples']}\")\n",
    "    print(\"  \" + \"-\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Calculate average R¬≤ across all metrics\n",
    "avg_r2 = np.mean([r['r2'] for r in training_results['metrics'].values()])\n",
    "print(f\"\\nüìà Average R¬≤ Score: {avg_r2:.4f}\")\n",
    "\n",
    "if avg_r2 > 0.8:\n",
    "    print(\"‚úÖ Excellent model performance!\")\n",
    "elif avg_r2 > 0.6:\n",
    "    print(\"‚úÖ Good model performance\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model performance could be improved - consider more training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation Section\n",
    "\n",
    "### Test Predictions on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation data\n",
    "print(\"üîÆ Making predictions on validation data...\")\n",
    "\n",
    "predictions = predictor.predict(val_data.head(50))\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions generated:\")\n",
    "print(f\"   Timestamp: {predictions['timestamp']}\")\n",
    "print(f\"   Metrics predicted: {len(predictions['predictions'])}\")\n",
    "print(f\"   Lookback window used: {predictions['lookback_window']}\")\n",
    "\n",
    "# Display predictions for each metric\n",
    "print(\"\\nüìä Prediction Results:\\n\")\n",
    "for metric_name, pred_data in predictions['predictions'].items():\n",
    "    forecast = pred_data['forecast']\n",
    "    confidence = pred_data.get('confidence', [0.5] * len(forecast))\n",
    "    \n",
    "    print(f\"{metric_name.upper().replace('_', ' ')}:\")\n",
    "    print(f\"  Forecast values (first 5): {[f'{v:.2f}' for v in forecast[:5]]}\")\n",
    "    print(f\"  Confidence (avg): {np.mean(confidence):.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Predictions vs Actual Values (Validation Set)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Select metrics to visualize\n",
    "vis_metrics = ['cpu_usage', 'memory_usage', 'disk_usage', 'network_in']\n",
    "\n",
    "for idx, metric in enumerate(vis_metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Plot actual values\n",
    "    actual_vals = val_data[metric].head(50).values\n",
    "    ax.plot(range(len(actual_vals)), actual_vals, label='Actual', color='blue', alpha=0.6, linewidth=2)\n",
    "    \n",
    "    # Plot predictions (if available)\n",
    "    if metric in predictions['predictions']:\n",
    "        forecast = predictions['predictions'][metric]['forecast']\n",
    "        # Start prediction from lookback_window position\n",
    "        pred_start = predictions['lookback_window']\n",
    "        ax.plot(range(pred_start, pred_start + len(forecast)), \n",
    "               forecast, label='Predicted', color='red', alpha=0.6, linewidth=2, linestyle='--')\n",
    "    \n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving Section\n",
    "\n",
    "### Save Model in KServe-Compatible Format\n",
    "\n",
    "This is the critical step that implements the **Issue #13 fix**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model with KServe-compatible structure\n",
    "print(\"üíæ Saving model in KServe-compatible format...\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"KSERVE COMPATIBILITY (Issue #13 Fix)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìÇ Directory Structure:\")\n",
    "print(f\"   Base: {MODELS_DIR}\")\n",
    "print(f\"   Model subdirectory: {MODEL_NAME}/\")\n",
    "print(f\"   Full path: {MODELS_DIR}/{MODEL_NAME}/model.pkl\")\n",
    "print(f\"\\n‚úÖ This matches KServe's expected structure!\")\n",
    "print(f\"   storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")\n",
    "print(f\"   KServe loads: /mnt/models/{MODEL_NAME}/model.pkl\")\n",
    "print(f\"   Registers as: '{MODEL_NAME}' (not 'model')\")\n",
    "\n",
    "# Save using the updated save_models() method\n",
    "# kserve_compatible=True is the default\n",
    "print(f\"\\nüîß Calling save_models() with kserve_compatible=True...\")\n",
    "predictor.save_models(str(MODELS_DIR), kserve_compatible=True)\n",
    "\n",
    "# Verify the saved model\n",
    "expected_path = MODELS_DIR / MODEL_NAME / 'model.pkl'\n",
    "print(f\"\\nüîç Verifying saved model...\")\n",
    "\n",
    "if expected_path.exists():\n",
    "    size_kb = expected_path.stat().st_size / 1024\n",
    "    print(f\"\\n‚úÖ SUCCESS! Model saved correctly:\")\n",
    "    print(f\"   Location: {expected_path}\")\n",
    "    print(f\"   Size: {size_kb:.2f} KB\")\n",
    "    print(f\"   Structure: ‚úÖ KServe-compatible subdirectory\")\n",
    "    \n",
    "    # Check for migration artifacts\n",
    "    old_files = list(MODELS_DIR.glob('*_step_*_model.pkl'))\n",
    "    if old_files:\n",
    "        print(f\"\\n‚ö†Ô∏è Found {len(old_files)} old model files (should be cleaned up)\")\n",
    "    else:\n",
    "        print(f\"   Cleanup: ‚úÖ No old files found\")\n",
    "    \n",
    "    print(f\"\\nüì° KServe Deployment Info:\")\n",
    "    print(f\"   InferenceService name: {MODEL_NAME}\")\n",
    "    print(f\"   Model registration: '{MODEL_NAME}' (fixed!)\")\n",
    "    print(f\"   Endpoint: /v1/models/{MODEL_NAME}:predict\")\n",
    "    print(f\"   Expected response to: curl http://<ip>:8080/v1/models\")\n",
    "    print(f\"   {{\\\"models\\\":[\\\"{MODEL_NAME}\\\"]}}  ‚Üê Correct! ‚úÖ\")\n",
    "    print(f\"   NOT: {{\\\"models\\\":[\\\"model\\\"]}}  ‚Üê This was the bug ‚ùå\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ùå ERROR: Model not found at expected location: {expected_path}\")\n",
    "    print(f\"   Check the save_models() implementation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model Loading (Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model can be loaded back\n",
    "print(\"üß™ Testing model loading...\")\n",
    "\n",
    "# Create a new instance and load the saved model\n",
    "test_predictor = PredictiveAnalytics()\n",
    "test_predictor.load_models(str(MODELS_DIR))\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Models loaded: {len(test_predictor.models)}\")\n",
    "print(f\"   Forecast horizon: {test_predictor.forecast_horizon}\")\n",
    "print(f\"   Lookback window: {test_predictor.lookback_window}\")\n",
    "print(f\"   Target metrics: {', '.join(test_predictor.target_metrics)}\")\n",
    "\n",
    "# Make a test prediction\n",
    "test_predictions = test_predictor.predict(val_data.head(50))\n",
    "print(f\"\\n‚úÖ Test prediction successful!\")\n",
    "print(f\"   Metrics predicted: {len(test_predictions['predictions'])}\")\n",
    "\n",
    "print(f\"\\nüéâ Model is ready for KServe deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Verification Section\n",
    "\n",
    "### Generate KServe Test Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate commands for testing the deployed model\n",
    "print(\"üìã KServe Deployment Test Commands:\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"After deploying the InferenceService, run these commands to verify:\\n\")\n",
    "\n",
    "print(\"# 1. Get the predictor pod IP\")\n",
    "print(f\"PREDICTOR_IP=$(oc get pod -l serving.kserve.io/inferenceservice={MODEL_NAME} -o jsonpath='{{.items[0].status.podIP}}')\")\n",
    "print(f\"echo \\\"Predictor IP: $PREDICTOR_IP\\\"\\n\")\n",
    "\n",
    "print(\"# 2. List available models (should return 'predictive-analytics', not 'model')\")\n",
    "print(\"curl http://${PREDICTOR_IP}:8080/v1/models\")\n",
    "print(f\"# Expected: {{\\\"models\\\":[\\\"{MODEL_NAME}\\\"]}}  ‚úÖ\\n\")\n",
    "\n",
    "print(\"# 3. Check model status\")\n",
    "print(f\"curl http://${{PREDICTOR_IP}}:8080/v1/models/{MODEL_NAME}\")\n",
    "print(f\"# Expected: {{\\\"name\\\":\\\"{MODEL_NAME}\\\",\\\"ready\\\":true}}  ‚úÖ\\n\")\n",
    "\n",
    "print(\"# 4. Test prediction endpoint\")\n",
    "print(f\"curl -X POST http://${{PREDICTOR_IP}}:8080/v1/models/{MODEL_NAME}:predict \\\\\")\n",
    "print(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "print(\"  -d '{\\\"instances\\\": [[0.5, 0.6, 0.4, 100, 80]]}'\")\n",
    "print(\"# Expected: Prediction response with forecast values  ‚úÖ\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ If all commands work, Issue #13 is fixed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What Was Accomplished\n",
    "\n",
    "‚úÖ **Model trained** with multi-metric forecasting (CPU, memory, disk, network)  \n",
    "‚úÖ **Saved in KServe format**: `/mnt/models/predictive-analytics/model.pkl`  \n",
    "‚úÖ **Issue #13 fixed**: Model will register as `\"predictive-analytics\"` not `\"model\"`  \n",
    "‚úÖ **Validated**: Model can be loaded and makes predictions  \n",
    "‚úÖ **Ready for deployment**: Compatible with KServe InferenceService\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Deploy InferenceService** (if not already deployed):\n",
    "   ```yaml\n",
    "   apiVersion: serving.kserve.io/v1beta1\n",
    "   kind: InferenceService\n",
    "   metadata:\n",
    "     name: predictive-analytics\n",
    "   spec:\n",
    "     predictor:\n",
    "       model:\n",
    "         name: predictive-analytics\n",
    "         runtime: sklearn-pvc-runtime\n",
    "         storageUri: \"pvc://model-storage-pvc/predictive-analytics\"\n",
    "   ```\n",
    "\n",
    "2. **Verify deployment** using the commands above\n",
    "\n",
    "3. **Test from coordination engine**:\n",
    "   - Ensure coordination engine can call `/v1/models/predictive-analytics:predict`\n",
    "   - Verify predictions work end-to-end\n",
    "\n",
    "4. **Monitor predictions**:\n",
    "   - Check prediction accuracy over time\n",
    "   - Retrain periodically with new data\n",
    "\n",
    "### References\n",
    "\n",
    "- **Issue**: [#13 - KServe model registration fix](https://github.com/tosin2013/openshift-aiops-platform/issues/13)\n",
    "- **Module**: `src/models/predictive_analytics.py`\n",
    "- **Training script**: `src/models/train_predictive_analytics.py`\n",
    "- **Documentation**: `src/models/KSERVE_FIX_README.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

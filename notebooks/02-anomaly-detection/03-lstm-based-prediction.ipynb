{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-Based Anomaly Detection\n",
    "\n",
    "## Overview\n",
    "This notebook implements LSTM (Long Short-Term Memory) neural networks for anomaly detection using reconstruction error. LSTMs learn normal patterns and flag deviations as anomalies.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `synthetic-anomaly-generation.ipynb` (Phase 1)\n",
    "- GPU access (recommended)\n",
    "- PyTorch 2025.1, TensorFlow/Keras\n",
    "- Synthetic dataset: `/opt/app-root/src/data/processed/synthetic_anomalies.parquet`\n",
    "\n",
    "## Why We Use Synthetic Data\n",
    "\n",
    "### The Problem: Real Anomalies Are Rare\n",
    "In production OpenShift clusters:\n",
    "- Anomalies occur <1% of the time\n",
    "- Collecting 1000 labeled anomalies takes months/years\n",
    "- Different anomaly types are hard to capture\n",
    "- Can't deliberately cause failures to collect data\n",
    "\n",
    "### The Solution: Synthetic Anomalies\n",
    "We generate synthetic anomalies because:\n",
    "- ‚úÖ Create 1000+ labeled anomalies in minutes\n",
    "- ‚úÖ Control anomaly types and severity\n",
    "- ‚úÖ Ensure balanced training data (50% normal, 50% anomaly)\n",
    "- ‚úÖ Reproducible and testable\n",
    "- ‚úÖ Models trained on synthetic data generalize to real anomalies\n",
    "\n",
    "### Machine Learning Best Practice\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "## Learning Objectives\n",
    "- Build LSTM autoencoder architecture trained on synthetic data\n",
    "- Train on GPU for efficiency\n",
    "- Use reconstruction error for anomaly detection\n",
    "- Optimize hyperparameters\n",
    "- Evaluate deep learning model performance with labeled data\n",
    "\n",
    "## Key Concepts\n",
    "- **LSTM**: Recurrent neural network for sequence learning\n",
    "- **Autoencoder**: Learns compressed representation of normal data\n",
    "- **Reconstruction Error**: Difference between input and reconstructed output\n",
    "- **GPU Acceleration**: Training on NVIDIA GPUs for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Why Synthetic Data for Training?\n",
    "- **He & Garcia (2009)**: \"Learning from Imbalanced Data\" - https://ieeexplore.ieee.org/document/5128907\n",
    "- **Nikolenko (2021)**: \"Synthetic Data for Deep Learning\" - https://arxiv.org/abs/1909.11373\n",
    "- **Goldstein & Uchida (2016)**: \"Anomaly Detection with Robust Deep Autoencoders\" - https://arxiv.org/abs/1511.08747\n",
    "\n",
    "### LSTM and Deep Learning for Anomaly Detection\n",
    "- **Malhotra et al. (2016)**: \"Time Series Anomaly Detection with LSTM Networks\" - https://arxiv.org/abs/1607.00148\n",
    "- **Hochreiter & Schmidhuber (1997)**: \"Long Short-Term Memory\" - Classic LSTM paper\n",
    "- **Goodfellow et al. (2016)**: \"Deep Learning\" - Comprehensive deep learning reference\n",
    "\n",
    "### Key Takeaway\n",
    "Synthetic data provides labeled training examples that allow us to:\n",
    "1. Train deep learning models with known ground truth\n",
    "2. Evaluate performance with precision, recall, and F1 scores\n",
    "3. Ensure reproducible and testable results\n",
    "4. Build models that generalize to real-world anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Setup path for utils module - works from any directory\n",
    "def find_utils_path():\n",
    "    \"\"\"Find utils path regardless of current working directory\"\"\"\n",
    "    possible_paths = [\n",
    "        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n",
    "        Path.cwd() / 'notebooks' / 'utils',\n",
    "        Path.cwd().parent / 'utils',\n",
    "        Path('/workspace/repo/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n",
    "    ]\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists() and (p / 'common_functions.py').exists():\n",
    "            return str(p)\n",
    "    current = Path.cwd()\n",
    "    for _ in range(5):\n",
    "        utils_path = current / 'notebooks' / 'utils'\n",
    "        if utils_path.exists():\n",
    "            return str(utils_path)\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "utils_path = find_utils_path()\n",
    "if utils_path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "    print(f\"‚úÖ Utils path found: {utils_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Utils path not found - will use fallback implementations\")\n",
    "\n",
    "# Try to import common functions, with fallback\n",
    "try:\n",
    "    from common_functions import setup_environment\n",
    "    print(\"‚úÖ Common functions imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Common functions not available: {e}\")\n",
    "    def setup_environment():\n",
    "        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n",
    "        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n",
    "        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Setup environment\n",
    "env_info = setup_environment()\n",
    "logger.info(f\"Environment ready: {env_info}\")\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('/opt/app-root/src/data')\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# Use /mnt/models for persistent storage (model-storage-pvc)\n",
    "# Fallback to local for development outside cluster\n",
    "MODELS_DIR = Path('/mnt/models') if Path('/mnt/models').exists() else Path('/opt/app-root/src/models')\n",
    "\n",
    "# Create KServe-compatible subdirectory structure\n",
    "MODEL_NAME = 'lstm-predictor'  # Separate model name from anomaly-detector\n",
    "MODEL_DIR = MODELS_DIR / MODEL_NAME\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Models directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Load Data with TARGET_METRICS\n",
    "\n",
    "TARGET_METRICS = [\n",
    "    # Resource Metrics (5)\n",
    "    'node_memory_utilization',\n",
    "    'pod_cpu_usage',\n",
    "    'pod_memory_usage',\n",
    "    'alt_cpu_usage',\n",
    "    'alt_memory_usage',\n",
    "    \n",
    "    # Stability Metrics (3)\n",
    "    'container_restart_count',\n",
    "    'container_restart_rate_1h',\n",
    "    'deployment_unavailable',\n",
    "    \n",
    "    # Pod Status Metrics (4)\n",
    "    'namespace_pod_count',\n",
    "    'pods_pending',\n",
    "    'pods_running',\n",
    "    'pods_failed',\n",
    "    \n",
    "    # Storage Metrics (2)\n",
    "    'persistent_volume_usage',\n",
    "    'cluster_resource_quota',\n",
    "    \n",
    "    # Control Plane Metrics (2)\n",
    "    'apiserver_request_total',\n",
    "    'apiserver_error_rate',\n",
    "]\n",
    "\n",
    "PROMETHEUS_QUERIES = {\n",
    "    'node_memory_utilization': 'instance:node_memory_utilisation:ratio * 100',\n",
    "    'pod_cpu_usage': 'sum by (pod, namespace) (node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate)',\n",
    "    'pod_memory_usage': 'sum by (pod, namespace) (container_memory_working_set_bytes{container!=\"POD\", container!=\"\"})',\n",
    "    'alt_cpu_usage': 'sum(rate(container_cpu_usage_seconds_total{container!=\"POD\", container!=\"\"}[5m])) by (pod, namespace)',\n",
    "    'alt_memory_usage': 'sum(container_memory_rss{container!=\"POD\", container!=\"\"}) by (pod, namespace)',\n",
    "    'container_restart_count': 'sum by (pod, namespace, container) (kube_pod_container_status_restarts_total)',\n",
    "    'container_restart_rate_1h': 'sum by (pod, namespace) (increase(kube_pod_container_status_restarts_total[1h]))',\n",
    "    'deployment_unavailable': 'sum by (deployment, namespace) (kube_deployment_status_replicas_unavailable)',\n",
    "    'namespace_pod_count': 'sum by (namespace) (kube_pod_status_phase)',\n",
    "    'pods_pending': 'sum by (namespace) (kube_pod_status_phase{phase=\"Pending\"})',\n",
    "    'pods_running': 'sum by (namespace) (kube_pod_status_phase{phase=\"Running\"})',\n",
    "    'pods_failed': 'sum by (namespace) (kube_pod_status_phase{phase=\"Failed\"})',\n",
    "    'persistent_volume_usage': 'kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100',\n",
    "    'cluster_resource_quota': 'kube_resourcequota',\n",
    "    'apiserver_request_total': 'sum(rate(apiserver_request_total[5m]))',\n",
    "    'apiserver_error_rate': 'sum(rate(apiserver_request_total{code=~\"5..\"}[5m])) / sum(rate(apiserver_request_total[5m])) * 100',\n",
    "}\n",
    "\n",
    "print(f\"üìä Target metrics for LSTM: {len(TARGET_METRICS)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PROMETHEUS CLIENT\n",
    "# =============================================================================\n",
    "\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class PrometheusClient:\n",
    "    \"\"\"Client for querying Prometheus in OpenShift.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        token_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'\n",
    "        self.token = None\n",
    "        if os.path.exists(token_path):\n",
    "            with open(token_path, 'r') as f:\n",
    "                self.token = f.read().strip()\n",
    "        \n",
    "        self.base_url = 'https://prometheus-k8s.openshift-monitoring.svc.cluster.local:9091'\n",
    "        self.session = requests.Session()\n",
    "        if self.token:\n",
    "            self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n",
    "        self.session.verify = False\n",
    "        \n",
    "        import urllib3\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/api/v1/status/config\", timeout=5)\n",
    "            self.connected = response.status_code == 200\n",
    "        except:\n",
    "            self.connected = False\n",
    "    \n",
    "    def query_range(self, query, start, end, step='1m'):\n",
    "        if not self.connected:\n",
    "            return None\n",
    "        url = f\"{self.base_url}/api/v1/query_range\"\n",
    "        params = {'query': query, 'start': start, 'end': end, 'step': step}\n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_data_for_lstm(duration_hours=24, use_real_data=True):\n",
    "    \"\"\"Load time series data for LSTM training.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ LOADING DATA FOR LSTM\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    prometheus = None\n",
    "    if use_real_data:\n",
    "        prometheus = PrometheusClient()\n",
    "        print(f\"   Prometheus connected: {prometheus.connected}\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=duration_hours)\n",
    "    time_index = pd.date_range(start=start_time, end=end_time, freq='1min')\n",
    "    \n",
    "    df = pd.DataFrame(index=time_index)\n",
    "    df.index.name = 'timestamp'\n",
    "    \n",
    "    data_sources = {}\n",
    "    \n",
    "    print(f\"\\nüìä Loading {len(TARGET_METRICS)} metrics...\")\n",
    "    \n",
    "    for i, metric in enumerate(TARGET_METRICS):\n",
    "        real_data_loaded = False\n",
    "        \n",
    "        if prometheus and prometheus.connected and metric in PROMETHEUS_QUERIES:\n",
    "            query = PROMETHEUS_QUERIES[metric]\n",
    "            result = prometheus.query_range(\n",
    "                query, start_time.timestamp(), end_time.timestamp(), step='1m'\n",
    "            )\n",
    "            \n",
    "            if result and result.get('status') == 'success':\n",
    "                data = result.get('data', {}).get('result', [])\n",
    "                if data:\n",
    "                    rows = []\n",
    "                    for series in data:\n",
    "                        for ts, value in series.get('values', []):\n",
    "                            try:\n",
    "                                rows.append({\n",
    "                                    'timestamp': pd.to_datetime(ts, unit='s'),\n",
    "                                    'value': float(value) if value != 'NaN' else np.nan\n",
    "                                })\n",
    "                            except:\n",
    "                                pass\n",
    "                    \n",
    "                    if rows:\n",
    "                        metric_df = pd.DataFrame(rows)\n",
    "                        metric_series = metric_df.groupby('timestamp')['value'].mean()\n",
    "                        metric_series = metric_series.reindex(time_index, method='nearest')\n",
    "                        df[metric] = metric_series\n",
    "                        data_sources[metric] = 'REAL'\n",
    "                        real_data_loaded = True\n",
    "                        print(f\"   ‚úÖ [{i+1:2}/{len(TARGET_METRICS)}] {metric}: REAL\")\n",
    "        \n",
    "        if not real_data_loaded:\n",
    "            # Generate synthetic data with realistic patterns\n",
    "            n_points = len(time_index)\n",
    "            trend = np.linspace(0, 5, n_points)\n",
    "            seasonal = np.sin(np.linspace(0, 4*np.pi, n_points))\n",
    "            noise = np.random.normal(0, 1, n_points)\n",
    "            \n",
    "            if 'cpu' in metric.lower():\n",
    "                base = np.clip(0.3 + 0.1 * seasonal + 0.02 * noise, 0, 2)\n",
    "            elif 'memory_utilization' in metric.lower():\n",
    "                base = np.clip(60 + 10 * seasonal + 2 * noise, 0, 100)\n",
    "            elif 'memory' in metric.lower():\n",
    "                base = np.clip(2.5e8 + 2e7 * seasonal + 5e6 * noise, 1e8, 5e8)\n",
    "            elif 'restart' in metric.lower():\n",
    "                base = np.clip(10 + 0.5 * np.abs(noise), 0, 50)\n",
    "            elif 'error' in metric.lower() or 'failed' in metric.lower():\n",
    "                base = np.clip(0.01 * np.abs(noise), 0, 0.1)\n",
    "            elif 'pending' in metric.lower() or 'unavailable' in metric.lower():\n",
    "                base = np.clip(0.01 * np.abs(noise), 0, 1)\n",
    "            else:\n",
    "                base = 50 + trend + 10 * seasonal + 2 * noise\n",
    "            \n",
    "            df[metric] = base\n",
    "            data_sources[metric] = 'SYNTHETIC'\n",
    "            print(f\"   üìä [{i+1:2}/{len(TARGET_METRICS)}] {metric}: SYNTHETIC\")\n",
    "    \n",
    "    # Add labels\n",
    "    df['label'] = 0\n",
    "    n_anomalies = int(len(df) * 0.03)\n",
    "    anomaly_indices = np.random.choice(len(df), n_anomalies, replace=False)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        anomaly_metrics = np.random.choice(TARGET_METRICS, 2, replace=False)\n",
    "        for metric in anomaly_metrics:\n",
    "            if metric in df.columns:\n",
    "                std = df[metric].std()\n",
    "                if std > 0:\n",
    "                    df.iloc[idx, df.columns.get_loc(metric)] += 3.0 * std * np.random.choice([-1, 1])\n",
    "        df.iloc[idx, df.columns.get_loc('label')] = 1\n",
    "    \n",
    "    real_count = sum(1 for s in data_sources.values() if s == 'REAL')\n",
    "    print(f\"\\n‚úÖ Data loaded: {df.shape}\")\n",
    "    print(f\"   REAL: {real_count} | SYNTHETIC: {len(data_sources) - real_count}\")\n",
    "    print(f\"   Anomalies: {df['label'].sum()} ({df['label'].mean():.1%})\")\n",
    "    \n",
    "    return df, data_sources\n",
    "\n",
    "# Load the data\n",
    "df, data_sources = load_data_for_lstm(duration_hours=24, use_real_data=True)\n",
    "\n",
    "# =============================================================================\n",
    "# PREPARE FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "# Get feature columns (use TARGET_METRICS that exist in df)\n",
    "feature_cols = [col for col in df.columns if col in TARGET_METRICS]\n",
    "print(f\"\\nüìä Feature columns: {len(feature_cols)}\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = df[feature_cols].fillna(method='ffill').fillna(method='bfill').values\n",
    "y = df['label'].values\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "logger.info(f\"Data shape: {X_scaled.shape}\")\n",
    "logger.info(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"\\nüìã Sample features: {feature_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Define LSTM Autoencoder\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Autoencoder for anomaly detection.\n",
    "    \n",
    "    Learns to reconstruct normal patterns.\n",
    "    High reconstruction error = anomaly.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Encode\n",
    "        _, (hidden, cell) = self.encoder(x)\n",
    "        \n",
    "        # Create decoder input (repeat encoded representation)\n",
    "        decoder_input = hidden[-1].unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output, _ = self.decoder(decoder_input, (hidden, cell))\n",
    "        \n",
    "        # Reconstruct\n",
    "        reconstruction = self.output_layer(decoder_output)\n",
    "        \n",
    "        return reconstruction\n",
    "\n",
    "# Model configuration\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Create model\n",
    "n_features = len(feature_cols)\n",
    "model = LSTMAutoencoder(\n",
    "    input_size=n_features,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ LSTM Autoencoder created\")\n",
    "print(f\"   Input size: {n_features} features\")\n",
    "print(f\"   Hidden size: {HIDDEN_SIZE}\")\n",
    "print(f\"   Layers: {NUM_LAYERS}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"\\n   Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Create Sequences and Train Model\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"Create sequences for LSTM training.\"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "    return np.array(sequences)\n",
    "\n",
    "# =============================================================================\n",
    "# PREPARE SEQUENCES\n",
    "# =============================================================================\n",
    "\n",
    "SEQUENCE_LENGTH = 30  # 30 time steps per sequence\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîÑ PREPARING SEQUENCES FOR LSTM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create sequences\n",
    "X_sequences = create_sequences(X_scaled, SEQUENCE_LENGTH)\n",
    "y_sequences = y[SEQUENCE_LENGTH - 1:]  # Label for last point in each sequence\n",
    "\n",
    "print(f\"\\nüìä Sequence preparation:\")\n",
    "print(f\"   Original data: {X_scaled.shape}\")\n",
    "print(f\"   Sequence length: {SEQUENCE_LENGTH}\")\n",
    "print(f\"   Sequences created: {X_sequences.shape}\")\n",
    "print(f\"   Labels: {y_sequences.shape}\")\n",
    "\n",
    "# Train/test split (80/20)\n",
    "split_idx = int(len(X_sequences) * 0.8)\n",
    "X_train = X_sequences[:split_idx]\n",
    "X_test = X_sequences[split_idx:]\n",
    "y_train = y_sequences[:split_idx]\n",
    "y_test = y_sequences[split_idx:]\n",
    "\n",
    "print(f\"\\n   Train sequences: {X_train.shape[0]}\")\n",
    "print(f\"   Test sequences: {X_test.shape[0]}\")\n",
    "print(f\"   Train anomalies: {y_train.sum()} ({y_train.mean():.2%})\")\n",
    "print(f\"   Test anomalies: {y_test.sum()} ({y_test.mean():.2%})\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"\\n   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîÑ TRAINING LSTM AUTOENCODER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        X_batch = batch[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstruction = model(X_batch)\n",
    "        loss = criterion(reconstruction, X_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"   Epoch [{epoch+1:3}/{EPOCHS}] Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Final loss: {train_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Detect Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Detect Anomalies using Reconstruction Error\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç DETECTING ANOMALIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get reconstruction errors on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Reconstruct test sequences\n",
    "    reconstructed = model(X_test_t)\n",
    "    \n",
    "    # Calculate MSE per sequence\n",
    "    errors = torch.mean((X_test_t - reconstructed) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "print(f\"\\nüìä Reconstruction errors:\")\n",
    "print(f\"   Min: {errors.min():.6f}\")\n",
    "print(f\"   Max: {errors.max():.6f}\")\n",
    "print(f\"   Mean: {errors.mean():.6f}\")\n",
    "print(f\"   Std: {errors.std():.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DETERMINE THRESHOLD\n",
    "# =============================================================================\n",
    "\n",
    "# Use percentile of errors (95th percentile is common)\n",
    "PERCENTILE = 95\n",
    "threshold = np.percentile(errors, PERCENTILE)\n",
    "\n",
    "print(f\"\\nüéØ Threshold ({PERCENTILE}th percentile): {threshold:.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PREDICT ANOMALIES\n",
    "# =============================================================================\n",
    "\n",
    "lstm_preds = (errors > threshold).astype(int)\n",
    "\n",
    "print(f\"\\nüìà Predictions:\")\n",
    "print(f\"   Total test samples: {len(lstm_preds)}\")\n",
    "print(f\"   Predicted anomalies: {lstm_preds.sum()} ({lstm_preds.mean():.2%})\")\n",
    "print(f\"   Actual anomalies: {y_test.sum()} ({y_test.mean():.2%})\")\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATE PERFORMANCE\n",
    "# =============================================================================\n",
    "\n",
    "precision = precision_score(y_test, lstm_preds, zero_division=0)\n",
    "recall = recall_score(y_test, lstm_preds, zero_division=0)\n",
    "f1 = f1_score(y_test, lstm_preds, zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä LSTM PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n   Precision: {precision:.3f}\")\n",
    "print(f\"   Recall:    {recall:.3f}\")\n",
    "print(f\"   F1 Score:  {f1:.3f}\")\n",
    "\n",
    "# Also get full dataset predictions for saving\n",
    "with torch.no_grad():\n",
    "    X_full_t = torch.FloatTensor(X_sequences).to(device)\n",
    "    reconstructed_full = model(X_full_t)\n",
    "    errors_full = torch.mean((X_full_t - reconstructed_full) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "lstm_preds_full = (errors_full > threshold).astype(int)\n",
    "\n",
    "print(f\"\\n   Full dataset:\")\n",
    "print(f\"   Total: {len(lstm_preds_full)} | Anomalies: {lstm_preds_full.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Save LSTM Model for KServe\n",
    "\n",
    "import pickle\n",
    "\n",
    "class LSTMPipeline:\n",
    "    \"\"\"\n",
    "    Wrapper combining scaler + LSTM model for KServe deployment.\n",
    "    KServe expects exactly ONE .pkl file in the model directory.\n",
    "    \"\"\"\n",
    "    def __init__(self, scaler, model_state_dict, model_config, threshold, feature_names, device='cpu'):\n",
    "        self.scaler = scaler\n",
    "        self.model_state_dict = model_state_dict\n",
    "        self.model_config = model_config\n",
    "        self.threshold = threshold\n",
    "        self.feature_names = feature_names\n",
    "        self.device = device\n",
    "        self._model = None\n",
    "    \n",
    "    def _get_model(self):\n",
    "        \"\"\"Lazy load model from state dict.\"\"\"\n",
    "        if self._model is None:\n",
    "            self._model = LSTMAutoencoder(**self.model_config)\n",
    "            self._model.load_state_dict(self.model_state_dict)\n",
    "            self._model.to(self.device)\n",
    "            self._model.eval()\n",
    "        return self._model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict anomalies.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features (numpy array, shape: n_samples x n_features)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: 1 for normal, -1 for anomaly (sklearn convention)\n",
    "        \"\"\"\n",
    "        # Scale input\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Create single sequence (use last SEQUENCE_LENGTH points)\n",
    "        seq_len = 30  # Must match training\n",
    "        if len(X_scaled) < seq_len:\n",
    "            # Pad with zeros if not enough data\n",
    "            padding = np.zeros((seq_len - len(X_scaled), X_scaled.shape[1]))\n",
    "            X_scaled = np.vstack([padding, X_scaled])\n",
    "        \n",
    "        X_seq = X_scaled[-seq_len:].reshape(1, seq_len, -1)\n",
    "        \n",
    "        # Get model and predict\n",
    "        model = self._get_model()\n",
    "        X_tensor = torch.FloatTensor(X_seq).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reconstructed = model(X_tensor)\n",
    "            error = torch.mean((X_tensor - reconstructed) ** 2).item()\n",
    "        \n",
    "        # Return -1 for anomaly, 1 for normal (sklearn convention)\n",
    "        return -1 if error > self.threshold else 1\n",
    "    \n",
    "    def predict_batch(self, X_sequences):\n",
    "        \"\"\"Predict on batch of sequences.\"\"\"\n",
    "        model = self._get_model()\n",
    "        X_tensor = torch.FloatTensor(X_sequences).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reconstructed = model(X_tensor)\n",
    "            errors = torch.mean((X_tensor - reconstructed) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "        \n",
    "        return np.where(errors > self.threshold, -1, 1)\n",
    "    \n",
    "    def get_reconstruction_error(self, X_sequences):\n",
    "        \"\"\"Get reconstruction errors for analysis.\"\"\"\n",
    "        model = self._get_model()\n",
    "        X_tensor = torch.FloatTensor(X_sequences).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reconstructed = model(X_tensor)\n",
    "            errors = torch.mean((X_tensor - reconstructed) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "        \n",
    "        return errors\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE AND SAVE PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üíæ SAVING LSTM MODEL FOR KSERVE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = LSTMPipeline(\n",
    "    scaler=scaler,\n",
    "    model_state_dict=model.cpu().state_dict(),  # Move to CPU for saving\n",
    "    model_config={\n",
    "        'input_size': n_features,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'dropout': DROPOUT\n",
    "    },\n",
    "    threshold=threshold,\n",
    "    feature_names=feature_cols,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "print(f\"\\nüì¶ LSTMPipeline created:\")\n",
    "print(f\"   Features: {len(feature_cols)}\")\n",
    "print(f\"   Threshold: {threshold:.6f}\")\n",
    "print(f\"   Hidden size: {HIDDEN_SIZE}\")\n",
    "\n",
    "# Clean up old files\n",
    "for old_file in MODEL_DIR.glob('*.pkl'):\n",
    "    old_file.unlink()\n",
    "    print(f\"   üóëÔ∏è  Removed: {old_file.name}\")\n",
    "for old_file in MODEL_DIR.glob('*.pt'):\n",
    "    old_file.unlink()\n",
    "    print(f\"   üóëÔ∏è  Removed: {old_file.name}\")\n",
    "\n",
    "# Save single .pkl file (KServe requirement)\n",
    "model_path = MODEL_DIR / 'model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved: {model_path}\")\n",
    "print(f\"   Size: {model_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Verify single file\n",
    "pkl_files = list(MODEL_DIR.glob('*.pkl'))\n",
    "assert len(pkl_files) == 1, f\"Expected 1 .pkl file, found {len(pkl_files)}\"\n",
    "print(f\"   Files in directory: {len(pkl_files)} ‚úì\")\n",
    "\n",
    "# =============================================================================\n",
    "# UPLOAD TO S3\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    from common_functions import upload_model_to_s3, test_s3_connection\n",
    "    \n",
    "    if test_s3_connection():\n",
    "        upload_model_to_s3(\n",
    "            str(model_path),\n",
    "            s3_key=f'models/anomaly-detection/{MODEL_NAME}/model.pkl'\n",
    "        )\n",
    "        print(f\"\\n‚òÅÔ∏è  Uploaded to S3: models/anomaly-detection/{MODEL_NAME}/model.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  S3 upload skipped: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'actual': y_sequences,\n",
    "    'lstm_pred': lstm_preds_full,\n",
    "    'reconstruction_error': errors_full\n",
    "})\n",
    "results_df.to_parquet(PROCESSED_DIR / 'lstm_predictions.parquet')\n",
    "print(f\"\\nüíæ Predictions saved: {PROCESSED_DIR / 'lstm_predictions.parquet'}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ LSTM MODEL SAVE COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n   Model name: {MODEL_NAME}\")\n",
    "print(f\"   Model path: {model_path}\")\n",
    "print(f\"   Performance: P={precision:.3f} R={recall:.3f} F1={f1:.3f}\")\n",
    "print(f\"\\n   Deploy to KServe with: storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Validation\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify model file\n",
    "assert (MODEL_DIR / 'model.pkl').exists(), \"Model not saved!\"\n",
    "print(f\"   ‚úÖ Model file exists\")\n",
    "\n",
    "# Verify predictions\n",
    "assert (PROCESSED_DIR / 'lstm_predictions.parquet').exists(), \"Predictions not saved!\"\n",
    "print(f\"   ‚úÖ Predictions saved\")\n",
    "\n",
    "# Test loading the pipeline\n",
    "with open(MODEL_DIR / 'model.pkl', 'rb') as f:\n",
    "    loaded_pipeline = pickle.load(f)\n",
    "\n",
    "print(f\"   ‚úÖ Pipeline loads correctly\")\n",
    "print(f\"   ‚úÖ Features: {len(loaded_pipeline.feature_names)}\")\n",
    "print(f\"   ‚úÖ Threshold: {loaded_pipeline.threshold:.6f}\")\n",
    "\n",
    "# Test prediction\n",
    "test_errors = loaded_pipeline.get_reconstruction_error(X_sequences[:10])\n",
    "print(f\"   ‚úÖ Prediction works: {len(test_errors)} errors computed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL VALIDATIONS PASSED\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Synthetic anomalies from `synthetic-anomaly-generation.ipynb`\n",
    "- **Output**: LSTM model for `ensemble-anomaly-methods.ipynb`\n",
    "- **Deployment**: Model can be exported to KServe for production\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Compare LSTM performance with ARIMA and Prophet\n",
    "2. Proceed to `ensemble-anomaly-methods.ipynb`\n",
    "3. Combine all methods for best performance\n",
    "4. Deploy ensemble to coordination engine\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [PyTorch LSTM Documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
    "- [Autoencoder Anomaly Detection](https://en.wikipedia.org/wiki/Autoencoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

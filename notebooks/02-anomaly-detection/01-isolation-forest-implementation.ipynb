{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest Anomaly Detection for Self-Healing Platform\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates implementing Isolation Forest for anomaly detection in OpenShift metrics. Isolation Forest is particularly effective for detecting anomalies in high-dimensional data without requiring labeled training data.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `synthetic-anomaly-generation.ipynb` (Phase 1)\n",
    "- PyTorch workbench environment with scikit-learn\n",
    "- Synthetic dataset: `/opt/app-root/src/data/processed/synthetic_anomalies.parquet`\n",
    "\n",
    "## Why We Use Synthetic Data\n",
    "\n",
    "### The Problem: Real Anomalies Are Rare\n",
    "In production OpenShift clusters:\n",
    "- Anomalies occur <1% of the time\n",
    "- Collecting 1000 labeled anomalies takes months/years\n",
    "- Different anomaly types are hard to capture\n",
    "- Can't deliberately cause failures to collect data\n",
    "\n",
    "### The Solution: Synthetic Anomalies\n",
    "We generate synthetic anomalies because:\n",
    "- ‚úÖ Create 1000+ labeled anomalies in minutes\n",
    "- ‚úÖ Control anomaly types and severity\n",
    "- ‚úÖ Ensure balanced training data (50% normal, 50% anomaly)\n",
    "- ‚úÖ Reproducible and testable\n",
    "- ‚úÖ Models trained on synthetic data generalize to real anomalies\n",
    "\n",
    "### Machine Learning Best Practice\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "## Expected Outcomes\n",
    "- Train Isolation Forest model on synthetic anomalies\n",
    "- Evaluate model performance (Precision, Recall, F1)\n",
    "- Save trained model for integration with coordination engine\n",
    "- Generate anomaly detection pipeline for real-time use\n",
    "\n",
    "## References\n",
    "- ADR-002: Hybrid Deterministic-AI Self-Healing Approach\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [Isolation Forest Paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf) - Liu, Ting & Zhou (2008)\n",
    "- [Learning from Imbalanced Data](https://ieeexplore.ieee.org/document/5128907) - He & Garcia (2009)\n",
    "- [Anomaly Detection with Robust Deep Autoencoders](https://arxiv.org/abs/1511.08747) - Goldstein & Uchida (2016)"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport sys\nimport os\nfrom pathlib import Path\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"‚úÖ Utils path found: {utils_path}\")\nelse:\n    print(\"‚ö†Ô∏è Utils path not found - will use fallback implementations\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine learning libraries\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline  # ‚ú® Added for KServe compatibility\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import (\n        setup_environment, print_environment_info,\n        generate_synthetic_timeseries, validate_data_quality,\n        plot_metric_overview, save_processed_data, load_processed_data\n    )\n    print(\"‚úÖ Common functions imported\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Common functions not available: {e}\")\n    print(\"   Using minimal fallback implementations\")\n    \n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models/anomaly-detection', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n    \n    def print_environment_info(env_info):\n        print(f\"üìÅ Data dir: {env_info.get('data_dir', 'N/A')}\")\n    \n    def generate_synthetic_timeseries(metric_name, duration_hours=24, interval_minutes=1, \n                                      add_anomalies=True, anomaly_probability=0.02):\n        num_points = int(duration_hours * 60 / interval_minutes)\n        timestamps = pd.date_range(end=datetime.now(), periods=num_points, freq=f'{interval_minutes}min')\n        values = np.random.normal(50, 10, num_points)\n        if add_anomalies:\n            anomaly_idx = np.random.choice(num_points, int(num_points * anomaly_probability), replace=False)\n            values[anomaly_idx] *= np.random.choice([0.3, 3.0], len(anomaly_idx))\n        df = pd.DataFrame({'timestamp': timestamps, 'value': values, 'metric': metric_name, 'is_anomaly': False})\n        if add_anomalies:\n            df.loc[anomaly_idx, 'is_anomaly'] = True\n        return df\n    \n    def save_processed_data(data, filename):\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        filepath = f'/opt/app-root/src/data/processed/{filename}'\n        if hasattr(data, 'to_parquet'):\n            data.to_parquet(filepath)\n        print(f\"üíæ Saved: {filepath}\")\n\nprint(\"‚úÖ Libraries imported successfully\")\nprint(f\"üî¨ Scikit-learn available with Pipeline support\")\nprint(f\"üìä Pandas version: {pd.__version__}\")",
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "env_info = setup_environment()\n",
    "print_environment_info(env_info)\n",
    "\n",
    "# Configuration for Isolation Forest\n",
    "ISOLATION_FOREST_CONFIG = {\n",
    "    'contamination': 0.05,  # Expected proportion of anomalies\n",
    "    'n_estimators': 200,    # Number of trees\n",
    "    'max_samples': 'auto',  # Number of samples to draw\n",
    "    'max_features': 1.0,    # Number of features to draw\n",
    "    'random_state': 42      # For reproducibility\n",
    "}\n",
    "\n",
    "# Metrics to focus on for anomaly detection\n",
    "TARGET_METRICS = [\n",
    "    'node_cpu_utilization',\n",
    "    'node_memory_utilization',\n",
    "    'pod_cpu_usage',\n",
    "    'pod_memory_usage',\n",
    "    'container_restart_count'\n",
    "]\n",
    "\n",
    "print(f\"üéØ Target metrics for anomaly detection: {len(TARGET_METRICS)}\")\n",
    "print(f\"üå≤ Isolation Forest configuration: {ISOLATION_FOREST_CONFIG['n_estimators']} trees\")"
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Load Synthetic Anomalies for Training\n",
    "\n",
    "We load synthetic anomalies from Phase 1 (`synthetic-anomaly-generation.ipynb`) for training.\n",
    "\n",
    "**Why Synthetic Data?**\n",
    "- Real anomalies are rare (<1% in production clusters)\n",
    "- Synthetic data provides labeled training examples\n",
    "- Models learn general patterns, not memorize specific examples\n",
    "- Balanced dataset (50% normal, 50% anomaly) improves performance\n",
    "- Reproducible and testable\n",
    "\n",
    "**Machine Learning Best Practice:**\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "**References:**\n",
    "- He & Garcia (2009): \"Learning from Imbalanced Data\" - https://ieeexplore.ieee.org/document/5128907\n",
    "- Nikolenko (2021): \"Synthetic Data for Deep Learning\" - https://arxiv.org/abs/1909.11373\n",
    "- Goldstein & Uchida (2016): \"Anomaly Detection with Robust Deep Autoencoders\" - https://arxiv.org/abs/1511.08747"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_anomaly_detection_data(duration_hours=48):\n",
    "    \"\"\"\n",
    "    Generate and prepare data for anomaly detection training\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Preparing anomaly detection dataset...\")\n",
    "    \n",
    "    # Generate synthetic data for each target metric\n",
    "    all_data = {}\n",
    "    \n",
    "    for metric in TARGET_METRICS:\n",
    "        print(f\"  üìä Generating {metric}...\")\n",
    "        df = generate_synthetic_timeseries(\n",
    "            metric_name=metric,\n",
    "            duration_hours=duration_hours,\n",
    "            interval_minutes=1,\n",
    "            add_anomalies=True,\n",
    "            anomaly_probability=0.03  # 3% anomalies\n",
    "        )\n",
    "        all_data[metric] = df\n",
    "        print(f\"    ‚úÖ {len(df)} points, {df['is_anomaly'].sum()} anomalies\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Generate training data\n",
    "training_data = prepare_anomaly_detection_data(duration_hours=48)\n",
    "\n",
    "# Display summary\n",
    "total_points = sum(len(df) for df in training_data.values())\n",
    "total_anomalies = sum(df['is_anomaly'].sum() for df in training_data.values())\n",
    "print(f\"\\nüìà Dataset Summary:\")\n",
    "print(f\"  Total data points: {total_points:,}\")\n",
    "print(f\"  Total anomalies: {total_anomalies:,} ({total_anomalies/total_points:.2%})\")\n",
    "print(f\"  Metrics: {len(training_data)}\")"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_matrix(data_dict):\n",
    "    \"\"\"\n",
    "    Create feature matrix for anomaly detection\n",
    "    \"\"\"\n",
    "    print(\"üîß Creating feature matrix...\")\n",
    "    \n",
    "    # Align all time series to common timestamps\n",
    "    # Find common time range\n",
    "    min_start = max(df['timestamp'].min() for df in data_dict.values())\n",
    "    max_end = min(df['timestamp'].max() for df in data_dict.values())\n",
    "    \n",
    "    print(f\"  üìÖ Time range: {min_start} to {max_end}\")\n",
    "    \n",
    "    # Create common time index\n",
    "    time_index = pd.date_range(start=min_start, end=max_end, freq='1min')\n",
    "    \n",
    "    # Build feature matrix\n",
    "    features = pd.DataFrame(index=time_index)\n",
    "    labels = pd.Series(index=time_index, dtype=bool, name='is_anomaly')\n",
    "    \n",
    "    for metric_name, df in data_dict.items():\n",
    "        # Resample to common time index\n",
    "        df_resampled = df.set_index('timestamp').reindex(time_index, method='nearest')\n",
    "        \n",
    "        # Add basic features\n",
    "        features[f'{metric_name}_value'] = df_resampled['value']\n",
    "        \n",
    "        # Add rolling statistics (5-minute windows)\n",
    "        features[f'{metric_name}_mean_5m'] = df_resampled['value'].rolling('5min').mean()\n",
    "        features[f'{metric_name}_std_5m'] = df_resampled['value'].rolling('5min').std()\n",
    "        features[f'{metric_name}_min_5m'] = df_resampled['value'].rolling('5min').min()\n",
    "        features[f'{metric_name}_max_5m'] = df_resampled['value'].rolling('5min').max()\n",
    "        \n",
    "        # Add lag features\n",
    "        features[f'{metric_name}_lag_1'] = df_resampled['value'].shift(1)\n",
    "        features[f'{metric_name}_lag_5'] = df_resampled['value'].shift(5)\n",
    "        \n",
    "        # Add rate of change\n",
    "        features[f'{metric_name}_diff'] = df_resampled['value'].diff()\n",
    "        features[f'{metric_name}_pct_change'] = df_resampled['value'].pct_change()\n",
    "        \n",
    "        # Combine anomaly labels (any metric anomaly = overall anomaly)\n",
    "        metric_anomalies = df_resampled['is_anomaly'].fillna(False)\n",
    "        labels = labels | metric_anomalies\n",
    "    \n",
    "    # Fill missing values\n",
    "    features = features.ffill().bfill()\n",
    "    labels = labels.fillna(False)\n",
    "    \n",
    "    # Replace infinity values with 0 and remaining NaN with 0\n",
    "    features = features.replace([np.inf, -np.inf], 0)\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    print(f\"  ‚úÖ Feature matrix: {features.shape}\")\n",
    "    print(f\"  üè∑Ô∏è Anomaly labels: {labels.sum()} anomalies ({labels.mean():.2%})\")\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Create feature matrix\n",
    "X, y = create_feature_matrix(training_data)\n",
    "\n",
    "print(f\"\\nüìä Feature Engineering Complete:\")\n",
    "print(f\"  Features: {X.shape[1]} columns\")\n",
    "print(f\"  Samples: {X.shape[0]:,} rows\")\n",
    "print(f\"  Anomaly rate: {y.mean():.2%}\")"
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Train Isolation Forest model and evaluate its performance."
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split data for training and testing\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(f\"üìä Data Split:\")\nprint(f\"  Training: {X_train.shape[0]:,} samples\")\nprint(f\"  Testing: {X_test.shape[0]:,} samples\")\nprint(f\"  Training anomalies: {y_train.sum()} ({y_train.mean():.2%})\")\nprint(f\"  Testing anomalies: {y_test.sum()} ({y_test.mean():.2%})\")\n\n# ‚ú® Create sklearn Pipeline combining scaler + model (KServe compatible)\nprint(\"\\nüîß Creating Isolation Forest pipeline (scaler + model)...\")\nprint(\"   This creates a SINGLE .pkl file for KServe deployment\")\n\nisolation_forest_pipeline = Pipeline([\n    ('scaler', RobustScaler()),  # More robust to outliers than StandardScaler\n    ('isolation_forest', IsolationForest(**ISOLATION_FOREST_CONFIG))\n])\n\nprint(\"‚úÖ Pipeline created (scaler + Isolation Forest)\")",
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train Isolation Forest Pipeline\nprint(\"üå≤ Training Isolation Forest pipeline...\")\nprint(\"   Pipeline automatically handles: scaler.fit_transform() ‚Üí model.fit()\")\n\n# Fit pipeline on training data\n# Pipeline will:\n#   1. Fit scaler on X_train and transform it\n#   2. Fit Isolation Forest on scaled data\nisolation_forest_pipeline.fit(X_train)\n\nprint(\"‚úÖ Training complete\")\n\n# Make predictions using pipeline\n# Pipeline will:\n#   1. Transform data using fitted scaler\n#   2. Predict using fitted model\nprint(\"üîÆ Making predictions...\")\ny_pred_train = isolation_forest_pipeline.predict(X_train)\ny_pred_test = isolation_forest_pipeline.predict(X_test)\n\n# Get anomaly scores\ntrain_scores = isolation_forest_pipeline.decision_function(X_train)\ntest_scores = isolation_forest_pipeline.decision_function(X_test)\n\n# Convert predictions to binary (1 = normal, -1 = anomaly)\ny_pred_train_binary = (y_pred_train == -1)\ny_pred_test_binary = (y_pred_test == -1)\n\nprint(f\"  Training predictions: {y_pred_train_binary.sum()} anomalies detected\")\nprint(f\"  Testing predictions: {y_pred_test_binary.sum()} anomalies detected\")\nprint(\"\\n‚úÖ Pipeline handles scaling automatically - no separate scaler needed!\")",
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "print(\"üìä Model Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training set performance\n",
    "print(\"\\nüèãÔ∏è Training Set Performance:\")\n",
    "print(classification_report(y_train, y_pred_train_binary, \n",
    "                          target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Test set performance\n",
    "print(\"\\nüß™ Test Set Performance:\")\n",
    "print(classification_report(y_test, y_pred_test_binary, \n",
    "                          target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training confusion matrix\n",
    "cm_train = confusion_matrix(y_train, y_pred_train_binary)\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Anomaly'], \n",
    "            yticklabels=['Normal', 'Anomaly'], ax=axes[0])\n",
    "axes[0].set_title('Training Set Confusion Matrix')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Test confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test_binary)\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Anomaly'], \n",
    "            yticklabels=['Normal', 'Anomaly'], ax=axes[1])\n",
    "axes[1].set_title('Test Set Confusion Matrix')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis and Visualization"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze anomaly scores distribution\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Isolation Forest Analysis', fontsize=16, fontweight='bold')\n\n# Score distribution\naxes[0, 0].hist(train_scores[~y_train], bins=50, alpha=0.7, label='Normal', density=True)\naxes[0, 0].hist(train_scores[y_train], bins=50, alpha=0.7, label='Anomaly', density=True)\naxes[0, 0].set_title('Anomaly Score Distribution (Training)')\naxes[0, 0].set_xlabel('Anomaly Score')\naxes[0, 0].set_ylabel('Density')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Score vs time (sample)\nsample_size = min(1000, len(test_scores))\nsample_indices = np.random.choice(len(test_scores), sample_size, replace=False)\nsample_indices = np.sort(sample_indices)\n\naxes[0, 1].plot(sample_indices, test_scores[sample_indices], 'b-', alpha=0.7, linewidth=1)\nanomaly_indices = sample_indices[y_test.iloc[sample_indices]]\nif len(anomaly_indices) > 0:\n    axes[0, 1].scatter(anomaly_indices, test_scores[anomaly_indices], \n                      color='red', s=30, alpha=0.8, label='True Anomalies')\naxes[0, 1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\naxes[0, 1].set_title('Anomaly Scores Over Time (Test Sample)')\naxes[0, 1].set_xlabel('Sample Index')\naxes[0, 1].set_ylabel('Anomaly Score')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Feature importance (using PCA to visualize)\npca = PCA(n_components=2)\nX_test_pca = pca.fit_transform(X_test)\n\nnormal_mask = ~y_test\nanomaly_mask = y_test\n\naxes[1, 0].scatter(X_test_pca[normal_mask, 0], X_test_pca[normal_mask, 1], \n                  c='blue', alpha=0.6, s=20, label='Normal')\naxes[1, 0].scatter(X_test_pca[anomaly_mask, 0], X_test_pca[anomaly_mask, 1], \n                  c='red', alpha=0.8, s=30, label='Anomaly')\naxes[1, 0].set_title('PCA Visualization (Test Set)')\naxes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\naxes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Model performance metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n\nprecision = precision_score(y_test, y_pred_test_binary)\nrecall = recall_score(y_test, y_pred_test_binary)\nf1 = f1_score(y_test, y_pred_test_binary)\n\n# Convert scores to probabilities for AUC calculation\ntest_scores_prob = (test_scores - test_scores.min()) / (test_scores.max() - test_scores.min())\nauc = roc_auc_score(y_test, 1 - test_scores_prob)  # Invert because lower scores = more anomalous\n\nmetrics_text = f\"\"\"\nModel Performance Metrics:\n\nPrecision: {precision:.3f}\nRecall: {recall:.3f}\nF1-Score: {f1:.3f}\nAUC-ROC: {auc:.3f}\n\nConfiguration:\nTrees: {ISOLATION_FOREST_CONFIG['n_estimators']}\nContamination: {ISOLATION_FOREST_CONFIG['contamination']}\nFeatures: {X.shape[1]}\n\nData:\nTraining: {X_train.shape[0]:,}\nTesting: {X_test.shape[0]:,}\n\"\"\"\n\naxes[1, 1].text(0.05, 0.95, metrics_text, transform=axes[1, 1].transAxes, \n               fontsize=10, verticalalignment='top',\n               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\naxes[1, 1].set_title('Model Summary')\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüéØ Model Performance Summary:\")\nprint(f\"  Precision: {precision:.3f}\")\nprint(f\"  Recall: {recall:.3f}\")\nprint(f\"  F1-Score: {f1:.3f}\")\nprint(f\"  AUC-ROC: {auc:.3f}\")",
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "source": "## Save Model and Upload to S3",
   "metadata": {},
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "source": "# Save pipeline model to persistent storage\n# Use /mnt/models for persistent storage (model-storage-pvc)\n# Fallback to local for development outside cluster\nMODELS_DIR = Path('/mnt/models') if Path('/mnt/models').exists() else Path('/opt/app-root/src/models')\n\n# Create KServe-compatible subdirectory structure\nMODEL_NAME = 'anomaly-detector'\nMODEL_DIR = MODELS_DIR / MODEL_NAME\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\n\n# Save with KServe expected filename\nmodel_path = MODEL_DIR / 'model.pkl'\n\n# Migration: Move old flat file if exists\nold_path = MODELS_DIR / 'anomaly-detector.pkl'\nif old_path.exists() and not model_path.exists():\n    import shutil\n    shutil.move(str(old_path), str(model_path))\n    print(f\"üîÑ Migrated model from {old_path} to {model_path}\")\n\n# ‚ú® Save SINGLE pipeline file (KServe compatible)\n# KServe sklearn server expects model at: /mnt/models/anomaly-detector/model.pkl\njoblib.dump(isolation_forest_pipeline, model_path)\nprint(f\"üíæ Saved Isolation Forest pipeline to: {model_path}\")\nprint(f\"   ‚úÖ KServe-compatible path: {MODEL_NAME}/model.pkl\")\nprint(f\"   ‚úÖ Single .pkl file (scaler + model combined)\")\n\n# Upload model to S3 for persistent storage\ntry:\n    from common_functions import upload_model_to_s3, test_s3_connection\n    \n    if test_s3_connection():\n        upload_model_to_s3(\n            str(model_path),\n            s3_key='models/anomaly-detection/anomaly-detector/model.pkl'\n        )\n        print(f\"‚òÅÔ∏è  Uploaded to S3: models/anomaly-detection/anomaly-detector/model.pkl\")\n    else:\n        print(\"‚ö†Ô∏è S3 not available - model saved locally only\")\nexcept ImportError:\n    print(\"‚ö†Ô∏è S3 functions not available - model saved locally only\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è S3 upload failed (non-critical): {e}\")\n\n# Verify model saved\nassert model_path.exists(), \"Pipeline model not saved\"\nprint(\"\\n‚úÖ Model pipeline saved successfully\")\nprint(f\"   Path: {model_path}\")\nprint(f\"   Size: {model_path.stat().st_size / 1024:.2f} KB\")\n\n# Clean up old separate model/scaler files if they exist\nold_model = MODELS_DIR / 'isolation_forest_model.pkl'\nold_scaler = MODELS_DIR / 'isolation_forest_scaler.pkl'\nfor old_file in [old_model, old_scaler]:\n    if old_file.exists():\n        old_file.unlink()\n        print(f\"üóëÔ∏è  Removed old file: {old_file.name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

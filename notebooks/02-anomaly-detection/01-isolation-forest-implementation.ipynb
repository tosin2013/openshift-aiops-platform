{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Isolation Forest Anomaly Detection for Self-Healing Platform\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates implementing Isolation Forest for anomaly detection in OpenShift metrics. Isolation Forest is particularly effective for detecting anomalies in high-dimensional data without requiring labeled training data.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `synthetic-anomaly-generation.ipynb` (Phase 1)\n",
    "- PyTorch workbench environment with scikit-learn\n",
    "- Synthetic dataset: `/opt/app-root/src/data/processed/synthetic_anomalies.parquet`\n",
    "\n",
    "## Why We Use Synthetic Data\n",
    "\n",
    "### The Problem: Real Anomalies Are Rare\n",
    "In production OpenShift clusters:\n",
    "- Anomalies occur <1% of the time\n",
    "- Collecting 1000 labeled anomalies takes months/years\n",
    "- Different anomaly types are hard to capture\n",
    "- Can't deliberately cause failures to collect data\n",
    "\n",
    "### The Solution: Synthetic Anomalies\n",
    "We generate synthetic anomalies because:\n",
    "- ‚úÖ Create 1000+ labeled anomalies in minutes\n",
    "- ‚úÖ Control anomaly types and severity\n",
    "- ‚úÖ Ensure balanced training data (50% normal, 50% anomaly)\n",
    "- ‚úÖ Reproducible and testable\n",
    "- ‚úÖ Models trained on synthetic data generalize to real anomalies\n",
    "\n",
    "### Machine Learning Best Practice\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "## Expected Outcomes\n",
    "- Train Isolation Forest model on synthetic anomalies\n",
    "- Evaluate model performance (Precision, Recall, F1)\n",
    "- Save trained model for integration with coordination engine\n",
    "- Generate anomaly detection pipeline for real-time use\n",
    "\n",
    "## References\n",
    "- ADR-002: Hybrid Deterministic-AI Self-Healing Approach\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [Isolation Forest Paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf) - Liu, Ting & Zhou (2008)\n",
    "- [Learning from Imbalanced Data](https://ieeexplore.ieee.org/document/5128907) - He & Garcia (2009)\n",
    "- [Anomaly Detection with Robust Deep Autoencoders](https://arxiv.org/abs/1511.08747) - Goldstein & Uchida (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path for utils module - works from any directory\n",
    "def find_utils_path():\n",
    "    \"\"\"Find utils path regardless of current working directory\"\"\"\n",
    "    possible_paths = [\n",
    "        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n",
    "        Path.cwd() / 'notebooks' / 'utils',\n",
    "        Path.cwd().parent / 'utils',\n",
    "        Path('/workspace/repo/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n",
    "    ]\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists() and (p / 'common_functions.py').exists():\n",
    "            return str(p)\n",
    "    current = Path.cwd()\n",
    "    for _ in range(5):\n",
    "        utils_path = current / 'notebooks' / 'utils'\n",
    "        if utils_path.exists():\n",
    "            return str(utils_path)\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "utils_path = find_utils_path()\n",
    "if utils_path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "    print(f\"‚úÖ Utils path found: {utils_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Utils path not found - will use fallback implementations\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline  # ‚ú® Added for KServe compatibility\n",
    "\n",
    "# Try to import common functions, with fallback\n",
    "try:\n",
    "    from common_functions import (\n",
    "        setup_environment, print_environment_info,\n",
    "        generate_synthetic_timeseries, validate_data_quality,\n",
    "        plot_metric_overview, save_processed_data, load_processed_data\n",
    "    )\n",
    "    print(\"‚úÖ Common functions imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Common functions not available: {e}\")\n",
    "    print(\"   Using minimal fallback implementations\")\n",
    "    \n",
    "    def setup_environment():\n",
    "        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n",
    "        os.makedirs('/opt/app-root/src/models/anomaly-detection', exist_ok=True)\n",
    "        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n",
    "    \n",
    "    def print_environment_info(env_info):\n",
    "        print(f\"üìÅ Data dir: {env_info.get('data_dir', 'N/A')}\")\n",
    "    \n",
    "    def generate_synthetic_timeseries(metric_name, duration_hours=24, interval_minutes=1, \n",
    "                                      add_anomalies=True, anomaly_probability=0.02):\n",
    "        num_points = int(duration_hours * 60 / interval_minutes)\n",
    "        timestamps = pd.date_range(end=datetime.now(), periods=num_points, freq=f'{interval_minutes}min')\n",
    "        values = np.random.normal(50, 10, num_points)\n",
    "        if add_anomalies:\n",
    "            anomaly_idx = np.random.choice(num_points, int(num_points * anomaly_probability), replace=False)\n",
    "            values[anomaly_idx] *= np.random.choice([0.3, 3.0], len(anomaly_idx))\n",
    "        df = pd.DataFrame({'timestamp': timestamps, 'value': values, 'metric': metric_name, 'is_anomaly': False})\n",
    "        if add_anomalies:\n",
    "            df.loc[anomaly_idx, 'is_anomaly'] = True\n",
    "        return df\n",
    "    \n",
    "    def save_processed_data(data, filename):\n",
    "        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n",
    "        filepath = f'/opt/app-root/src/data/processed/{filename}'\n",
    "        if hasattr(data, 'to_parquet'):\n",
    "            data.to_parquet(filepath)\n",
    "        print(f\"üíæ Saved: {filepath}\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üî¨ Scikit-learn available with Pipeline support\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "env_info = setup_environment()\n",
    "print_environment_info(env_info)\n",
    "\n",
    "# Configuration for Isolation Forest\n",
    "ISOLATION_FOREST_CONFIG = {\n",
    "    'contamination': 0.05,  # Expected proportion of anomalies\n",
    "    'n_estimators': 200,    # Number of trees\n",
    "    'max_samples': 'auto',  # Number of samples to draw\n",
    "    'max_features': 1.0,    # Number of features to draw\n",
    "    'random_state': 42      # For reproducibility\n",
    "}\n",
    "\n",
    "TARGET_METRICS = [\n",
    "    # Resource Metrics (5) - CPU and Memory\n",
    "    'node_memory_utilization',      # ‚úÖ 91 points - Node memory %\n",
    "    'pod_cpu_usage',                # ‚úÖ 5,076 points - Pod CPU cores\n",
    "    'pod_memory_usage',             # ‚úÖ 5,071 points - Pod memory bytes\n",
    "    'alt_cpu_usage',                # ‚úÖ 5,077 points - Alternative CPU metric\n",
    "    'alt_memory_usage',             # ‚úÖ 5,071 points - Alternative memory metric\n",
    "    \n",
    "    # Stability Metrics (3) - Restarts and Availability\n",
    "    'container_restart_count',      # ‚úÖ 10,668 points - Total restarts\n",
    "    'container_restart_rate_1h',    # ‚úÖ 7,027 points - Restarts per hour\n",
    "    'deployment_unavailable',       # ‚úÖ 2,392 points - Unavailable replicas\n",
    "    \n",
    "    # Pod Status Metrics (4) - Pod lifecycle\n",
    "    'namespace_pod_count',          # ‚úÖ 962 points - Pods per namespace\n",
    "    'pods_pending',                 # ‚úÖ 962 points - Pending pods\n",
    "    'pods_running',                 # ‚úÖ 962 points - Running pods\n",
    "    'pods_failed',                  # ‚úÖ 962 points - Failed pods\n",
    "    \n",
    "    # Storage Metrics (2) - PVC and quotas\n",
    "    'persistent_volume_usage',      # ‚úÖ 130 points - PVC usage %\n",
    "    'cluster_resource_quota',       # ‚úÖ 130 points - Resource quotas\n",
    "    \n",
    "    # Control Plane Metrics (2) - API server health\n",
    "    'apiserver_request_total',      # ‚úÖ 13 points - API request rate\n",
    "    'apiserver_error_rate',         # ‚úÖ 13 points - API error rate %\n",
    "]\n",
    "\n",
    "# PromQL queries for each metric (for reference)\n",
    "PROMETHEUS_QUERIES = {\n",
    "    'node_memory_utilization': 'instance:node_memory_utilisation:ratio * 100',\n",
    "    'pod_cpu_usage': 'sum by (pod, namespace) (node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate)',\n",
    "    'pod_memory_usage': 'sum by (pod, namespace) (container_memory_working_set_bytes{container!=\"POD\", container!=\"\"})',\n",
    "    'alt_cpu_usage': 'sum(rate(container_cpu_usage_seconds_total{container!=\"POD\", container!=\"\"}[5m])) by (pod, namespace)',\n",
    "    'alt_memory_usage': 'sum(container_memory_rss{container!=\"POD\", container!=\"\"}) by (pod, namespace)',\n",
    "    'container_restart_count': 'sum by (pod, namespace, container) (kube_pod_container_status_restarts_total)',\n",
    "    'container_restart_rate_1h': 'sum by (pod, namespace) (increase(kube_pod_container_status_restarts_total[1h]))',\n",
    "    'deployment_unavailable': 'sum by (deployment, namespace) (kube_deployment_status_replicas_unavailable)',\n",
    "    'namespace_pod_count': 'sum by (namespace) (kube_pod_status_phase)',\n",
    "    'pods_pending': 'sum by (namespace) (kube_pod_status_phase{phase=\"Pending\"})',\n",
    "    'pods_running': 'sum by (namespace) (kube_pod_status_phase{phase=\"Running\"})',\n",
    "    'pods_failed': 'sum by (namespace) (kube_pod_status_phase{phase=\"Failed\"})',\n",
    "    'persistent_volume_usage': 'kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100',\n",
    "    'cluster_resource_quota': 'kube_resourcequota',\n",
    "    'apiserver_request_total': 'sum(rate(apiserver_request_total[5m]))',\n",
    "    'apiserver_error_rate': 'sum(rate(apiserver_request_total{code=~\"5..\"}[5m])) / sum(rate(apiserver_request_total[5m])) * 100',\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY CONFIGURATION SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üéØ ISOLATION FOREST CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüå≤ Model Configuration:\")\n",
    "print(f\"   Trees: {ISOLATION_FOREST_CONFIG['n_estimators']}\")\n",
    "print(f\"   Contamination: {ISOLATION_FOREST_CONFIG['contamination']} ({ISOLATION_FOREST_CONFIG['contamination']*100:.0f}% expected anomalies)\")\n",
    "print(f\"   Max samples: {ISOLATION_FOREST_CONFIG['max_samples']}\")\n",
    "\n",
    "print(f\"\\nüìä Target Metrics: {len(TARGET_METRICS)} (ALL VERIFIED WORKING)\")\n",
    "\n",
    "# Group metrics by category\n",
    "categories = {\n",
    "    'Resource (CPU/Memory)': [m for m in TARGET_METRICS if any(x in m for x in ['cpu', 'memory'])],\n",
    "    'Stability (Restarts)': [m for m in TARGET_METRICS if any(x in m for x in ['restart', 'unavailable'])],\n",
    "    'Pod Status': [m for m in TARGET_METRICS if any(x in m for x in ['pod', 'namespace']) and 'cpu' not in m and 'memory' not in m],\n",
    "    'Storage': [m for m in TARGET_METRICS if any(x in m for x in ['volume', 'quota', 'persistent'])],\n",
    "    'Control Plane': [m for m in TARGET_METRICS if 'apiserver' in m],\n",
    "}\n",
    "\n",
    "for category, metrics in categories.items():\n",
    "    if metrics:\n",
    "        print(f\"\\n   üìÅ {category}: {len(metrics)} metrics\")\n",
    "        for m in metrics:\n",
    "            print(f\"      ‚úÖ {m}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Configuration complete - ready for training!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Load Synthetic Anomalies for Training\n",
    "\n",
    "We load synthetic anomalies from Phase 1 (`synthetic-anomaly-generation.ipynb`) for training.\n",
    "\n",
    "**Why Synthetic Data?**\n",
    "- Real anomalies are rare (<1% in production clusters)\n",
    "- Synthetic data provides labeled training examples\n",
    "- Models learn general patterns, not memorize specific examples\n",
    "- Balanced dataset (50% normal, 50% anomaly) improves performance\n",
    "- Reproducible and testable\n",
    "\n",
    "**Machine Learning Best Practice:**\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "**References:**\n",
    "- He & Garcia (2009): \"Learning from Imbalanced Data\" - https://ieeexplore.ieee.org/document/5128907\n",
    "- Nikolenko (2021): \"Synthetic Data for Deep Learning\" - https://arxiv.org/abs/1909.11373\n",
    "- Goldstein & Uchida (2016): \"Anomaly Detection with Robust Deep Autoencoders\" - https://arxiv.org/abs/1511.08747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =============================================================================\n",
    "# PROMETHEUS CLIENT (for loading real data)\n",
    "# =============================================================================\n",
    "\n",
    "class PrometheusDataLoader:\n",
    "    \"\"\"Load real metrics from Prometheus.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        token_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'\n",
    "        self.token = None\n",
    "        if os.path.exists(token_path):\n",
    "            with open(token_path, 'r') as f:\n",
    "                self.token = f.read().strip()\n",
    "        \n",
    "        self.base_url = 'https://prometheus-k8s.openshift-monitoring.svc.cluster.local:9091'\n",
    "        self.session = requests.Session()\n",
    "        if self.token:\n",
    "            self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n",
    "        self.session.verify = False\n",
    "        \n",
    "        import urllib3\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        \n",
    "        # Test connection\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/api/v1/status/config\", timeout=5)\n",
    "            self.connected = response.status_code == 200\n",
    "        except:\n",
    "            self.connected = False\n",
    "    \n",
    "    def query_range(self, query, start, end, step='1m'):\n",
    "        if not self.connected:\n",
    "            return None\n",
    "        \n",
    "        url = f\"{self.base_url}/api/v1/query_range\"\n",
    "        params = {'query': query, 'start': start, 'end': end, 'step': step}\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPARATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_anomaly_detection_data(duration_hours=24, use_real_data=True):\n",
    "    \"\"\"\n",
    "    Prepare data for anomaly detection training.\n",
    "    \n",
    "    Args:\n",
    "        duration_hours: Hours of data to collect\n",
    "        use_real_data: If True, try to load from Prometheus. If False, use synthetic.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {metric_name: DataFrame with 'timestamp', 'value', 'is_anomaly'}\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ PREPARING ANOMALY DETECTION DATASET\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Duration: {duration_hours} hours\")\n",
    "    print(f\"   Metrics: {len(TARGET_METRICS)}\")\n",
    "    print(f\"   Use real data: {use_real_data}\")\n",
    "    \n",
    "    all_data = {}\n",
    "    data_sources = {}\n",
    "    \n",
    "    # Try to connect to Prometheus\n",
    "    prometheus = None\n",
    "    if use_real_data:\n",
    "        prometheus = PrometheusDataLoader()\n",
    "        print(f\"\\nüì° Prometheus connected: {prometheus.connected}\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=duration_hours)\n",
    "    \n",
    "    print(f\"\\nüìä Collecting metrics...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, metric in enumerate(TARGET_METRICS):\n",
    "        print(f\"   [{i+1}/{len(TARGET_METRICS)}] {metric}...\", end=\" \")\n",
    "        \n",
    "        # Try real data first\n",
    "        real_data_loaded = False\n",
    "        \n",
    "        if prometheus and prometheus.connected and metric in PROMETHEUS_QUERIES:\n",
    "            query = PROMETHEUS_QUERIES[metric]\n",
    "            result = prometheus.query_range(\n",
    "                query,\n",
    "                start_time.timestamp(),\n",
    "                end_time.timestamp(),\n",
    "                step='1m'\n",
    "            )\n",
    "            \n",
    "            if result and result.get('status') == 'success':\n",
    "                data = result.get('data', {}).get('result', [])\n",
    "                if data:\n",
    "                    # Parse Prometheus response\n",
    "                    rows = []\n",
    "                    for series in data:\n",
    "                        for ts, value in series.get('values', []):\n",
    "                            rows.append({\n",
    "                                'timestamp': pd.to_datetime(ts, unit='s'),\n",
    "                                'value': float(value) if value != 'NaN' else np.nan,\n",
    "                                'metric': metric,\n",
    "                                'is_anomaly': False  # Will be labeled by model\n",
    "                            })\n",
    "                    \n",
    "                    if rows:\n",
    "                        df = pd.DataFrame(rows)\n",
    "                        # Aggregate by timestamp (in case of multiple series)\n",
    "                        df = df.groupby('timestamp').agg({\n",
    "                            'value': 'mean',\n",
    "                            'metric': 'first',\n",
    "                            'is_anomaly': 'first'\n",
    "                        }).reset_index()\n",
    "                        \n",
    "                        all_data[metric] = df\n",
    "                        data_sources[metric] = 'REAL'\n",
    "                        real_data_loaded = True\n",
    "                        print(f\"‚úÖ REAL ({len(df)} points)\")\n",
    "        \n",
    "        # Fallback to synthetic data\n",
    "        if not real_data_loaded:\n",
    "            df = generate_synthetic_timeseries(\n",
    "                metric_name=metric,\n",
    "                duration_hours=duration_hours,\n",
    "                interval_minutes=1,\n",
    "                add_anomalies=True,\n",
    "                anomaly_probability=0.03\n",
    "            )\n",
    "            all_data[metric] = df\n",
    "            data_sources[metric] = 'SYNTHETIC'\n",
    "            print(f\"üìä SYNTHETIC ({len(df)} points)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä DATA COLLECTION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    real_count = sum(1 for s in data_sources.values() if s == 'REAL')\n",
    "    synthetic_count = sum(1 for s in data_sources.values() if s == 'SYNTHETIC')\n",
    "    \n",
    "    print(f\"\\n   Total metrics: {len(all_data)}\")\n",
    "    print(f\"   ‚úÖ REAL data: {real_count} metrics\")\n",
    "    print(f\"   üìä SYNTHETIC data: {synthetic_count} metrics\")\n",
    "    \n",
    "    total_points = sum(len(df) for df in all_data.values())\n",
    "    total_anomalies = sum(df['is_anomaly'].sum() for df in all_data.values())\n",
    "    \n",
    "    print(f\"\\n   Total data points: {total_points:,}\")\n",
    "    print(f\"   Total anomalies: {total_anomalies:,} ({total_anomalies/total_points:.2%})\")\n",
    "    \n",
    "    if real_count > 0:\n",
    "        print(f\"\\n   üéâ Using {real_count} REAL metrics from your OpenShift cluster!\")\n",
    "    \n",
    "    return all_data, data_sources\n",
    "\n",
    "# =============================================================================\n",
    "# RUN DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "# Set to True to try loading real Prometheus data\n",
    "# Set to False to use only synthetic data (faster, for testing)\n",
    "USE_REAL_PROMETHEUS_DATA = True\n",
    "\n",
    "training_data, data_sources = prepare_anomaly_detection_data(\n",
    "    duration_hours=24,  # Last 24 hours\n",
    "    use_real_data=USE_REAL_PROMETHEUS_DATA\n",
    ")\n",
    "\n",
    "# Display what we got\n",
    "print(\"\\nüìã METRICS BY DATA SOURCE:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\n‚úÖ REAL DATA:\")\n",
    "for metric, source in data_sources.items():\n",
    "    if source == 'REAL':\n",
    "        print(f\"   {metric}: {len(training_data[metric])} points\")\n",
    "\n",
    "print(\"\\nüìä SYNTHETIC DATA:\")\n",
    "for metric, source in data_sources.items():\n",
    "    if source == 'SYNTHETIC':\n",
    "        print(f\"   {metric}: {len(training_data[metric])} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_matrix(data_dict):\n",
    "    \"\"\"\n",
    "    Create feature matrix for anomaly detection\n",
    "    \"\"\"\n",
    "    print(\"üîß Creating feature matrix...\")\n",
    "    \n",
    "    # Align all time series to common timestamps\n",
    "    # Find common time range\n",
    "    min_start = max(df['timestamp'].min() for df in data_dict.values())\n",
    "    max_end = min(df['timestamp'].max() for df in data_dict.values())\n",
    "    \n",
    "    print(f\"  üìÖ Time range: {min_start} to {max_end}\")\n",
    "    \n",
    "    # Create common time index\n",
    "    time_index = pd.date_range(start=min_start, end=max_end, freq='1min')\n",
    "    \n",
    "    # Build feature matrix\n",
    "    features = pd.DataFrame(index=time_index)\n",
    "    labels = pd.Series(index=time_index, dtype=bool, name='is_anomaly')\n",
    "    \n",
    "    for metric_name, df in data_dict.items():\n",
    "        # Resample to common time index\n",
    "        df_resampled = df.set_index('timestamp').reindex(time_index, method='nearest')\n",
    "        \n",
    "        # Add basic features\n",
    "        features[f'{metric_name}_value'] = df_resampled['value']\n",
    "        \n",
    "        # Add rolling statistics (5-minute windows)\n",
    "        features[f'{metric_name}_mean_5m'] = df_resampled['value'].rolling('5min').mean()\n",
    "        features[f'{metric_name}_std_5m'] = df_resampled['value'].rolling('5min').std()\n",
    "        features[f'{metric_name}_min_5m'] = df_resampled['value'].rolling('5min').min()\n",
    "        features[f'{metric_name}_max_5m'] = df_resampled['value'].rolling('5min').max()\n",
    "        \n",
    "        # Add lag features\n",
    "        features[f'{metric_name}_lag_1'] = df_resampled['value'].shift(1)\n",
    "        features[f'{metric_name}_lag_5'] = df_resampled['value'].shift(5)\n",
    "        \n",
    "        # Add rate of change\n",
    "        features[f'{metric_name}_diff'] = df_resampled['value'].diff()\n",
    "        features[f'{metric_name}_pct_change'] = df_resampled['value'].pct_change()\n",
    "        \n",
    "        # Combine anomaly labels (any metric anomaly = overall anomaly)\n",
    "        metric_anomalies = df_resampled['is_anomaly'].fillna(False)\n",
    "        labels = labels | metric_anomalies\n",
    "    \n",
    "    # Fill missing values\n",
    "    features = features.ffill().bfill()\n",
    "    labels = labels.fillna(False)\n",
    "    \n",
    "    # Replace infinity values with 0 and remaining NaN with 0\n",
    "    features = features.replace([np.inf, -np.inf], 0)\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    print(f\"  ‚úÖ Feature matrix: {features.shape}\")\n",
    "    print(f\"  üè∑Ô∏è Anomaly labels: {labels.sum()} anomalies ({labels.mean():.2%})\")\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Create feature matrix\n",
    "X, y = create_feature_matrix(training_data)\n",
    "\n",
    "print(f\"\\nüìä Feature Engineering Complete:\")\n",
    "print(f\"  Features: {X.shape[1]} columns\")\n",
    "print(f\"  Samples: {X.shape[0]:,} rows\")\n",
    "print(f\"  Anomaly rate: {y.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Train Isolation Forest model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"  Training: {X_train.shape[0]:,} samples\")\n",
    "print(f\"  Testing: {X_test.shape[0]:,} samples\")\n",
    "print(f\"  Training anomalies: {y_train.sum()} ({y_train.mean():.2%})\")\n",
    "print(f\"  Testing anomalies: {y_test.sum()} ({y_test.mean():.2%})\")\n",
    "\n",
    "# ‚ú® Create sklearn Pipeline combining scaler + model (KServe compatible)\n",
    "print(\"\\nüîß Creating Isolation Forest pipeline (scaler + model)...\")\n",
    "print(\"   This creates a SINGLE .pkl file for KServe deployment\")\n",
    "\n",
    "isolation_forest_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),  # More robust to outliers than StandardScaler\n",
    "    ('isolation_forest', IsolationForest(**ISOLATION_FOREST_CONFIG))\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Pipeline created (scaler + Isolation Forest)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Isolation Forest Pipeline\n",
    "print(\"üå≤ Training Isolation Forest pipeline...\")\n",
    "print(\"   Pipeline automatically handles: scaler.fit_transform() ‚Üí model.fit()\")\n",
    "\n",
    "# Fit pipeline on training data\n",
    "# Pipeline will:\n",
    "#   1. Fit scaler on X_train and transform it\n",
    "#   2. Fit Isolation Forest on scaled data\n",
    "isolation_forest_pipeline.fit(X_train)\n",
    "\n",
    "print(\"‚úÖ Training complete\")\n",
    "\n",
    "# Make predictions using pipeline\n",
    "# Pipeline will:\n",
    "#   1. Transform data using fitted scaler\n",
    "#   2. Predict using fitted model\n",
    "print(\"üîÆ Making predictions...\")\n",
    "y_pred_train = isolation_forest_pipeline.predict(X_train)\n",
    "y_pred_test = isolation_forest_pipeline.predict(X_test)\n",
    "\n",
    "# Get anomaly scores\n",
    "train_scores = isolation_forest_pipeline.decision_function(X_train)\n",
    "test_scores = isolation_forest_pipeline.decision_function(X_test)\n",
    "\n",
    "# Convert predictions to binary (1 = normal, -1 = anomaly)\n",
    "y_pred_train_binary = (y_pred_train == -1)\n",
    "y_pred_test_binary = (y_pred_test == -1)\n",
    "\n",
    "print(f\"  Training predictions: {y_pred_train_binary.sum()} anomalies detected\")\n",
    "print(f\"  Testing predictions: {y_pred_test_binary.sum()} anomalies detected\")\n",
    "print(\"\\n‚úÖ Pipeline handles scaling automatically - no separate scaler needed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "print(\"üìä Model Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training set performance\n",
    "print(\"\\nüèãÔ∏è Training Set Performance:\")\n",
    "print(classification_report(y_train, y_pred_train_binary, \n",
    "                          target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Test set performance\n",
    "print(\"\\nüß™ Test Set Performance:\")\n",
    "print(classification_report(y_test, y_pred_test_binary, \n",
    "                          target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training confusion matrix\n",
    "cm_train = confusion_matrix(y_train, y_pred_train_binary)\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Anomaly'], \n",
    "            yticklabels=['Normal', 'Anomaly'], ax=axes[0])\n",
    "axes[0].set_title('Training Set Confusion Matrix')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Test confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test_binary)\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Anomaly'], \n",
    "            yticklabels=['Normal', 'Anomaly'], ax=axes[1])\n",
    "axes[1].set_title('Test Set Confusion Matrix')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze anomaly scores distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Isolation Forest Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Score distribution\n",
    "axes[0, 0].hist(train_scores[~y_train], bins=50, alpha=0.7, label='Normal', density=True)\n",
    "axes[0, 0].hist(train_scores[y_train], bins=50, alpha=0.7, label='Anomaly', density=True)\n",
    "axes[0, 0].set_title('Anomaly Score Distribution (Training)')\n",
    "axes[0, 0].set_xlabel('Anomaly Score')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Score vs time (sample)\n",
    "sample_size = min(1000, len(test_scores))\n",
    "sample_indices = np.random.choice(len(test_scores), sample_size, replace=False)\n",
    "sample_indices = np.sort(sample_indices)\n",
    "\n",
    "axes[0, 1].plot(sample_indices, test_scores[sample_indices], 'b-', alpha=0.7, linewidth=1)\n",
    "anomaly_indices = sample_indices[y_test.iloc[sample_indices]]\n",
    "if len(anomaly_indices) > 0:\n",
    "    axes[0, 1].scatter(anomaly_indices, test_scores[anomaly_indices], \n",
    "                      color='red', s=30, alpha=0.8, label='True Anomalies')\n",
    "axes[0, 1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].set_title('Anomaly Scores Over Time (Test Sample)')\n",
    "axes[0, 1].set_xlabel('Sample Index')\n",
    "axes[0, 1].set_ylabel('Anomaly Score')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance (using PCA to visualize)\n",
    "pca = PCA(n_components=2)\n",
    "X_test_pca = pca.fit_transform(X_test)\n",
    "\n",
    "normal_mask = ~y_test\n",
    "anomaly_mask = y_test\n",
    "\n",
    "axes[1, 0].scatter(X_test_pca[normal_mask, 0], X_test_pca[normal_mask, 1], \n",
    "                  c='blue', alpha=0.6, s=20, label='Normal')\n",
    "axes[1, 0].scatter(X_test_pca[anomaly_mask, 0], X_test_pca[anomaly_mask, 1], \n",
    "                  c='red', alpha=0.8, s=30, label='Anomaly')\n",
    "axes[1, 0].set_title('PCA Visualization (Test Set)')\n",
    "axes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model performance metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred_test_binary)\n",
    "recall = recall_score(y_test, y_pred_test_binary)\n",
    "f1 = f1_score(y_test, y_pred_test_binary)\n",
    "\n",
    "# Convert scores to probabilities for AUC calculation\n",
    "test_scores_prob = (test_scores - test_scores.min()) / (test_scores.max() - test_scores.min())\n",
    "auc = roc_auc_score(y_test, 1 - test_scores_prob)  # Invert because lower scores = more anomalous\n",
    "\n",
    "metrics_text = f\"\"\"\n",
    "Model Performance Metrics:\n",
    "\n",
    "Precision: {precision:.3f}\n",
    "Recall: {recall:.3f}\n",
    "F1-Score: {f1:.3f}\n",
    "AUC-ROC: {auc:.3f}\n",
    "\n",
    "Configuration:\n",
    "Trees: {ISOLATION_FOREST_CONFIG['n_estimators']}\n",
    "Contamination: {ISOLATION_FOREST_CONFIG['contamination']}\n",
    "Features: {X.shape[1]}\n",
    "\n",
    "Data:\n",
    "Training: {X_train.shape[0]:,}\n",
    "Testing: {X_test.shape[0]:,}\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 1].text(0.05, 0.95, metrics_text, transform=axes[1, 1].transAxes, \n",
    "               fontsize=10, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "axes[1, 1].set_title('Model Summary')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Model Performance Summary:\")\n",
    "print(f\"  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall: {recall:.3f}\")\n",
    "print(f\"  F1-Score: {f1:.3f}\")\n",
    "print(f\"  AUC-ROC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pipeline model to persistent storage\n",
    "# Use /mnt/models for persistent storage (model-storage-pvc)\n",
    "# Fallback to local for development outside cluster\n",
    "MODELS_DIR = Path('/mnt/models') if Path('/mnt/models').exists() else Path('/opt/app-root/src/models')\n",
    "\n",
    "# Create KServe-compatible subdirectory structure\n",
    "MODEL_NAME = 'anomaly-detector'\n",
    "MODEL_DIR = MODELS_DIR / MODEL_NAME\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save with KServe expected filename\n",
    "model_path = MODEL_DIR / 'model.pkl'\n",
    "\n",
    "# Migration: Move old flat file if exists\n",
    "old_path = MODELS_DIR / 'anomaly-detector.pkl'\n",
    "if old_path.exists() and not model_path.exists():\n",
    "    import shutil\n",
    "    shutil.move(str(old_path), str(model_path))\n",
    "    print(f\"üîÑ Migrated model from {old_path} to {model_path}\")\n",
    "\n",
    "# ‚ú® Save SINGLE pipeline file (KServe compatible)\n",
    "# KServe sklearn server expects model at: /mnt/models/anomaly-detector/model.pkl\n",
    "joblib.dump(isolation_forest_pipeline, model_path)\n",
    "print(f\"üíæ Saved Isolation Forest pipeline to: {model_path}\")\n",
    "print(f\"   ‚úÖ KServe-compatible path: {MODEL_NAME}/model.pkl\")\n",
    "print(f\"   ‚úÖ Single .pkl file (scaler + model combined)\")\n",
    "\n",
    "# Upload model to S3 for persistent storage\n",
    "try:\n",
    "    from common_functions import upload_model_to_s3, test_s3_connection\n",
    "    \n",
    "    if test_s3_connection():\n",
    "        upload_model_to_s3(\n",
    "            str(model_path),\n",
    "            s3_key='models/anomaly-detection/anomaly-detector/model.pkl'\n",
    "        )\n",
    "        print(f\"‚òÅÔ∏è  Uploaded to S3: models/anomaly-detection/anomaly-detector/model.pkl\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è S3 not available - model saved locally only\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è S3 functions not available - model saved locally only\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è S3 upload failed (non-critical): {e}\")\n",
    "\n",
    "# Verify model saved\n",
    "assert model_path.exists(), \"Pipeline model not saved\"\n",
    "print(\"\\n‚úÖ Model pipeline saved successfully\")\n",
    "print(f\"   Path: {model_path}\")\n",
    "print(f\"   Size: {model_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Clean up old separate model/scaler files if they exist\n",
    "old_model = MODELS_DIR / 'isolation_forest_model.pkl'\n",
    "old_scaler = MODELS_DIR / 'isolation_forest_scaler.pkl'\n",
    "for old_file in [old_model, old_scaler]:\n",
    "    if old_file.exists():\n",
    "        old_file.unlink()\n",
    "        print(f\"üóëÔ∏è  Removed old file: {old_file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


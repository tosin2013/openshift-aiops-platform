{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Anomaly Detection\n",
    "\n",
    "## Overview\n",
    "This notebook implements time series anomaly detection using ARIMA and Prophet forecasting methods. It detects anomalies by identifying deviations from predicted values.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `synthetic-anomaly-generation.ipynb` (Phase 1)\n",
    "- Libraries: statsmodels, prophet, pandas, numpy\n",
    "- Synthetic dataset: `/opt/app-root/src/data/processed/synthetic_anomalies.parquet`\n",
    "\n",
    "## Why We Use Synthetic Data\n",
    "\n",
    "### The Problem: Real Anomalies Are Rare\n",
    "In production OpenShift clusters:\n",
    "- Anomalies occur <1% of the time\n",
    "- Collecting 1000 labeled anomalies takes months/years\n",
    "- Different anomaly types are hard to capture\n",
    "- Can't deliberately cause failures to collect data\n",
    "\n",
    "### The Solution: Synthetic Anomalies\n",
    "We generate synthetic anomalies because:\n",
    "- ‚úÖ Create 1000+ labeled anomalies in minutes\n",
    "- ‚úÖ Control anomaly types and severity\n",
    "- ‚úÖ Ensure balanced training data (50% normal, 50% anomaly)\n",
    "- ‚úÖ Reproducible and testable\n",
    "- ‚úÖ Models trained on synthetic data generalize to real anomalies\n",
    "\n",
    "### Machine Learning Best Practice\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement ARIMA forecasting on synthetic data\n",
    "- Use Prophet for time series analysis\n",
    "- Detect anomalies via forecast deviations\n",
    "- Handle seasonal patterns\n",
    "- Evaluate detection performance with labeled data\n",
    "\n",
    "## Key Concepts\n",
    "- **ARIMA**: AutoRegressive Integrated Moving Average\n",
    "- **Prophet**: Facebook's time series forecasting tool\n",
    "- **Forecast Error**: Deviation between actual and predicted values\n",
    "- **Seasonality**: Repeating patterns in time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Why Synthetic Data for Training?\n",
    "- **He & Garcia (2009)**: \"Learning from Imbalanced Data\" - https://ieeexplore.ieee.org/document/5128907\n",
    "- **Nikolenko (2021)**: \"Synthetic Data for Deep Learning\" - https://arxiv.org/abs/1909.11373\n",
    "- **Goldstein & Uchida (2016)**: \"Anomaly Detection with Robust Deep Autoencoders\" - https://arxiv.org/abs/1511.08747\n",
    "\n",
    "### Time Series Anomaly Detection\n",
    "- **Malhotra et al. (2016)**: \"Time Series Anomaly Detection with LSTM Networks\" - https://arxiv.org/abs/1607.00148\n",
    "- **Taylor & Letham (2018)**: \"Forecasting at Scale (Prophet)\" - https://peerj.com/articles/3190\n",
    "- **Box & Jenkins (1970)**: \"Time Series Analysis, Forecasting and Control (ARIMA)\" - Classic reference\n",
    "\n",
    "### Key Takeaway\n",
    "Synthetic data provides labeled training examples that allow us to:\n",
    "1. Train models with known ground truth\n",
    "2. Evaluate performance with precision, recall, and F1 scores\n",
    "3. Ensure reproducible and testable results\n",
    "4. Build models that generalize to real-world anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Setup path for utils module - works from any directory\n",
    "def find_utils_path():\n",
    "    \"\"\"Find utils path regardless of current working directory\"\"\"\n",
    "    possible_paths = [\n",
    "        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n",
    "        Path.cwd() / 'notebooks' / 'utils',\n",
    "        Path.cwd().parent / 'utils',\n",
    "        Path('/workspace/repo/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n",
    "    ]\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists() and (p / 'common_functions.py').exists():\n",
    "            return str(p)\n",
    "    current = Path.cwd()\n",
    "    for _ in range(5):\n",
    "        utils_path = current / 'notebooks' / 'utils'\n",
    "        if utils_path.exists():\n",
    "            return str(utils_path)\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "utils_path = find_utils_path()\n",
    "if utils_path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "    print(f\"‚úÖ Utils path found: {utils_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Utils path not found - will use fallback implementations\")\n",
    "\n",
    "# Try to import common functions, with fallback\n",
    "try:\n",
    "    from common_functions import setup_environment\n",
    "    print(\"‚úÖ Common functions imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Common functions not available: {e}\")\n",
    "    def setup_environment():\n",
    "        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n",
    "        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n",
    "        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Setup environment\n",
    "env_info = setup_environment()\n",
    "logger.info(f\"Environment ready: {env_info}\")\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('/opt/app-root/src/data')\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# Use /mnt/models for persistent storage (model-storage-pvc)\n",
    "# Fallback to local for development outside cluster\n",
    "MODELS_DIR = Path('/mnt/models') if Path('/mnt/models').exists() else Path('/opt/app-root/src/models')\n",
    "\n",
    "# Create KServe-compatible subdirectory structure\n",
    "# CHANGED: Use unique model name to avoid conflict with isolation-forest's anomaly-detector model\n",
    "MODEL_NAME = 'timeseries-predictor'\n",
    "MODEL_DIR = MODELS_DIR / MODEL_NAME\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Data directory: {DATA_DIR}\")\n",
    "logger.info(f\"Models directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Load Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_METRICS = [\n",
    "    # Resource Metrics (5)\n",
    "    'node_memory_utilization',\n",
    "    'pod_cpu_usage',\n",
    "    'pod_memory_usage',\n",
    "    'alt_cpu_usage',\n",
    "    'alt_memory_usage',\n",
    "    \n",
    "    # Stability Metrics (3)\n",
    "    'container_restart_count',\n",
    "    'container_restart_rate_1h',\n",
    "    'deployment_unavailable',\n",
    "    \n",
    "    # Pod Status Metrics (4)\n",
    "    'namespace_pod_count',\n",
    "    'pods_pending',\n",
    "    'pods_running',\n",
    "    'pods_failed',\n",
    "    \n",
    "    # Storage Metrics (2)\n",
    "    'persistent_volume_usage',\n",
    "    'cluster_resource_quota',\n",
    "    \n",
    "    # Control Plane Metrics (2)\n",
    "    'apiserver_request_total',\n",
    "    'apiserver_error_rate',\n",
    "]\n",
    "\n",
    "# Prometheus queries for real data collection\n",
    "PROMETHEUS_QUERIES = {\n",
    "    'node_memory_utilization': 'instance:node_memory_utilisation:ratio * 100',\n",
    "    'pod_cpu_usage': 'sum by (pod, namespace) (node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate)',\n",
    "    'pod_memory_usage': 'sum by (pod, namespace) (container_memory_working_set_bytes{container!=\"POD\", container!=\"\"})',\n",
    "    'alt_cpu_usage': 'sum(rate(container_cpu_usage_seconds_total{container!=\"POD\", container!=\"\"}[5m])) by (pod, namespace)',\n",
    "    'alt_memory_usage': 'sum(container_memory_rss{container!=\"POD\", container!=\"\"}) by (pod, namespace)',\n",
    "    'container_restart_count': 'sum by (pod, namespace, container) (kube_pod_container_status_restarts_total)',\n",
    "    'container_restart_rate_1h': 'sum by (pod, namespace) (increase(kube_pod_container_status_restarts_total[1h]))',\n",
    "    'deployment_unavailable': 'sum by (deployment, namespace) (kube_deployment_status_replicas_unavailable)',\n",
    "    'namespace_pod_count': 'sum by (namespace) (kube_pod_status_phase)',\n",
    "    'pods_pending': 'sum by (namespace) (kube_pod_status_phase{phase=\"Pending\"})',\n",
    "    'pods_running': 'sum by (namespace) (kube_pod_status_phase{phase=\"Running\"})',\n",
    "    'pods_failed': 'sum by (namespace) (kube_pod_status_phase{phase=\"Failed\"})',\n",
    "    'persistent_volume_usage': 'kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100',\n",
    "    'cluster_resource_quota': 'kube_resourcequota',\n",
    "    'apiserver_request_total': 'sum(rate(apiserver_request_total[5m]))',\n",
    "    'apiserver_error_rate': 'sum(rate(apiserver_request_total{code=~\"5..\"}[5m])) / sum(rate(apiserver_request_total[5m])) * 100',\n",
    "}\n",
    "\n",
    "print(f\"üìä Target metrics for time series analysis: {len(TARGET_METRICS)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PROMETHEUS CLIENT (for loading real data)\n",
    "# =============================================================================\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "class PrometheusClient:\n",
    "    \"\"\"Client for querying Prometheus in OpenShift.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        token_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'\n",
    "        self.token = None\n",
    "        if os.path.exists(token_path):\n",
    "            with open(token_path, 'r') as f:\n",
    "                self.token = f.read().strip()\n",
    "        \n",
    "        self.base_url = 'https://prometheus-k8s.openshift-monitoring.svc.cluster.local:9091'\n",
    "        self.session = requests.Session()\n",
    "        if self.token:\n",
    "            self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n",
    "        self.session.verify = False\n",
    "        \n",
    "        import urllib3\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        \n",
    "        # Test connection\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/api/v1/status/config\", timeout=5)\n",
    "            self.connected = response.status_code == 200\n",
    "        except:\n",
    "            self.connected = False\n",
    "    \n",
    "    def query_range(self, query, start, end, step='1m'):\n",
    "        if not self.connected:\n",
    "            return None\n",
    "        \n",
    "        url = f\"{self.base_url}/api/v1/query_range\"\n",
    "        params = {'query': query, 'start': start, 'end': end, 'step': step}\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def load_timeseries_data(duration_hours=24, use_real_data=True):\n",
    "    \"\"\"\n",
    "    Load time series data for anomaly detection.\n",
    "    \n",
    "    For ARIMA/Prophet, we need univariate time series, so we'll create\n",
    "    a DataFrame where each column is one metric's aggregated values over time.\n",
    "    \n",
    "    Args:\n",
    "        duration_hours: Hours of historical data\n",
    "        use_real_data: Try Prometheus first, fallback to synthetic\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with timestamp index and metric columns\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ LOADING TIME SERIES DATA\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Duration: {duration_hours} hours\")\n",
    "    print(f\"   Metrics: {len(TARGET_METRICS)}\")\n",
    "    print(f\"   Use real data: {use_real_data}\")\n",
    "    \n",
    "    # Try to connect to Prometheus\n",
    "    prometheus = None\n",
    "    if use_real_data:\n",
    "        prometheus = PrometheusClient()\n",
    "        print(f\"   Prometheus connected: {prometheus.connected}\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=duration_hours)\n",
    "    \n",
    "    # Create time index (1-minute intervals)\n",
    "    time_index = pd.date_range(start=start_time, end=end_time, freq='1min')\n",
    "    \n",
    "    # Initialize DataFrame\n",
    "    df = pd.DataFrame(index=time_index)\n",
    "    df.index.name = 'timestamp'\n",
    "    \n",
    "    data_sources = {}\n",
    "    \n",
    "    print(f\"\\nüìä Loading {len(TARGET_METRICS)} metrics...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, metric in enumerate(TARGET_METRICS):\n",
    "        real_data_loaded = False\n",
    "        \n",
    "        if prometheus and prometheus.connected and metric in PROMETHEUS_QUERIES:\n",
    "            query = PROMETHEUS_QUERIES[metric]\n",
    "            result = prometheus.query_range(\n",
    "                query,\n",
    "                start_time.timestamp(),\n",
    "                end_time.timestamp(),\n",
    "                step='1m'\n",
    "            )\n",
    "            \n",
    "            if result and result.get('status') == 'success':\n",
    "                data = result.get('data', {}).get('result', [])\n",
    "                if data:\n",
    "                    # Parse and aggregate Prometheus data\n",
    "                    rows = []\n",
    "                    for series in data:\n",
    "                        for ts, value in series.get('values', []):\n",
    "                            try:\n",
    "                                rows.append({\n",
    "                                    'timestamp': pd.to_datetime(ts, unit='s'),\n",
    "                                    'value': float(value) if value != 'NaN' else np.nan\n",
    "                                })\n",
    "                            except:\n",
    "                                pass\n",
    "                    \n",
    "                    if rows:\n",
    "                        metric_df = pd.DataFrame(rows)\n",
    "                        # Aggregate by timestamp (mean across all series)\n",
    "                        metric_series = metric_df.groupby('timestamp')['value'].mean()\n",
    "                        # Resample to our time index\n",
    "                        metric_series = metric_series.reindex(time_index, method='nearest')\n",
    "                        df[metric] = metric_series\n",
    "                        data_sources[metric] = 'REAL'\n",
    "                        real_data_loaded = True\n",
    "                        print(f\"   ‚úÖ [{i+1:2}/{len(TARGET_METRICS)}] {metric}: REAL ({metric_series.notna().sum()} points)\")\n",
    "        \n",
    "        if not real_data_loaded:\n",
    "            # Generate synthetic time series with realistic patterns\n",
    "            n_points = len(time_index)\n",
    "            \n",
    "            # Base pattern: trend + seasonality + noise\n",
    "            trend = np.linspace(50, 55, n_points)  # Slight upward trend\n",
    "            daily_seasonal = 10 * np.sin(np.linspace(0, 2*np.pi * (duration_hours/24), n_points))\n",
    "            hourly_seasonal = 3 * np.sin(np.linspace(0, 2*np.pi * duration_hours, n_points))\n",
    "            noise = np.random.normal(0, 2, n_points)\n",
    "            \n",
    "            # Customize based on metric type\n",
    "            if 'cpu' in metric.lower():\n",
    "                base = 30 + trend * 0.5 + daily_seasonal + noise\n",
    "            elif 'memory' in metric.lower():\n",
    "                base = 60 + trend * 0.3 + daily_seasonal * 0.5 + noise\n",
    "            elif 'restart' in metric.lower():\n",
    "                base = np.abs(noise * 0.5)  # Low values, occasional spikes\n",
    "            elif 'pending' in metric.lower() or 'failed' in metric.lower():\n",
    "                base = np.abs(noise * 0.2)  # Mostly zeros\n",
    "            else:\n",
    "                base = 50 + trend + daily_seasonal + noise\n",
    "            \n",
    "            df[metric] = base\n",
    "            data_sources[metric] = 'SYNTHETIC'\n",
    "            print(f\"   üìä [{i+1:2}/{len(TARGET_METRICS)}] {metric}: SYNTHETIC ({len(base)} points)\")\n",
    "    \n",
    "    # Add labels column (for synthetic anomalies)\n",
    "    df['label'] = 0\n",
    "    \n",
    "    # Inject some anomalies for training/testing\n",
    "    anomaly_rate = 0.03  # 3% anomalies\n",
    "    n_anomalies = int(len(df) * anomaly_rate)\n",
    "    anomaly_indices = np.random.choice(len(df), n_anomalies, replace=False)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        # Pick random metrics to make anomalous\n",
    "        anomaly_metrics = np.random.choice(TARGET_METRICS, 2, replace=False)\n",
    "        for metric in anomaly_metrics:\n",
    "            if metric in df.columns:\n",
    "                std = df[metric].std()\n",
    "                df.iloc[idx, df.columns.get_loc(metric)] += 3.0 * std * np.random.choice([-1, 1])\n",
    "        df.iloc[idx, df.columns.get_loc('label')] = 1\n",
    "    \n",
    "    # Summary\n",
    "    real_count = sum(1 for s in data_sources.values() if s == 'REAL')\n",
    "    synthetic_count = sum(1 for s in data_sources.values() if s == 'SYNTHETIC')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä DATA LOADING SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Total metrics: {len(TARGET_METRICS)}\")\n",
    "    print(f\"   ‚úÖ REAL data: {real_count} metrics\")\n",
    "    print(f\"   üìä SYNTHETIC data: {synthetic_count} metrics\")\n",
    "    print(f\"   Data points: {len(df):,}\")\n",
    "    print(f\"   Anomalies injected: {df['label'].sum()} ({df['label'].mean():.1%})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return df, data_sources\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD THE DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Set to True to try loading real Prometheus data\n",
    "USE_REAL_DATA = True\n",
    "\n",
    "df, data_sources = load_timeseries_data(\n",
    "    duration_hours=24,\n",
    "    use_real_data=USE_REAL_DATA\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìã DATA LOADED:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Columns: {list(df.columns[:5])}... + {len(df.columns)-5} more\")\n",
    "print(f\"   Time range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"\\n   Normal samples: {(df['label'] == 0).sum()}\")\n",
    "print(f\"   Anomalous samples: {(df['label'] == 1).sum()}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nüìä Sample data:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ARIMA-Based Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 3 - Load or generate synthetic data with TARGET_METRICS names\n",
    "\n",
    "data_file = PROCESSED_DIR / 'synthetic_anomalies.parquet'\n",
    "\n",
    "# Check if we already have real data loaded with proper columns\n",
    "real_data_loaded = (\n",
    "    'df' in dir() and \n",
    "    not df.empty and \n",
    "    any(col in df.columns for col in TARGET_METRICS)\n",
    ")\n",
    "\n",
    "if real_data_loaded:\n",
    "    logger.info(f\"‚úÖ Using real Prometheus data already loaded: {df.shape}\")\n",
    "    print(f\"‚úÖ Real data already loaded with {len([c for c in df.columns if c in TARGET_METRICS])} metrics\")\n",
    "elif data_file.exists():\n",
    "    # Check if existing file has TARGET_METRICS columns\n",
    "    existing_df = pd.read_parquet(data_file)\n",
    "    if any(col in existing_df.columns for col in TARGET_METRICS):\n",
    "        df = existing_df\n",
    "        logger.info(f\"‚úÖ Loaded existing data with TARGET_METRICS: {df.shape}\")\n",
    "    else:\n",
    "        logger.info(\"‚ö†Ô∏è Existing synthetic data uses old column names - regenerating...\")\n",
    "        # Will regenerate below\n",
    "        data_file = None  # Force regeneration\n",
    "\n",
    "if not real_data_loaded and (not data_file or not data_file.exists()):\n",
    "    logger.info(\"üìä Generating synthetic data with TARGET_METRICS names...\")\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    np.random.seed(42)\n",
    "    n_points = 1000\n",
    "    \n",
    "    # Create timestamp index\n",
    "    start_time = datetime.now() - timedelta(days=30)\n",
    "    timestamps = [start_time + timedelta(minutes=i) for i in range(n_points)]\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Generate realistic patterns for each metric type\n",
    "    for metric in TARGET_METRICS:\n",
    "        # Base components\n",
    "        trend = np.linspace(0, 5, n_points)  # Slight upward trend\n",
    "        daily_seasonal = np.sin(np.linspace(0, 4*np.pi, n_points))  # ~2 day cycles\n",
    "        hourly_seasonal = 0.3 * np.sin(np.linspace(0, 48*np.pi, n_points))  # Hourly variation\n",
    "        noise = np.random.normal(0, 1, n_points)\n",
    "        \n",
    "        # Customize based on metric type\n",
    "        if metric == 'node_memory_utilization':\n",
    "            # Memory utilization: 40-80% range with gradual changes\n",
    "            base = 60 + trend + 10 * daily_seasonal + 2 * noise\n",
    "            data[metric] = np.clip(base, 0, 100)\n",
    "            \n",
    "        elif metric == 'pod_cpu_usage':\n",
    "            # CPU usage: 0-1 range (cores), spiky\n",
    "            base = 0.3 + 0.1 * daily_seasonal + 0.05 * hourly_seasonal + 0.02 * np.abs(noise)\n",
    "            data[metric] = np.clip(base, 0, 2)\n",
    "            \n",
    "        elif metric == 'pod_memory_usage':\n",
    "            # Memory in bytes: ~200-300MB range\n",
    "            base = 2.5e8 + 2e7 * daily_seasonal + 5e6 * noise\n",
    "            data[metric] = np.clip(base, 1e8, 5e8)\n",
    "            \n",
    "        elif metric == 'alt_cpu_usage':\n",
    "            # Alternative CPU metric: similar to pod_cpu_usage\n",
    "            base = 0.25 + 0.08 * daily_seasonal + 0.03 * hourly_seasonal + 0.015 * np.abs(noise)\n",
    "            data[metric] = np.clip(base, 0, 2)\n",
    "            \n",
    "        elif metric == 'alt_memory_usage':\n",
    "            # Alternative memory: ~150MB range\n",
    "            base = 1.5e8 + 1e7 * daily_seasonal + 3e6 * noise\n",
    "            data[metric] = np.clip(base, 5e7, 3e8)\n",
    "            \n",
    "        elif metric == 'container_restart_count':\n",
    "            # Restart count: low integers, mostly stable\n",
    "            base = 10 + 0.5 * np.abs(noise)\n",
    "            data[metric] = np.clip(base, 0, 50).astype(int)\n",
    "            \n",
    "        elif metric == 'container_restart_rate_1h':\n",
    "            # Restart rate: very low, occasional spikes\n",
    "            base = 0.002 + 0.001 * np.abs(noise)\n",
    "            # Add occasional spikes\n",
    "            spike_indices = np.random.choice(n_points, 20, replace=False)\n",
    "            base[spike_indices] += np.random.uniform(0.01, 0.05, 20)\n",
    "            data[metric] = np.clip(base, 0, 0.1)\n",
    "            \n",
    "        elif metric == 'deployment_unavailable':\n",
    "            # Unavailable replicas: mostly 0, occasional non-zero\n",
    "            base = np.zeros(n_points)\n",
    "            unavail_indices = np.random.choice(n_points, 30, replace=False)\n",
    "            base[unavail_indices] = np.random.randint(1, 3, 30)\n",
    "            data[metric] = base\n",
    "            \n",
    "        elif metric == 'namespace_pod_count':\n",
    "            # Pod count per namespace: 5-15 range\n",
    "            base = 8 + 2 * daily_seasonal + 0.5 * noise\n",
    "            data[metric] = np.clip(base, 1, 20)\n",
    "            \n",
    "        elif metric == 'pods_pending':\n",
    "            # Pending pods: mostly 0, occasional small values\n",
    "            base = 0.01 + 0.005 * np.abs(noise)\n",
    "            data[metric] = np.clip(base, 0, 1)\n",
    "            \n",
    "        elif metric == 'pods_running':\n",
    "            # Running pods: stable around 5-7\n",
    "            base = 6 + 0.5 * daily_seasonal + 0.2 * noise\n",
    "            data[metric] = np.clip(base, 1, 15)\n",
    "            \n",
    "        elif metric == 'pods_failed':\n",
    "            # Failed pods: mostly 0, rare failures\n",
    "            base = 0.1 + 0.05 * np.abs(noise)\n",
    "            data[metric] = np.clip(base, 0, 2)\n",
    "            \n",
    "        elif metric == 'persistent_volume_usage':\n",
    "            # PV usage: 5-15% range, slow growth\n",
    "            base = 0.08 + 0.02 * (trend / 5) + 0.01 * daily_seasonal + 0.005 * noise\n",
    "            data[metric] = np.clip(base, 0, 1)\n",
    "            \n",
    "        elif metric == 'cluster_resource_quota':\n",
    "            # Resource quota: often 0 (not set)\n",
    "            data[metric] = np.zeros(n_points)\n",
    "            \n",
    "        elif metric == 'apiserver_request_total':\n",
    "            # API server requests: 80-150 range, variable\n",
    "            base = 110 + 20 * daily_seasonal + 10 * hourly_seasonal + 5 * noise\n",
    "            data[metric] = np.clip(base, 50, 200)\n",
    "            \n",
    "        elif metric == 'apiserver_error_rate':\n",
    "            # Error rate: mostly 0, occasional spikes\n",
    "            base = np.zeros(n_points)\n",
    "            error_indices = np.random.choice(n_points, 15, replace=False)\n",
    "            base[error_indices] = np.random.uniform(0.01, 0.05, 15)\n",
    "            data[metric] = np.clip(base, 0, 0.1)\n",
    "        \n",
    "        else:\n",
    "            # Default pattern for any unknown metrics\n",
    "            base = 50 + trend + 10 * daily_seasonal + 2 * noise\n",
    "            data[metric] = base\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = timestamps\n",
    "    df['label'] = 0\n",
    "    \n",
    "    # Set timestamp as index\n",
    "    df = df.set_index('timestamp')\n",
    "    df.index.name = 'timestamp'\n",
    "    \n",
    "    # Inject anomalies (5% of data)\n",
    "    n_anomalies = int(len(df) * 0.05)\n",
    "    anomaly_indices = np.random.choice(len(df), n_anomalies, replace=False)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        # Pick 2-3 random metrics to make anomalous\n",
    "        n_affected = np.random.randint(2, 4)\n",
    "        affected_metrics = np.random.choice(TARGET_METRICS, n_affected, replace=False)\n",
    "        \n",
    "        for metric in affected_metrics:\n",
    "            if metric in df.columns:\n",
    "                std = df[metric].std()\n",
    "                mean = df[metric].mean()\n",
    "                # Add anomaly: 3-5 std deviations\n",
    "                multiplier = np.random.uniform(3.0, 5.0) * np.random.choice([-1, 1])\n",
    "                df.iloc[idx, df.columns.get_loc(metric)] = mean + multiplier * std\n",
    "        \n",
    "        df.iloc[idx, df.columns.get_loc('label')] = 1\n",
    "    \n",
    "    # Save for downstream notebooks\n",
    "    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(PROCESSED_DIR / 'synthetic_anomalies.parquet')\n",
    "    logger.info(f\"‚úÖ Generated and saved synthetic data: {df.shape}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nüìã DATA SUMMARY:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "metric_cols = [c for c in df.columns if c in TARGET_METRICS]\n",
    "print(f\"   TARGET_METRICS columns: {len(metric_cols)}\")\n",
    "print(f\"   Columns: {metric_cols[:5]}... + {max(0, len(metric_cols)-5)} more\")\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    print(f\"\\n   Normal samples: {(df['label'] == 0).sum()}\")\n",
    "    print(f\"   Anomalous samples: {(df['label'] == 1).sum()}\")\n",
    "\n",
    "print(f\"\\nüìä Sample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CELL - Run this after Cell 2 to verify data state\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç DIAGNOSTIC: Checking data state\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. Is 'df' defined? {'df' in dir()}\")\n",
    "print(f\"2. Is 'df' in globals? {'df' in globals()}\")\n",
    "\n",
    "if 'df' in dir():\n",
    "    print(f\"3. df is None? {df is None}\")\n",
    "    if df is not None:\n",
    "        print(f\"4. df.empty? {df.empty}\")\n",
    "        print(f\"5. df.shape: {df.shape}\")\n",
    "        print(f\"6. df.columns: {list(df.columns)[:8]}...\")\n",
    "        \n",
    "        # Check for TARGET_METRICS\n",
    "        if 'TARGET_METRICS' in dir():\n",
    "            matches = [c for c in df.columns if c in TARGET_METRICS]\n",
    "            print(f\"7. TARGET_METRICS columns found: {len(matches)}\")\n",
    "            print(f\"   Matching: {matches[:5]}...\")\n",
    "        else:\n",
    "            print(\"7. TARGET_METRICS not defined!\")\n",
    "else:\n",
    "    print(\"3. df is NOT defined - this is the problem!\")\n",
    "\n",
    "# Check the parquet file\n",
    "parquet_path = PROCESSED_DIR / 'synthetic_anomalies.parquet'\n",
    "print(f\"\\n8. Parquet file exists? {parquet_path.exists()}\")\n",
    "if parquet_path.exists():\n",
    "    temp_df = pd.read_parquet(parquet_path)\n",
    "    print(f\"   Parquet columns: {list(temp_df.columns)[:5]}...\")\n",
    "    has_target = any(c in temp_df.columns for c in TARGET_METRICS)\n",
    "    print(f\"   Has TARGET_METRICS? {has_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - ARIMA-Based Detection (FIXED shape alignment)\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def detect_anomalies_arima(series, threshold_std=2.5, metric_name=\"unknown\"):\n",
    "    \"\"\"\n",
    "    Detect anomalies using ARIMA forecasting - FIXED for shape alignment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if series is None:\n",
    "            return None, None, None, \"Series is None\"\n",
    "        \n",
    "        series_clean = series.dropna()\n",
    "        \n",
    "        if len(series_clean) < 50:\n",
    "            return None, None, None, f\"Insufficient data: {len(series_clean)} points\"\n",
    "        \n",
    "        std_val = series_clean.std()\n",
    "        if std_val == 0 or np.isnan(std_val):\n",
    "            return None, None, None, f\"Constant values (std={std_val})\"\n",
    "        \n",
    "        if np.isinf(series_clean).any():\n",
    "            return None, None, None, \"Contains infinity values\"\n",
    "        \n",
    "        # Fit ARIMA model\n",
    "        model = ARIMA(series_clean.values, order=(1, 1, 1))\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Get fitted values\n",
    "        fitted = results.fittedvalues\n",
    "        n_fitted = len(fitted)\n",
    "        \n",
    "        # FIXED: Align lengths properly - take last n_fitted values from original\n",
    "        actual_values = series_clean.values[-n_fitted:]\n",
    "        \n",
    "        # Calculate residuals (now same length)\n",
    "        residuals = actual_values - fitted\n",
    "        \n",
    "        # Detect anomalies based on residual threshold\n",
    "        residual_std = np.std(residuals)\n",
    "        if residual_std == 0 or np.isnan(residual_std):\n",
    "            return None, None, None, f\"Residual std is {residual_std}\"\n",
    "        \n",
    "        threshold = threshold_std * residual_std\n",
    "        anomaly_flags = (np.abs(residuals) > threshold).astype(int)\n",
    "        \n",
    "        # Create full prediction series aligned with original index\n",
    "        full_predictions = pd.Series(0, index=series.index)\n",
    "        \n",
    "        # Map predictions to the END of the series (where fitted values align)\n",
    "        start_idx = len(series) - n_fitted\n",
    "        for i, flag in enumerate(anomaly_flags):\n",
    "            full_predictions.iloc[start_idx + i] = flag\n",
    "        \n",
    "        return full_predictions, results, residuals, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None, None, None, f\"{type(e).__name__}: {str(e)}\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFY DATA STATE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîÑ ARIMA ANOMALY DETECTION (FIXED)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "metric_columns = [col for col in df.columns if col in TARGET_METRICS]\n",
    "\n",
    "print(f\"\\nüìä Data verification:\")\n",
    "print(f\"   DataFrame shape: {df.shape}\")\n",
    "print(f\"   TARGET_METRICS found: {len(metric_columns)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ANALYZE EACH METRIC\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìà Analyzing {len(metric_columns)} metrics...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "arima_results = {}\n",
    "arima_models = {}\n",
    "arima_errors = {}\n",
    "arima_skipped = {}\n",
    "\n",
    "for i, metric in enumerate(metric_columns):\n",
    "    series = df[metric]\n",
    "    \n",
    "    n_unique = len(series.unique())\n",
    "    std_val = series.std()\n",
    "    \n",
    "    print(f\"\\n[{i+1:2}/{len(metric_columns)}] {metric}\")\n",
    "    print(f\"      Points: {len(series)} | Unique: {n_unique} | Std: {std_val:.6f}\")\n",
    "    \n",
    "    # Skip metrics with issues\n",
    "    if std_val == 0:\n",
    "        arima_skipped[metric] = \"Constant values (std=0)\"\n",
    "        print(f\"      ‚è≠Ô∏è  SKIPPED: Constant values\")\n",
    "        continue\n",
    "    \n",
    "    if n_unique < 10:\n",
    "        arima_skipped[metric] = f\"Too few unique values ({n_unique})\"\n",
    "        print(f\"      ‚è≠Ô∏è  SKIPPED: Only {n_unique} unique values\")\n",
    "        continue\n",
    "    \n",
    "    # Run ARIMA\n",
    "    predictions, model, residuals, error = detect_anomalies_arima(series, metric_name=metric)\n",
    "    \n",
    "    if predictions is not None:\n",
    "        arima_results[metric] = predictions\n",
    "        arima_models[metric] = model\n",
    "        \n",
    "        anomalies = predictions.sum()\n",
    "        print(f\"      ‚úÖ Success! Detected {anomalies} anomalies\")\n",
    "        \n",
    "        if 'label' in df.columns:\n",
    "            p = precision_score(df['label'], predictions, zero_division=0)\n",
    "            r = recall_score(df['label'], predictions, zero_division=0)\n",
    "            f = f1_score(df['label'], predictions, zero_division=0)\n",
    "            print(f\"      üìä P={p:.3f} | R={r:.3f} | F1={f:.3f}\")\n",
    "    else:\n",
    "        arima_errors[metric] = error\n",
    "        print(f\"      ‚ùå FAILED: {error}\")\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä ARIMA RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n   ‚úÖ Successful: {len(arima_results)}/{len(metric_columns)} metrics\")\n",
    "print(f\"   ‚è≠Ô∏è  Skipped:    {len(arima_skipped)}/{len(metric_columns)} metrics\")\n",
    "print(f\"   ‚ùå Failed:     {len(arima_errors)}/{len(metric_columns)} metrics\")\n",
    "\n",
    "if arima_skipped:\n",
    "    print(f\"\\n   Skipped metrics (unsuitable for ARIMA):\")\n",
    "    for m, reason in arima_skipped.items():\n",
    "        print(f\"      - {m}: {reason}\")\n",
    "\n",
    "if arima_errors:\n",
    "    print(f\"\\n   Failed metrics:\")\n",
    "    for m, error in arima_errors.items():\n",
    "        print(f\"      - {m}: {error}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENSEMBLE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "if arima_results:\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"üîπ ENSEMBLE PREDICTIONS\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    arima_ensemble = pd.DataFrame(arima_results)\n",
    "    ensemble_any = (arima_ensemble.sum(axis=1) > 0).astype(int)\n",
    "    ensemble_majority = (arima_ensemble.sum(axis=1) > len(arima_results) / 2).astype(int)\n",
    "    \n",
    "    print(f\"\\n   Metrics in ensemble: {len(arima_results)}\")\n",
    "    print(f\"   ANY method:      {ensemble_any.sum()} anomalies ({ensemble_any.mean():.2%})\")\n",
    "    print(f\"   MAJORITY method: {ensemble_majority.sum()} anomalies ({ensemble_majority.mean():.2%})\")\n",
    "    \n",
    "    if 'label' in df.columns:\n",
    "        print(f\"\\n   üìà Ensemble Performance (ANY method):\")\n",
    "        p = precision_score(df['label'], ensemble_any, zero_division=0)\n",
    "        r = recall_score(df['label'], ensemble_any, zero_division=0)\n",
    "        f = f1_score(df['label'], ensemble_any, zero_division=0)\n",
    "        print(f\"      Precision: {p:.3f} | Recall: {r:.3f} | F1: {f:.3f}\")\n",
    "    \n",
    "    arima_preds = ensemble_any\n",
    "    arima_model = arima_models\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ ARIMA ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"\\n‚ùå No ARIMA models trained - cannot create ensemble\")\n",
    "    arima_preds = None\n",
    "    arima_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prophet-Based Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Prophet not installed. Run: pip install prophet\")\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def detect_anomalies_prophet(series, threshold_std=2.5):\n",
    "    \"\"\"\n",
    "    Detect anomalies using Prophet forecasting.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data (pandas Series with datetime index)\n",
    "        threshold_std: Number of standard deviations for anomaly threshold\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predictions, model, forecast)\n",
    "    \"\"\"\n",
    "    if not PROPHET_AVAILABLE:\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        # Prepare data for Prophet (requires 'ds' and 'y' columns)\n",
    "        prophet_df = pd.DataFrame({\n",
    "            'ds': series.index,\n",
    "            'y': series.values\n",
    "        }).dropna()\n",
    "        \n",
    "        if len(prophet_df) < 50:\n",
    "            return None, None, None\n",
    "        \n",
    "        # Fit Prophet model (suppress logging)\n",
    "        import logging\n",
    "        logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "        \n",
    "        model = Prophet(\n",
    "            daily_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            yearly_seasonality=False,  # Not enough data\n",
    "            changepoint_prior_scale=0.05\n",
    "        )\n",
    "        model.fit(prophet_df)\n",
    "        \n",
    "        # Make predictions\n",
    "        forecast = model.predict(prophet_df[['ds']])\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = prophet_df['y'].values - forecast['yhat'].values\n",
    "        threshold = threshold_std * np.std(residuals)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        predictions = (np.abs(residuals) > threshold).astype(int)\n",
    "        \n",
    "        # Create full prediction series aligned with original index\n",
    "        full_predictions = pd.Series(0, index=series.index)\n",
    "        for i, (idx, row) in enumerate(prophet_df.iterrows()):\n",
    "            if i < len(predictions):\n",
    "                full_predictions.loc[row['ds']] = predictions[i]\n",
    "        \n",
    "        return full_predictions, model, forecast\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Prophet error: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# =============================================================================\n",
    "# ANALYZE ALL 16 METRICS WITH PROPHET\n",
    "# =============================================================================\n",
    "\n",
    "if PROPHET_AVAILABLE:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ PROPHET ANOMALY DETECTION ON ALL METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    prophet_results = {}\n",
    "    prophet_models = {}\n",
    "    \n",
    "    # Get only metric columns (exclude 'label')\n",
    "    metric_columns = [col for col in df.columns if col in TARGET_METRICS]\n",
    "    \n",
    "    # Prophet is slow, so we'll analyze a subset or all based on user choice\n",
    "    ANALYZE_ALL_METRICS = True  # Set to False to only analyze top 5 metrics\n",
    "    \n",
    "    if not ANALYZE_ALL_METRICS:\n",
    "        # Prioritize most important metrics for time series analysis\n",
    "        priority_metrics = [\n",
    "            'pod_cpu_usage',\n",
    "            'pod_memory_usage', \n",
    "            'container_restart_rate_1h',\n",
    "            'apiserver_error_rate',\n",
    "            'deployment_unavailable'\n",
    "        ]\n",
    "        metric_columns = [m for m in priority_metrics if m in metric_columns]\n",
    "        print(f\"\\n‚ö° Fast mode: Analyzing {len(metric_columns)} priority metrics\")\n",
    "    else:\n",
    "        print(f\"\\nüìä Full mode: Analyzing all {len(metric_columns)} metrics (this may take a few minutes)\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        print(f\"\\n[{i+1:2}/{len(metric_columns)}] {metric}...\", end=\" \")\n",
    "        \n",
    "        # Get the time series for this metric\n",
    "        series = df[metric]\n",
    "        \n",
    "        # Run Prophet detection\n",
    "        predictions, model, forecast = detect_anomalies_prophet(series)\n",
    "        \n",
    "        if predictions is not None:\n",
    "            prophet_results[metric] = predictions\n",
    "            prophet_models[metric] = model\n",
    "            \n",
    "            anomalies_detected = predictions.sum()\n",
    "            print(f\"‚úÖ {anomalies_detected} anomalies\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # COMBINE RESULTS - ENSEMBLE ACROSS METRICS\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä PROPHET ENSEMBLE RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if prophet_results:\n",
    "        # Combine all metric predictions\n",
    "        prophet_ensemble = pd.DataFrame(prophet_results)\n",
    "        \n",
    "        # Ensemble prediction: anomaly if ANY metric flags it\n",
    "        ensemble_any = (prophet_ensemble.sum(axis=1) > 0).astype(int)\n",
    "        \n",
    "        # Ensemble prediction: anomaly if MAJORITY of metrics flag it\n",
    "        ensemble_majority = (prophet_ensemble.sum(axis=1) > len(prophet_results) / 2).astype(int)\n",
    "        \n",
    "        print(f\"\\n   Metrics analyzed: {len(prophet_results)}\")\n",
    "        print(f\"   Total data points: {len(df)}\")\n",
    "        \n",
    "        print(f\"\\n   üîπ ANY metric anomaly: {ensemble_any.sum()} anomalies ({ensemble_any.mean():.2%})\")\n",
    "        print(f\"   üîπ MAJORITY metrics anomaly: {ensemble_majority.sum()} anomalies ({ensemble_majority.mean():.2%})\")\n",
    "        \n",
    "        if 'label' in df.columns:\n",
    "            print(f\"\\n   üìà Performance (ANY method):\")\n",
    "            precision = precision_score(df['label'], ensemble_any, zero_division=0)\n",
    "            recall = recall_score(df['label'], ensemble_any, zero_division=0)\n",
    "            f1 = f1_score(df['label'], ensemble_any, zero_division=0)\n",
    "            print(f\"      Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n",
    "        \n",
    "        # Store for later use\n",
    "        prophet_preds = ensemble_any\n",
    "        prophet_model = prophet_models\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"‚úÖ Prophet analysis complete!\")\n",
    "        print(\"=\" * 70)\n",
    "    else:\n",
    "        print(\"‚ùå No Prophet models were successfully trained\")\n",
    "        prophet_preds = None\n",
    "        prophet_model = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Prophet not available - skipping Prophet analysis\")\n",
    "    print(\"   Install with: pip install prophet\")\n",
    "    prophet_preds = None\n",
    "    prophet_model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell - TimeSeriesEnsemble class and Save Models\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "\n",
    "class TimeSeriesEnsemble(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wrapper class that combines ARIMA and Prophet models for KServe compatibility.\n",
    "    KServe requires a single .pkl file, not multiple model files.\n",
    "    \n",
    "    Now supports multiple models (one per metric) for multi-metric anomaly detection.\n",
    "    \"\"\"\n",
    "    def __init__(self, arima_models=None, prophet_models=None):\n",
    "        # Support both single model (legacy) and dict of models (new)\n",
    "        self.arima_models = arima_models or {}\n",
    "        self.prophet_models = prophet_models or {}\n",
    "        \n",
    "        # List of metrics we have models for\n",
    "        self.metrics = list(set(\n",
    "            list(self.arima_models.keys()) + \n",
    "            list(self.prophet_models.keys())\n",
    "        ))\n",
    "    \n",
    "    def predict(self, X, periods=None):\n",
    "        \"\"\"\n",
    "        Make predictions using both models and return ensemble result.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (DataFrame with metric columns, or dict of series)\n",
    "            periods: Number of periods to forecast (for time series)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with predictions from both models per metric\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'arima': {},\n",
    "            'prophet': {},\n",
    "            'ensemble': {},\n",
    "            'anomalies': {}\n",
    "        }\n",
    "        \n",
    "        # Handle DataFrame input\n",
    "        if hasattr(X, 'columns'):\n",
    "            metrics_to_predict = [col for col in X.columns if col in self.metrics]\n",
    "        else:\n",
    "            metrics_to_predict = self.metrics\n",
    "        \n",
    "        for metric in metrics_to_predict:\n",
    "            # ARIMA predictions\n",
    "            if metric in self.arima_models and self.arima_models[metric] is not None:\n",
    "                try:\n",
    "                    model = self.arima_models[metric]\n",
    "                    forecast = model.forecast(steps=periods or 10)\n",
    "                    results['arima'][metric] = forecast\n",
    "                except Exception as e:\n",
    "                    results['arima'][f'{metric}_error'] = str(e)\n",
    "            \n",
    "            # Prophet predictions\n",
    "            if metric in self.prophet_models and self.prophet_models[metric] is not None:\n",
    "                try:\n",
    "                    model = self.prophet_models[metric]\n",
    "                    future = model.make_future_dataframe(periods=periods or 10, freq='1min')\n",
    "                    forecast = model.predict(future)\n",
    "                    results['prophet'][metric] = forecast['yhat'].values[-periods:] if periods else forecast['yhat'].values\n",
    "                except Exception as e:\n",
    "                    results['prophet'][f'{metric}_error'] = str(e)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"Return summary of loaded models.\"\"\"\n",
    "        return {\n",
    "            'arima_count': len(self.arima_models),\n",
    "            'prophet_count': len(self.prophet_models),\n",
    "            'arima_metrics': list(self.arima_models.keys()),\n",
    "            'prophet_metrics': list(self.prophet_models.keys()),\n",
    "            'total_metrics': len(self.metrics)\n",
    "        }\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'arima_models': self.arima_models,\n",
    "            'prophet_models': self.prophet_models\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "print(\"‚úÖ TimeSeriesEnsemble class defined for KServe compatibility\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE AND SAVE ENSEMBLE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üíæ SAVING MODELS FOR KSERVE DEPLOYMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the model dictionaries from earlier cells\n",
    "arima_models_dict = arima_models if 'arima_models' in dir() else {}\n",
    "prophet_models_dict = prophet_models if 'prophet_models' in dir() else {}\n",
    "\n",
    "print(f\"\\nüìä Models available:\")\n",
    "print(f\"   ARIMA models:   {len(arima_models_dict)} metrics\")\n",
    "print(f\"   Prophet models: {len(prophet_models_dict)} metrics\")\n",
    "\n",
    "# Create ensemble wrapper\n",
    "ensemble_model = TimeSeriesEnsemble(\n",
    "    arima_models=arima_models_dict,\n",
    "    prophet_models=prophet_models_dict\n",
    ")\n",
    "\n",
    "# Verify\n",
    "summary = ensemble_model.get_model_summary()\n",
    "print(f\"\\nüì¶ TimeSeriesEnsemble created:\")\n",
    "print(f\"   ARIMA metrics:   {summary['arima_metrics'][:3]}... ({summary['arima_count']} total)\")\n",
    "print(f\"   Prophet metrics: {summary['prophet_metrics'][:3]}... ({summary['prophet_count']} total)\")\n",
    "\n",
    "# Clean up old files in model directory\n",
    "for old_file in MODEL_DIR.glob('*.pkl'):\n",
    "    old_file.unlink()\n",
    "    print(f\"   üóëÔ∏è  Removed old: {old_file.name}\")\n",
    "\n",
    "# Save single .pkl file (KServe requirement)\n",
    "model_path = MODEL_DIR / 'model.pkl'\n",
    "joblib.dump(ensemble_model, model_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved: {model_path}\")\n",
    "print(f\"   Size: {model_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Verify only ONE .pkl file exists\n",
    "pkl_files = list(MODEL_DIR.glob('*.pkl'))\n",
    "if len(pkl_files) != 1:\n",
    "    raise RuntimeError(f\"Expected 1 .pkl file, found {len(pkl_files)}: {pkl_files}\")\n",
    "print(f\"   Files in directory: {len(pkl_files)} ‚úì (KServe requirement met)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'label': df['label'],\n",
    "    'arima_pred': arima_preds if arima_preds is not None else 0,\n",
    "    'prophet_pred': prophet_preds if prophet_preds is not None else 0\n",
    "})\n",
    "\n",
    "# Combined ensemble: anomaly if EITHER model flags it\n",
    "results_df['combined_pred'] = (\n",
    "    (results_df['arima_pred'] == 1) | (results_df['prophet_pred'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "predictions_path = PROCESSED_DIR / 'timeseries_predictions.parquet'\n",
    "results_df.to_parquet(predictions_path)\n",
    "print(f\"\\nüíæ Predictions saved: {predictions_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"üìà MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for name, preds in [('ARIMA', arima_preds), ('Prophet', prophet_preds), ('Combined', results_df['combined_pred'])]:\n",
    "    if preds is not None:\n",
    "        p = precision_score(df['label'], preds, zero_division=0)\n",
    "        r = recall_score(df['label'], preds, zero_division=0)\n",
    "        f = f1_score(df['label'], preds, zero_division=0)\n",
    "        n_anom = preds.sum()\n",
    "        print(f\"   {name:10}: {n_anom:4} anomalies | P={p:.3f} R={r:.3f} F1={f:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# S3 UPLOAD (OPTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    from common_functions import upload_model_to_s3, test_s3_connection\n",
    "    if test_s3_connection():\n",
    "        upload_model_to_s3(\n",
    "            str(model_path), \n",
    "            s3_key=f'models/anomaly-detection/{MODEL_NAME}/model.pkl'\n",
    "        )\n",
    "        print(f\"\\n‚òÅÔ∏è  Uploaded to S3: s3://first.bucket/models/anomaly-detection/{MODEL_NAME}/model.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  S3 upload skipped: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîç VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Reload and verify\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_summary = loaded_model.get_model_summary()\n",
    "\n",
    "print(f\"\\n   ‚úÖ Model reloaded successfully\")\n",
    "print(f\"   ‚úÖ ARIMA models: {loaded_summary['arima_count']}\")\n",
    "print(f\"   ‚úÖ Prophet models: {loaded_summary['prophet_count']}\")\n",
    "print(f\"   ‚úÖ Total metrics: {loaded_summary['total_metrics']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ MODEL SAVE COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n   Model name: {MODEL_NAME}\")\n",
    "print(f\"   Model path: {model_path}\")\n",
    "print(f\"   Deploy to KServe with: storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify outputs - KServe compatible structure\n",
    "model_path = MODEL_DIR / 'model.pkl'\n",
    "\n",
    "# Check model file exists\n",
    "assert model_path.exists(), f\"Model not saved: {model_path}\"\n",
    "\n",
    "# Check predictions saved\n",
    "assert (PROCESSED_DIR / 'timeseries_predictions.parquet').exists(), \"Predictions not saved\"\n",
    "\n",
    "# Verify KServe requirement: EXACTLY ONE .pkl file\n",
    "pkl_files = list(MODEL_DIR.glob('*.pkl'))\n",
    "assert len(pkl_files) == 1, f\"ERROR: Expected 1 .pkl file, found {len(pkl_files)}: {pkl_files}\"\n",
    "\n",
    "# Verify can load and use the model\n",
    "loaded_model = joblib.load(model_path)\n",
    "assert hasattr(loaded_model, 'predict'), \"Model doesn't have predict method\"\n",
    "assert hasattr(loaded_model, 'arima_model'), \"Missing arima_model attribute\"\n",
    "assert hasattr(loaded_model, 'prophet_model'), \"Missing prophet_model attribute\"\n",
    "\n",
    "logger.info(\"‚úÖ All validations passed\")\n",
    "print(f\"\\n‚úÖ KServe Validation Complete:\")\n",
    "print(f\"   Model path: {model_path}\")\n",
    "print(f\"   File count: {len(pkl_files)} (correct - must be 1)\")\n",
    "print(f\"   Model type: {type(loaded_model).__name__}\")\n",
    "print(f\"   Predictions: {PROCESSED_DIR / 'timeseries_predictions.parquet'}\")\n",
    "print(f\"\\nüéØ Ready for KServe deployment!\")\n",
    "print(f\"   Deploy with: oc apply -f <inference-service.yaml>\")\n",
    "print(f\"   storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Synthetic anomalies from `synthetic-anomaly-generation.ipynb`\n",
    "- **Output**: Time series models for `ensemble-anomaly-methods.ipynb`\n",
    "- **Coordination Engine**: Models can be deployed for real-time detection\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review model performance metrics\n",
    "2. Proceed to `lstm-based-prediction.ipynb` for deep learning approach\n",
    "3. Compare with ensemble methods\n",
    "4. Deploy best model to coordination engine\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [ARIMA Documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html)\n",
    "- [Prophet Documentation](https://facebook.github.io/prophet/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

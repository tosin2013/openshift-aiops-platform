{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Anomaly Detection\n",
    "\n",
    "## Overview\n",
    "This notebook implements time series anomaly detection using ARIMA and Prophet forecasting methods. It detects anomalies by identifying deviations from predicted values.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `synthetic-anomaly-generation.ipynb` (Phase 1)\n",
    "- Libraries: statsmodels, prophet, pandas, numpy\n",
    "- Synthetic dataset: `/opt/app-root/src/data/processed/synthetic_anomalies.parquet`\n",
    "\n",
    "## Why We Use Synthetic Data\n",
    "\n",
    "### The Problem: Real Anomalies Are Rare\n",
    "In production OpenShift clusters:\n",
    "- Anomalies occur <1% of the time\n",
    "- Collecting 1000 labeled anomalies takes months/years\n",
    "- Different anomaly types are hard to capture\n",
    "- Can't deliberately cause failures to collect data\n",
    "\n",
    "### The Solution: Synthetic Anomalies\n",
    "We generate synthetic anomalies because:\n",
    "- ‚úÖ Create 1000+ labeled anomalies in minutes\n",
    "- ‚úÖ Control anomaly types and severity\n",
    "- ‚úÖ Ensure balanced training data (50% normal, 50% anomaly)\n",
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Anomaly Detection\n",
    "\n",
    "## Overview\n",
    "This notebook implements time series anomaly detection using ARIMA and Prophet forecasting methods. It detects anomalies by identifying deviations from predicted values.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `synthetic-anomaly-generation.ipynb` (Phase 1)\n",
    "- Libraries: statsmodels, prophet, pandas, numpy\n",
    "- Synthetic dataset: `/opt/app-root/src/data/processed/synthetic_anomalies.parquet`\n",
    "\n",
    "## Why We Use Synthetic Data\n",
    "\n",
    "### The Problem: Real Anomalies Are Rare\n",
    "In production OpenShift clusters:\n",
    "- Anomalies occur <1% of the time\n",
    "- Collecting 1000 labeled anomalies takes months/years\n",
    "- Different anomaly types are hard to capture\n",
    "- Can't deliberately cause failures to collect data\n",
    "\n",
    "### The Solution: Synthetic Anomalies\n",
    "We generate synthetic anomalies because:\n",
    "- ‚úÖ Create 1000+ labeled anomalies in minutes\n",
    "- ‚úÖ Control anomaly types and severity\n",
    "- ‚úÖ Ensure balanced training data (50% normal, 50% anomaly)\n",
    "- ‚úÖ Reproducible and testable\n",
    "- ‚úÖ Models trained on synthetic data generalize to real anomalies\n",
    "\n",
    "### Machine Learning Best Practice\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement ARIMA forecasting on synthetic data\n",
    "- Use Prophet for time series analysis\n",
    "- Detect anomalies via forecast deviations\n",
    "- Handle seasonal patterns\n",
    "- Evaluate detection performance with labeled data\n",
    "\n",
    "## Key Concepts\n",
    "- **ARIMA**: AutoRegressive Integrated Moving Average\n",
    "- **Prophet**: Facebook's time series forecasting tool\n",
    "- **Forecast Error**: Deviation between actual and predicted values\n",
    "- **Seasonality**: Repeating patterns in time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Why Synthetic Data for Training?\n",
    "- **He & Garcia (2009)**: \"Learning from Imbalanced Data\" - https://ieeexplore.ieee.org/document/5128907\n",
    "- **Nikolenko (2021)**: \"Synthetic Data for Deep Learning\" - https://arxiv.org/abs/1909.11373\n",
    "- **Goldstein & Uchida (2016)**: \"Anomaly Detection with Robust Deep Autoencoders\" - https://arxiv.org/abs/1511.08747\n",
    "\n",
    "### Time Series Anomaly Detection\n",
    "- **Malhotra et al. (2016)**: \"Time Series Anomaly Detection with LSTM Networks\" - https://arxiv.org/abs/1607.00148\n",
    "- **Taylor & Letham (2018)**: \"Forecasting at Scale (Prophet)\" - https://peerj.com/articles/3190\n",
    "- **Box & Jenkins (1970)**: \"Time Series Analysis, Forecasting and Control (ARIMA)\" - Classic reference\n",
    "\n",
    "### Key Takeaway\n",
    "Synthetic data provides labeled training examples that allow us to:\n",
    "1. Train models with known ground truth\n",
    "2. Evaluate performance with precision, recall, and F1 scores\n",
    "3. Ensure reproducible and testable results\n",
    "4. Build models that generalize to real-world anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ARIMA (statsmodels) available\n",
      "‚úÖ Utils path found: /opt/app-root/src/openshift-aiops-platform/notebooks/utils\n",
      "‚úÖ Common functions module loaded successfully\n",
      "üìÅ Data directory: /opt/app-root/src/data\n",
      "üóÉÔ∏è Models directory: /opt/app-root/src/models\n",
      "‚öôÔ∏è Configuration directory: /opt/app-root/src/.config\n",
      "üìä Quality thresholds: 4 defined\n",
      "üîß Available functions: setup_environment, validate_data_quality, plot_metric_overview\n",
      "üíæ Storage functions: save_processed_data, load_processed_data\n",
      "üìä Prometheus functions: query_prometheus, test_prometheus_connection\n",
      "‚òÅÔ∏è S3 functions: upload_model_to_s3, download_model_from_s3, save_model_with_s3_backup\n",
      "üéØ Use setup_environment() to initialize your notebook environment\n",
      "üîç Use test_prometheus_connection() to verify Prometheus access\n",
      "‚òÅÔ∏è Use test_s3_connection() to verify S3 model storage access\n",
      "‚úÖ Common functions imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Environment ready: {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models', 'workbench_detected': True, 'storage_available_gb': 19.488563537597656, 'python_version': '3.11.11 (main, Aug 21 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'key_libraries': {'torch': '2.6.0+cu126', 'pandas': '2.2.3', 'numpy': '2.2.6'}}\n",
      "INFO:__main__:Data directory: /opt/app-root/src/data\n",
      "INFO:__main__:Models directory: /mnt/models/timeseries-predictor\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import ARIMA\n",
    "try:\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    ARIMA_AVAILABLE = True\n",
    "    print(\"‚úÖ ARIMA (statsmodels) available\")\n",
    "except ImportError:\n",
    "    ARIMA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è statsmodels not available - ARIMA will use fallback\")\n",
    "\n",
    "\n",
    "# Setup path for utils module - works from any directory\n",
    "def find_utils_path():\n",
    "    \"\"\"Find utils path regardless of current working directory\"\"\"\n",
    "    possible_paths = [\n",
    "        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n",
    "        Path.cwd() / 'notebooks' / 'utils',\n",
    "        Path.cwd().parent / 'utils',\n",
    "        Path('/workspace/repo/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n",
    "    ]\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists() and (p / 'common_functions.py').exists():\n",
    "            return str(p)\n",
    "    current = Path.cwd()\n",
    "    for _ in range(5):\n",
    "        utils_path = current / 'notebooks' / 'utils'\n",
    "        if utils_path.exists():\n",
    "            return str(utils_path)\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "utils_path = find_utils_path()\n",
    "if utils_path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "    print(f\"‚úÖ Utils path found: {utils_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Utils path not found - will use fallback implementations\")\n",
    "\n",
    "# Try to import common functions, with fallback\n",
    "try:\n",
    "    from common_functions import setup_environment\n",
    "    print(\"‚úÖ Common functions imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Common functions not available: {e}\")\n",
    "    def setup_environment():\n",
    "        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n",
    "        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n",
    "        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Setup environment\n",
    "env_info = setup_environment()\n",
    "logger.info(f\"Environment ready: {env_info}\")\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('/opt/app-root/src/data')\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# Use /mnt/models for persistent storage (model-storage-pvc)\n",
    "# Fallback to local for development outside cluster\n",
    "MODELS_DIR = Path('/mnt/models') if Path('/mnt/models').exists() else Path('/opt/app-root/src/models')\n",
    "\n",
    "# Create KServe-compatible subdirectory structure\n",
    "# CHANGED: Use unique model name to avoid conflict with isolation-forest's anomaly-detector model\n",
    "MODEL_NAME = 'timeseries-predictor'\n",
    "MODEL_DIR = MODELS_DIR / MODEL_NAME\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Data directory: {DATA_DIR}\")\n",
    "logger.info(f\"Models directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Load Prometheus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Target metrics for time series analysis: 16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# DEFINE OUR 16 WORKING METRICS (same as other notebooks)\n",
    "# =============================================================================\n",
    "\n",
    "TARGET_METRICS = [\n",
    "    # Resource Metrics (5)\n",
    "    'node_memory_utilization',\n",
    "    'pod_cpu_usage',\n",
    "    'pod_memory_usage',\n",
    "    'alt_cpu_usage',\n",
    "    'alt_memory_usage',\n",
    "    \n",
    "    # Stability Metrics (3)\n",
    "    'container_restart_count',\n",
    "    'container_restart_rate_1h',\n",
    "    'deployment_unavailable',\n",
    "    \n",
    "    # Pod Status Metrics (4)\n",
    "    'namespace_pod_count',\n",
    "    'pods_pending',\n",
    "    'pods_running',\n",
    "    'pods_failed',\n",
    "    \n",
    "    # Storage Metrics (2)\n",
    "    'persistent_volume_usage',\n",
    "    'cluster_resource_quota',\n",
    "    \n",
    "    # Control Plane Metrics (2)\n",
    "    'apiserver_request_total',\n",
    "    'apiserver_error_rate',\n",
    "]\n",
    "\n",
    "# Prometheus queries for real data collection\n",
    "PROMETHEUS_QUERIES = {\n",
    "    'node_memory_utilization': 'instance:node_memory_utilisation:ratio * 100',\n",
    "    'pod_cpu_usage': 'sum by (pod, namespace) (node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate)',\n",
    "    'pod_memory_usage': 'sum by (pod, namespace) (container_memory_working_set_bytes{container!=\"POD\", container!=\"\"})',\n",
    "    'alt_cpu_usage': 'sum(rate(container_cpu_usage_seconds_total{container!=\"POD\", container!=\"\"}[5m])) by (pod, namespace)',\n",
    "    'alt_memory_usage': 'sum(container_memory_rss{container!=\"POD\", container!=\"\"}) by (pod, namespace)',\n",
    "    'container_restart_count': 'sum by (pod, namespace, container) (kube_pod_container_status_restarts_total)',\n",
    "    'container_restart_rate_1h': 'sum by (pod, namespace) (increase(kube_pod_container_status_restarts_total[1h]))',\n",
    "    'deployment_unavailable': 'sum by (deployment, namespace) (kube_deployment_status_replicas_unavailable)',\n",
    "    'namespace_pod_count': 'sum by (namespace) (kube_pod_status_phase)',\n",
    "    'pods_pending': 'sum by (namespace) (kube_pod_status_phase{phase=\"Pending\"})',\n",
    "    'pods_running': 'sum by (namespace) (kube_pod_status_phase{phase=\"Running\"})',\n",
    "    'pods_failed': 'sum by (namespace) (kube_pod_status_phase{phase=\"Failed\"})',\n",
    "    'persistent_volume_usage': 'kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100',\n",
    "    'cluster_resource_quota': 'kube_resourcequota',\n",
    "    'apiserver_request_total': 'sum(rate(apiserver_request_total[5m]))',\n",
    "    'apiserver_error_rate': 'sum(rate(apiserver_request_total{code=~\"5..\"}[5m])) / sum(rate(apiserver_request_total[5m])) * 100',\n",
    "}\n",
    "\n",
    "print(f\"üìä Target metrics for time series analysis: {len(TARGET_METRICS)}\")\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "class PrometheusClient:\n",
    "    \"\"\"Client for querying Prometheus in OpenShift.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        token_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'\n",
    "        self.token = None\n",
    "        if os.path.exists(token_path):\n",
    "            with open(token_path, 'r') as f:\n",
    "                self.token = f.read().strip()\n",
    "        \n",
    "        self.base_url = 'https://prometheus-k8s.openshift-monitoring.svc.cluster.local:9091'\n",
    "        self.session = requests.Session()\n",
    "        if self.token:\n",
    "            self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n",
    "        self.session.verify = False\n",
    "        \n",
    "        import urllib3\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        \n",
    "        # Test connection\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/api/v1/status/config\", timeout=5)\n",
    "            self.connected = response.status_code == 200\n",
    "        except:\n",
    "            self.connected = False\n",
    "    \n",
    "    def query_range(self, query, start, end, step='1m'):\n",
    "        if not self.connected:\n",
    "            return None\n",
    "        \n",
    "        url = f\"{self.base_url}/api/v1/query_range\"\n",
    "        params = {'query': query, 'start': start, 'end': end, 'step': step}\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except:\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîÑ LOADING TIME SERIES DATA\n",
      "======================================================================\n",
      "   Duration: 24 hours\n",
      "   Metrics: 16\n",
      "   Use real data: True\n",
      "   Prometheus connected: True\n",
      "\n",
      "üìä Loading 16 metrics...\n",
      "--------------------------------------------------\n",
      "   ‚úÖ [ 1/16] node_memory_utilization: REAL (1441 points)\n",
      "   ‚úÖ [ 2/16] pod_cpu_usage: REAL (1441 points)\n",
      "   ‚úÖ [ 3/16] pod_memory_usage: REAL (1441 points)\n",
      "   ‚úÖ [ 4/16] alt_cpu_usage: REAL (1441 points)\n",
      "   ‚úÖ [ 5/16] alt_memory_usage: REAL (1441 points)\n",
      "   ‚úÖ [ 6/16] container_restart_count: REAL (1441 points)\n",
      "   ‚úÖ [ 7/16] container_restart_rate_1h: REAL (1441 points)\n",
      "   ‚úÖ [ 8/16] deployment_unavailable: REAL (1441 points)\n",
      "   ‚úÖ [ 9/16] namespace_pod_count: REAL (1441 points)\n",
      "   ‚úÖ [10/16] pods_pending: REAL (1441 points)\n",
      "   ‚úÖ [11/16] pods_running: REAL (1441 points)\n",
      "   ‚úÖ [12/16] pods_failed: REAL (1441 points)\n",
      "   ‚úÖ [13/16] persistent_volume_usage: REAL (1441 points)\n",
      "   ‚úÖ [14/16] cluster_resource_quota: REAL (1441 points)\n",
      "   ‚úÖ [15/16] apiserver_request_total: REAL (1441 points)\n",
      "   ‚úÖ [16/16] apiserver_error_rate: REAL (1441 points)\n",
      "\n",
      "======================================================================\n",
      "üìä DATA LOADING SUMMARY\n",
      "======================================================================\n",
      "   Total metrics: 16\n",
      "   ‚úÖ REAL data: 16 metrics\n",
      "   üìä SYNTHETIC data: 0 metrics\n",
      "   Data points: 1,441\n",
      "   Anomalies injected: 43 (3.0%)\n",
      "======================================================================\n",
      "\n",
      "üìã DATA LOADED:\n",
      "   Shape: (1441, 17)\n",
      "   Columns: ['node_memory_utilization', 'pod_cpu_usage', 'pod_memory_usage', 'alt_cpu_usage', 'alt_memory_usage']... + 12 more\n",
      "   Time range: 2026-01-14 06:28:08.273704 to 2026-01-15 06:28:08.273704\n",
      "\n",
      "   Normal samples: 1398\n",
      "   Anomalous samples: 43\n",
      "\n",
      "üìä Sample data:\n",
      "                            node_memory_utilization  pod_cpu_usage  \\\n",
      "timestamp                                                            \n",
      "2026-01-14 06:28:08.273704                21.191408       0.029047   \n",
      "2026-01-14 06:29:08.273704                21.191408       0.029047   \n",
      "2026-01-14 06:30:08.273704                21.191408       0.029047   \n",
      "2026-01-14 06:31:08.273704                21.191408       0.029047   \n",
      "2026-01-14 06:32:08.273704                21.191408       0.029047   \n",
      "\n",
      "                            pod_memory_usage  alt_cpu_usage  alt_memory_usage  \\\n",
      "timestamp                                                                       \n",
      "2026-01-14 06:28:08.273704      1.119256e+08            0.0      5.748508e+07   \n",
      "2026-01-14 06:29:08.273704      1.119256e+08            0.0      5.748508e+07   \n",
      "2026-01-14 06:30:08.273704      1.119256e+08            0.0      5.748508e+07   \n",
      "2026-01-14 06:31:08.273704      1.119256e+08            0.0      5.748508e+07   \n",
      "2026-01-14 06:32:08.273704      1.119256e+08            0.0      5.748508e+07   \n",
      "\n",
      "                            container_restart_count  \\\n",
      "timestamp                                             \n",
      "2026-01-14 06:28:08.273704                      8.0   \n",
      "2026-01-14 06:29:08.273704                      8.0   \n",
      "2026-01-14 06:30:08.273704                      8.0   \n",
      "2026-01-14 06:31:08.273704                      8.0   \n",
      "2026-01-14 06:32:08.273704                      8.0   \n",
      "\n",
      "                            container_restart_rate_1h  deployment_unavailable  \\\n",
      "timestamp                                                                       \n",
      "2026-01-14 06:28:08.273704                        0.0                0.070652   \n",
      "2026-01-14 06:29:08.273704                        0.0                0.070652   \n",
      "2026-01-14 06:30:08.273704                        0.0                0.070652   \n",
      "2026-01-14 06:31:08.273704                        0.0                0.070652   \n",
      "2026-01-14 06:32:08.273704                        0.0                0.070652   \n",
      "\n",
      "                            namespace_pod_count  pods_pending  pods_running  \\\n",
      "timestamp                                                                     \n",
      "2026-01-14 06:28:08.273704             7.162162      0.121622      5.175676   \n",
      "2026-01-14 06:29:08.273704             7.162162      0.121622      5.175676   \n",
      "2026-01-14 06:30:08.273704             7.162162      0.121622      5.175676   \n",
      "2026-01-14 06:31:08.273704             7.162162      0.121622      5.175676   \n",
      "2026-01-14 06:32:08.273704             7.162162      0.121622      5.175676   \n",
      "\n",
      "                            pods_failed  persistent_volume_usage  \\\n",
      "timestamp                                                          \n",
      "2026-01-14 06:28:08.273704     0.094595                 0.050523   \n",
      "2026-01-14 06:29:08.273704     0.094595                 0.050523   \n",
      "2026-01-14 06:30:08.273704     0.094595                 0.050523   \n",
      "2026-01-14 06:31:08.273704     0.094595                 0.050523   \n",
      "2026-01-14 06:32:08.273704     0.094595                 0.050523   \n",
      "\n",
      "                            cluster_resource_quota  apiserver_request_total  \\\n",
      "timestamp                                                                     \n",
      "2026-01-14 06:28:08.273704                     0.0                62.627381   \n",
      "2026-01-14 06:29:08.273704                     0.0                62.627381   \n",
      "2026-01-14 06:30:08.273704                     0.0                62.627381   \n",
      "2026-01-14 06:31:08.273704                     0.0                62.627381   \n",
      "2026-01-14 06:32:08.273704                     0.0                62.627381   \n",
      "\n",
      "                            apiserver_error_rate  label  \n",
      "timestamp                                                \n",
      "2026-01-14 06:28:08.273704              0.104303      0  \n",
      "2026-01-14 06:29:08.273704              0.104303      0  \n",
      "2026-01-14 06:30:08.273704              0.104303      0  \n",
      "2026-01-14 06:31:08.273704              0.104303      0  \n",
      "2026-01-14 06:32:08.273704              0.104303      0  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def load_timeseries_data(duration_hours=24, use_real_data=True):\n",
    "    \"\"\"\n",
    "    Load time series data for anomaly detection.\n",
    "    \n",
    "    For ARIMA/Prophet, we need univariate time series, so we'll create\n",
    "    a DataFrame where each column is one metric's aggregated values over time.\n",
    "    \n",
    "    Args:\n",
    "        duration_hours: Hours of historical data\n",
    "        use_real_data: Try Prometheus first, fallback to synthetic\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with timestamp index and metric columns\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ LOADING TIME SERIES DATA\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Duration: {duration_hours} hours\")\n",
    "    print(f\"   Metrics: {len(TARGET_METRICS)}\")\n",
    "    print(f\"   Use real data: {use_real_data}\")\n",
    "    \n",
    "    # Try to connect to Prometheus\n",
    "    prometheus = None\n",
    "    if use_real_data:\n",
    "        prometheus = PrometheusClient()\n",
    "        print(f\"   Prometheus connected: {prometheus.connected}\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=duration_hours)\n",
    "    \n",
    "    # Create time index (1-minute intervals)\n",
    "    time_index = pd.date_range(start=start_time, end=end_time, freq='1min')\n",
    "    \n",
    "    # Initialize DataFrame\n",
    "    df = pd.DataFrame(index=time_index)\n",
    "    df.index.name = 'timestamp'\n",
    "    \n",
    "    data_sources = {}\n",
    "    \n",
    "    print(f\"\\nüìä Loading {len(TARGET_METRICS)} metrics...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, metric in enumerate(TARGET_METRICS):\n",
    "        real_data_loaded = False\n",
    "        \n",
    "        if prometheus and prometheus.connected and metric in PROMETHEUS_QUERIES:\n",
    "            query = PROMETHEUS_QUERIES[metric]\n",
    "            result = prometheus.query_range(\n",
    "                query,\n",
    "                start_time.timestamp(),\n",
    "                end_time.timestamp(),\n",
    "                step='1m'\n",
    "            )\n",
    "            \n",
    "            if result and result.get('status') == 'success':\n",
    "                data = result.get('data', {}).get('result', [])\n",
    "                if data:\n",
    "                    # Parse and aggregate Prometheus data\n",
    "                    rows = []\n",
    "                    for series in data:\n",
    "                        for ts, value in series.get('values', []):\n",
    "                            try:\n",
    "                                rows.append({\n",
    "                                    'timestamp': pd.to_datetime(ts, unit='s'),\n",
    "                                    'value': float(value) if value != 'NaN' else np.nan\n",
    "                                })\n",
    "                            except:\n",
    "                                pass\n",
    "                    \n",
    "                    if rows:\n",
    "                        metric_df = pd.DataFrame(rows)\n",
    "                        # Aggregate by timestamp (mean across all series)\n",
    "                        metric_series = metric_df.groupby('timestamp')['value'].mean()\n",
    "                        # Resample to our time index\n",
    "                        metric_series = metric_series.reindex(time_index, method='nearest')\n",
    "                        df[metric] = metric_series\n",
    "                        data_sources[metric] = 'REAL'\n",
    "                        real_data_loaded = True\n",
    "                        print(f\"   ‚úÖ [{i+1:2}/{len(TARGET_METRICS)}] {metric}: REAL ({metric_series.notna().sum()} points)\")\n",
    "        \n",
    "        if not real_data_loaded:\n",
    "            # Generate synthetic time series with realistic patterns\n",
    "            n_points = len(time_index)\n",
    "            \n",
    "            # Base pattern: trend + seasonality + noise\n",
    "            trend = np.linspace(50, 55, n_points)  # Slight upward trend\n",
    "            daily_seasonal = 10 * np.sin(np.linspace(0, 2*np.pi * (duration_hours/24), n_points))\n",
    "            hourly_seasonal = 3 * np.sin(np.linspace(0, 2*np.pi * duration_hours, n_points))\n",
    "            noise = np.random.normal(0, 2, n_points)\n",
    "            \n",
    "            # Customize based on metric type\n",
    "            if 'cpu' in metric.lower():\n",
    "                base = 30 + trend * 0.5 + daily_seasonal + noise\n",
    "            elif 'memory' in metric.lower():\n",
    "                base = 60 + trend * 0.3 + daily_seasonal * 0.5 + noise\n",
    "            elif 'restart' in metric.lower():\n",
    "                base = np.abs(noise * 0.5)  # Low values, occasional spikes\n",
    "            elif 'pending' in metric.lower() or 'failed' in metric.lower():\n",
    "                base = np.abs(noise * 0.2)  # Mostly zeros\n",
    "            else:\n",
    "                base = 50 + trend + daily_seasonal + noise\n",
    "            \n",
    "            df[metric] = base\n",
    "            data_sources[metric] = 'SYNTHETIC'\n",
    "            print(f\"   üìä [{i+1:2}/{len(TARGET_METRICS)}] {metric}: SYNTHETIC ({len(base)} points)\")\n",
    "    \n",
    "    # Add labels column (for synthetic anomalies)\n",
    "    df['label'] = 0\n",
    "    \n",
    "    # Inject some anomalies for training/testing\n",
    "    anomaly_rate = 0.03  # 3% anomalies\n",
    "    n_anomalies = int(len(df) * anomaly_rate)\n",
    "    anomaly_indices = np.random.choice(len(df), n_anomalies, replace=False)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        # Pick random metrics to make anomalous\n",
    "        anomaly_metrics = np.random.choice(TARGET_METRICS, 2, replace=False)\n",
    "        for metric in anomaly_metrics:\n",
    "            if metric in df.columns:\n",
    "                std = df[metric].std()\n",
    "                df.iloc[idx, df.columns.get_loc(metric)] += 3.0 * std * np.random.choice([-1, 1])\n",
    "        df.iloc[idx, df.columns.get_loc('label')] = 1\n",
    "    \n",
    "    # Summary\n",
    "    real_count = sum(1 for s in data_sources.values() if s == 'REAL')\n",
    "    synthetic_count = sum(1 for s in data_sources.values() if s == 'SYNTHETIC')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä DATA LOADING SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Total metrics: {len(TARGET_METRICS)}\")\n",
    "    print(f\"   ‚úÖ REAL data: {real_count} metrics\")\n",
    "    print(f\"   üìä SYNTHETIC data: {synthetic_count} metrics\")\n",
    "    print(f\"   Data points: {len(df):,}\")\n",
    "    print(f\"   Anomalies injected: {df['label'].sum()} ({df['label'].mean():.1%})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return df, data_sources\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD THE DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Set to True to try loading real Prometheus data\n",
    "USE_REAL_DATA = True\n",
    "\n",
    "df, data_sources = load_timeseries_data(\n",
    "    duration_hours=24,\n",
    "    use_real_data=USE_REAL_DATA\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìã DATA LOADED:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Columns: {list(df.columns[:5])}... + {len(df.columns)-5} more\")\n",
    "print(f\"   Time range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"\\n   Normal samples: {(df['label'] == 0).sum()}\")\n",
    "print(f\"   Anomalous samples: {(df['label'] == 1).sum()}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nüìä Sample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ARIMA-Based Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîÑ ARIMA ANOMALY DETECTION ON ALL METRICS\n",
      "======================================================================\n",
      "\n",
      "üìä Analyzing 16 metrics with ARIMA...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[ 1/16] node_memory_utilization... "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ARIMA_AVAILABLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     75\u001b[39m series = df[metric]\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Run ARIMA detection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m predictions, model, residuals = \u001b[43mdetect_anomalies_arima\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     81\u001b[39m     arima_results[metric] = predictions\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mdetect_anomalies_arima\u001b[39m\u001b[34m(series, threshold_std)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetect_anomalies_arima\u001b[39m(series, threshold_std=\u001b[32m2.5\u001b[39m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Detect anomalies using ARIMA forecasting.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m        tuple: (predictions, model, residuals)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mARIMA_AVAILABLE\u001b[49m:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     16\u001b[39m         \u001b[38;5;66;03m# Handle NaN values\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'ARIMA_AVAILABLE' is not defined"
     ]
    }
   ],
   "source": [
    "def detect_anomalies_arima(series, threshold_std=2.5):\n",
    "    \"\"\"\n",
    "    Detect anomalies using ARIMA forecasting.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data (pandas Series)\n",
    "        threshold_std: Number of standard deviations for anomaly threshold\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predictions, model, residuals)\n",
    "    \"\"\"\n",
    "    if not ARIMA_AVAILABLE:\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        # Handle NaN values\n",
    "        series_clean = series.dropna().reset_index(drop=True)\n",
    "        \n",
    "        if len(series_clean) < 50:\n",
    "            print(f\"      ‚ö†Ô∏è Not enough data points ({len(series_clean)})\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Fit ARIMA model\n",
    "        model = ARIMA(series_clean, order=(1, 1, 1))\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Get fitted values and residuals\n",
    "        fitted = results.fittedvalues\n",
    "        \n",
    "        # Residuals: actual - fitted (ARIMA drops first value due to differencing)\n",
    "        # fitted values start from index 1\n",
    "        actual_values = series_clean.iloc[1:len(fitted)+1]\n",
    "        residuals = actual_values.values - fitted.values\n",
    "        \n",
    "        # Detect anomalies based on residual threshold\n",
    "        threshold = threshold_std * np.std(residuals)\n",
    "        anomaly_mask = np.abs(residuals) > threshold\n",
    "        \n",
    "        # Create full predictions array matching original series length\n",
    "        full_predictions = np.zeros(len(series), dtype=int)\n",
    "        \n",
    "        # Map back anomalies to original indices\n",
    "        # The residuals correspond to indices 1 through len(residuals)\n",
    "        for i, is_anomaly in enumerate(anomaly_mask):\n",
    "            if is_anomaly and (i + 1) < len(full_predictions):\n",
    "                full_predictions[i + 1] = 1\n",
    "        \n",
    "        return full_predictions, results, residuals\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è ARIMA error: {str(e)[:50]}\")\n",
    "        return None, None, None\n",
    "\n",
    "# =============================================================================\n",
    "# ANALYZE ALL 16 METRICS WITH ARIMA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîÑ ARIMA ANOMALY DETECTION ON ALL METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "arima_results = {}\n",
    "arima_models = {}\n",
    "\n",
    "# Get only metric columns (exclude timestamp, label)\n",
    "metric_columns = [col for col in df.columns if col in TARGET_METRICS]\n",
    "\n",
    "print(f\"\\nüìä Analyzing {len(metric_columns)} metrics with ARIMA...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, metric in enumerate(metric_columns):\n",
    "    print(f\"\\n[{i+1:2}/{len(metric_columns)}] {metric}...\", end=\" \")\n",
    "    \n",
    "    # Get the time series for this metric\n",
    "    series = df[metric]\n",
    "    \n",
    "    # Run ARIMA detection\n",
    "    predictions, model, residuals = detect_anomalies_arima(series)\n",
    "    \n",
    "    if predictions is not None:\n",
    "        arima_results[metric] = predictions\n",
    "        arima_models[metric] = model\n",
    "        \n",
    "        anomalies_detected = np.sum(predictions)\n",
    "        print(f\"‚úÖ {anomalies_detected} anomalies\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMBINE RESULTS - ENSEMBLE ACROSS METRICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä ARIMA ENSEMBLE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if arima_results:\n",
    "    # Combine all metric predictions into DataFrame\n",
    "    arima_df = pd.DataFrame(arima_results)\n",
    "    \n",
    "    # Ensemble: anomaly if ANY metric flags it\n",
    "    ensemble_any = (arima_df.sum(axis=1) > 0).astype(int).values\n",
    "    \n",
    "    # Ensemble: anomaly if MAJORITY of metrics flag it\n",
    "    ensemble_majority = (arima_df.sum(axis=1) > len(arima_results) / 2).astype(int).values\n",
    "    \n",
    "    print(f\"\\n   Metrics analyzed: {len(arima_results)}\")\n",
    "    print(f\"   Total data points: {len(df)}\")\n",
    "    \n",
    "    print(f\"\\n   üîπ ANY metric anomaly: {ensemble_any.sum()} anomalies ({ensemble_any.mean():.2%})\")\n",
    "    print(f\"   üîπ MAJORITY metrics anomaly: {ensemble_majority.sum()} anomalies ({ensemble_majority.mean():.2%})\")\n",
    "    \n",
    "    # Evaluate against labels\n",
    "    if 'label' in df.columns:\n",
    "        y_true_local = df['label'].values[:len(ensemble_any)]\n",
    "        \n",
    "        print(f\"\\n   üìà Performance (ANY method):\")\n",
    "        precision = precision_score(y_true_local, ensemble_any, zero_division=0)\n",
    "        recall = recall_score(y_true_local, ensemble_any, zero_division=0)\n",
    "        f1 = f1_score(y_true_local, ensemble_any, zero_division=0)\n",
    "        print(f\"      Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n",
    "        \n",
    "        print(f\"\\n   üìà Performance (MAJORITY method):\")\n",
    "        precision = precision_score(y_true_local, ensemble_majority, zero_division=0)\n",
    "        recall = recall_score(y_true_local, ensemble_majority, zero_division=0)\n",
    "        f1 = f1_score(y_true_local, ensemble_majority, zero_division=0)\n",
    "        print(f\"      Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    arima_preds = ensemble_any\n",
    "    arima_model = arima_models\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ ARIMA analysis complete!\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"‚ùå No ARIMA models were successfully trained\")\n",
    "    # Fallback to zeros\n",
    "    arima_preds = np.zeros(len(df), dtype=int)\n",
    "    arima_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prophet-Based Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:51:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "05:51:00 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîÑ PROPHET ANOMALY DETECTION ON ALL METRICS\n",
      "======================================================================\n",
      "\n",
      "üìä Full mode: Analyzing all 16 metrics (this may take a few minutes)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[ 1/16] node_memory_utilization... ‚úÖ 26 anomalies\n",
      "\n",
      "[ 2/16] pod_cpu_usage... ‚úÖ 44 anomalies\n",
      "\n",
      "[ 3/16] pod_memory_usage... ‚úÖ 23 anomalies\n",
      "\n",
      "[ 4/16] alt_cpu_usage... ‚úÖ 44 anomalies\n",
      "\n",
      "[ 5/16] alt_memory_usage... ‚úÖ 23 anomalies\n",
      "\n",
      "[ 6/16] container_restart_count... ‚úÖ 53 anomalies\n",
      "\n",
      "[ 7/16] container_restart_rate_1h... ‚úÖ 104 anomalies\n",
      "\n",
      "[ 8/16] deployment_unavailable... ‚úÖ 43 anomalies\n",
      "\n",
      "[ 9/16] namespace_pod_count... ‚úÖ 30 anomalies\n",
      "\n",
      "[10/16] pods_pending... ‚úÖ 49 anomalies\n",
      "\n",
      "[11/16] pods_running... ‚úÖ 28 anomalies\n",
      "\n",
      "[12/16] pods_failed... ‚úÖ 7 anomalies\n",
      "\n",
      "[13/16] persistent_volume_usage... ‚úÖ 31 anomalies\n",
      "\n",
      "[14/16] cluster_resource_quota... ‚úÖ 0 anomalies\n",
      "\n",
      "[15/16] apiserver_request_total... ‚úÖ 66 anomalies\n",
      "\n",
      "[16/16] apiserver_error_rate... ‚úÖ 58 anomalies\n",
      "\n",
      "======================================================================\n",
      "üìä PROPHET ENSEMBLE RESULTS\n",
      "======================================================================\n",
      "\n",
      "   Metrics analyzed: 16\n",
      "   Total data points: 1441\n",
      "\n",
      "   üîπ ANY metric anomaly: 249 anomalies (17.28%)\n",
      "   üîπ MAJORITY metrics anomaly: 0 anomalies (0.00%)\n",
      "\n",
      "   üìà Performance (ANY method):\n",
      "      Precision: 0.173 | Recall: 1.000 | F1: 0.295\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Prophet analysis complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Prophet not installed. Run: pip install prophet\")\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def detect_anomalies_prophet(series, threshold_std=2.5):\n",
    "    \"\"\"\n",
    "    Detect anomalies using Prophet forecasting.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data (pandas Series with datetime index)\n",
    "        threshold_std: Number of standard deviations for anomaly threshold\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predictions, model, forecast)\n",
    "    \"\"\"\n",
    "    if not PROPHET_AVAILABLE:\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        # Prepare data for Prophet (requires 'ds' and 'y' columns)\n",
    "        prophet_df = pd.DataFrame({\n",
    "            'ds': series.index,\n",
    "            'y': series.values\n",
    "        }).dropna()\n",
    "        \n",
    "        if len(prophet_df) < 50:\n",
    "            return None, None, None\n",
    "        \n",
    "        # Fit Prophet model (suppress logging)\n",
    "        import logging\n",
    "        logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "        \n",
    "        model = Prophet(\n",
    "            daily_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            yearly_seasonality=False,  # Not enough data\n",
    "            changepoint_prior_scale=0.05\n",
    "        )\n",
    "        model.fit(prophet_df)\n",
    "        \n",
    "        # Make predictions\n",
    "        forecast = model.predict(prophet_df[['ds']])\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = prophet_df['y'].values - forecast['yhat'].values\n",
    "        threshold = threshold_std * np.std(residuals)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        predictions = (np.abs(residuals) > threshold).astype(int)\n",
    "        \n",
    "        # Create full prediction series aligned with original index\n",
    "        full_predictions = pd.Series(0, index=series.index)\n",
    "        for i, (idx, row) in enumerate(prophet_df.iterrows()):\n",
    "            if i < len(predictions):\n",
    "                full_predictions.loc[row['ds']] = predictions[i]\n",
    "        \n",
    "        return full_predictions, model, forecast\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Prophet error: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# =============================================================================\n",
    "# ANALYZE ALL 16 METRICS WITH PROPHET\n",
    "# =============================================================================\n",
    "\n",
    "if PROPHET_AVAILABLE:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ PROPHET ANOMALY DETECTION ON ALL METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    prophet_results = {}\n",
    "    prophet_models = {}\n",
    "    \n",
    "    # Get only metric columns (exclude 'label')\n",
    "    metric_columns = [col for col in df.columns if col in TARGET_METRICS]\n",
    "    \n",
    "    # Prophet is slow, so we'll analyze a subset or all based on user choice\n",
    "    ANALYZE_ALL_METRICS = True  # Set to False to only analyze top 5 metrics\n",
    "    \n",
    "    if not ANALYZE_ALL_METRICS:\n",
    "        # Prioritize most important metrics for time series analysis\n",
    "        priority_metrics = [\n",
    "            'pod_cpu_usage',\n",
    "            'pod_memory_usage', \n",
    "            'container_restart_rate_1h',\n",
    "            'apiserver_error_rate',\n",
    "            'deployment_unavailable'\n",
    "        ]\n",
    "        metric_columns = [m for m in priority_metrics if m in metric_columns]\n",
    "        print(f\"\\n‚ö° Fast mode: Analyzing {len(metric_columns)} priority metrics\")\n",
    "    else:\n",
    "        print(f\"\\nüìä Full mode: Analyzing all {len(metric_columns)} metrics (this may take a few minutes)\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        print(f\"\\n[{i+1:2}/{len(metric_columns)}] {metric}...\", end=\" \")\n",
    "        \n",
    "        # Get the time series for this metric\n",
    "        series = df[metric]\n",
    "        \n",
    "        # Run Prophet detection\n",
    "        predictions, model, forecast = detect_anomalies_prophet(series)\n",
    "        \n",
    "        if predictions is not None:\n",
    "            prophet_results[metric] = predictions\n",
    "            prophet_models[metric] = model\n",
    "            \n",
    "            anomalies_detected = predictions.sum()\n",
    "            print(f\"‚úÖ {anomalies_detected} anomalies\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # COMBINE RESULTS - ENSEMBLE ACROSS METRICS\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä PROPHET ENSEMBLE RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if prophet_results:\n",
    "        # Combine all metric predictions\n",
    "        prophet_ensemble = pd.DataFrame(prophet_results)\n",
    "        \n",
    "        # Ensemble prediction: anomaly if ANY metric flags it\n",
    "        ensemble_any = (prophet_ensemble.sum(axis=1) > 0).astype(int)\n",
    "        \n",
    "        # Ensemble prediction: anomaly if MAJORITY of metrics flag it\n",
    "        ensemble_majority = (prophet_ensemble.sum(axis=1) > len(prophet_results) / 2).astype(int)\n",
    "        \n",
    "        print(f\"\\n   Metrics analyzed: {len(prophet_results)}\")\n",
    "        print(f\"   Total data points: {len(df)}\")\n",
    "        \n",
    "        print(f\"\\n   üîπ ANY metric anomaly: {ensemble_any.sum()} anomalies ({ensemble_any.mean():.2%})\")\n",
    "        print(f\"   üîπ MAJORITY metrics anomaly: {ensemble_majority.sum()} anomalies ({ensemble_majority.mean():.2%})\")\n",
    "        \n",
    "        if 'label' in df.columns:\n",
    "            print(f\"\\n   üìà Performance (ANY method):\")\n",
    "            precision = precision_score(df['label'], ensemble_any, zero_division=0)\n",
    "            recall = recall_score(df['label'], ensemble_any, zero_division=0)\n",
    "            f1 = f1_score(df['label'], ensemble_any, zero_division=0)\n",
    "            print(f\"      Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n",
    "        \n",
    "        # Store for later use\n",
    "        prophet_preds = ensemble_any\n",
    "        prophet_model = prophet_models\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"‚úÖ Prophet analysis complete!\")\n",
    "        print(\"=\" * 70)\n",
    "    else:\n",
    "        print(\"‚ùå No Prophet models were successfully trained\")\n",
    "        prophet_preds = None\n",
    "        prophet_model = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Prophet not available - skipping Prophet analysis\")\n",
    "    print(\"   Install with: pip install prophet\")\n",
    "    prophet_preds = None\n",
    "    prophet_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TimeSeriesEnsemble class defined for KServe compatibility\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "\n",
    "class TimeSeriesEnsemble(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wrapper class that combines ARIMA and Prophet models for KServe compatibility.\n",
    "    KServe requires a single .pkl file, not multiple model files.\n",
    "    \"\"\"\n",
    "    def __init__(self, arima_model=None, prophet_model=None):\n",
    "        self.arima_model = arima_model\n",
    "        self.prophet_model = prophet_model\n",
    "\n",
    "    def predict(self, X, periods=None):\n",
    "        \"\"\"\n",
    "        Make predictions using both models and return ensemble result.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (DataFrame or array-like)\n",
    "            periods: Number of periods to forecast (for time series)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with predictions from both models\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if self.arima_model is not None:\n",
    "            try:\n",
    "                # ARIMA forecast\n",
    "                arima_forecast = self.arima_model.forecast(steps=periods or len(X))\n",
    "                results['arima'] = arima_forecast\n",
    "            except Exception as e:\n",
    "                results['arima_error'] = str(e)\n",
    "                logger.warning(f\"ARIMA prediction failed: {e}\")\n",
    "        \n",
    "        if self.prophet_model is not None:\n",
    "            try:\n",
    "                # Prophet forecast\n",
    "                import pandas as pd\n",
    "                future = self.prophet_model.make_future_dataframe(periods=periods or len(X))\n",
    "                prophet_forecast = self.prophet_model.predict(future)\n",
    "                results['prophet'] = prophet_forecast['yhat'].values\n",
    "            except Exception as e:\n",
    "                results['prophet_error'] = str(e)\n",
    "                logger.warning(f\"Prophet prediction failed: {e}\")\n",
    "        \n",
    "        # Return ensemble (average of both predictions)\n",
    "        if 'arima' in results and 'prophet' in results:\n",
    "            results['ensemble'] = (results['arima'] + results['prophet']) / 2\n",
    "        elif 'arima' in results:\n",
    "            results['ensemble'] = results['arima']\n",
    "        elif 'prophet' in results:\n",
    "            results['ensemble'] = results['prophet']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'arima_model': self.arima_model,\n",
    "            'prophet_model': self.prophet_model\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "print(\"‚úÖ TimeSeriesEnsemble class defined for KServe compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arima_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create ensemble wrapper combining both models (KServe compatible)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# KServe sklearn server requires EXACTLY ONE .pkl file per model directory\u001b[39;00m\n\u001b[32m      3\u001b[39m ensemble_model = TimeSeriesEnsemble(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     arima_model=\u001b[43marima_model\u001b[49m,\n\u001b[32m      5\u001b[39m     prophet_model=prophet_model\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Created TimeSeriesEnsemble wrapper\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   ARIMA model:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m‚úì\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arima_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m‚úó\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'arima_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Create ensemble wrapper combining both models (KServe compatible)\n",
    "# KServe sklearn server requires EXACTLY ONE .pkl file per model directory\n",
    "ensemble_model = TimeSeriesEnsemble(\n",
    "    arima_model=arima_model,\n",
    "    prophet_model=prophet_model\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created TimeSeriesEnsemble wrapper\")\n",
    "print(\"   ARIMA model:\", \"‚úì\" if arima_model is not None else \"‚úó\")\n",
    "print(\"   Prophet model:\", \"‚úì\" if prophet_model is not None else \"‚úó\")\n",
    "\n",
    "# Save as SINGLE .pkl file for KServe\n",
    "# Path: /mnt/models/anomaly-detector/model.pkl\n",
    "model_path = MODEL_DIR / 'model.pkl'\n",
    "\n",
    "# Remove old separate model files if they exist (migration)\n",
    "old_arima = MODEL_DIR / 'arima_model.pkl'\n",
    "old_prophet = MODEL_DIR / 'prophet_model.pkl'\n",
    "for old_file in [old_arima, old_prophet]:\n",
    "    if old_file.exists():\n",
    "        old_file.unlink()\n",
    "        logger.info(f\"üóëÔ∏è  Removed old file: {old_file.name}\")\n",
    "\n",
    "# Save ensemble model\n",
    "joblib.dump(ensemble_model, model_path)\n",
    "logger.info(f\"üíæ Saved ensemble model to {model_path}\")\n",
    "\n",
    "# Verify only ONE .pkl file exists (KServe requirement)\n",
    "pkl_files = list(MODEL_DIR.glob('*.pkl'))\n",
    "if len(pkl_files) != 1:\n",
    "    raise RuntimeError(\n",
    "        f\"‚ùå ERROR: Expected 1 .pkl file, found {len(pkl_files)}: {pkl_files}\\n\"\n",
    "        f\"KServe requires EXACTLY ONE .pkl file per model directory.\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Model saved correctly for KServe:\")\n",
    "print(f\"   Path: {pkl_files[0]}\")\n",
    "print(f\"   Size: {pkl_files[0].stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"   Files in directory: {len(pkl_files)} (correct - must be 1)\")\n",
    "\n",
    "# Upload to S3 for persistent storage (optional)\n",
    "try:\n",
    "    from common_functions import upload_model_to_s3, test_s3_connection\n",
    "    \n",
    "    if test_s3_connection():\n",
    "        upload_model_to_s3(\n",
    "            str(model_path),\n",
    "            s3_key='models/anomaly-detection/anomaly-detector/model.pkl'\n",
    "        )\n",
    "        logger.info(\"‚òÅÔ∏è  Uploaded to S3\")\n",
    "    else:\n",
    "        logger.info(\"‚ö†Ô∏è S3 not available - model saved locally only\")\n",
    "except ImportError:\n",
    "    logger.info(\"‚ö†Ô∏è S3 functions not available - model saved locally only\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"‚ö†Ô∏è S3 upload failed (non-critical): {e}\")\n",
    "\n",
    "# Save predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'actual': df['label'],\n",
    "    'arima_pred': arima_preds if arima_preds is not None else 0,\n",
    "    'prophet_pred': prophet_preds if prophet_preds is not None else 0\n",
    "})\n",
    "results_df.to_parquet(PROCESSED_DIR / 'timeseries_predictions.parquet')\n",
    "logger.info(\"üíæ Saved predictions\")\n",
    "\n",
    "print(f\"\\nüéâ KServe model deployment ready!\")\n",
    "print(f\"   Model name: {MODEL_NAME}\")\n",
    "print(f\"   Model path: {model_path}\")\n",
    "print(f\"   Deploy to KServe with: storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify outputs - KServe compatible structure\n",
    "model_path = MODEL_DIR / 'model.pkl'\n",
    "\n",
    "# Check model file exists\n",
    "assert model_path.exists(), f\"Model not saved: {model_path}\"\n",
    "\n",
    "# Check predictions saved\n",
    "assert (PROCESSED_DIR / 'timeseries_predictions.parquet').exists(), \"Predictions not saved\"\n",
    "\n",
    "# Verify KServe requirement: EXACTLY ONE .pkl file\n",
    "pkl_files = list(MODEL_DIR.glob('*.pkl'))\n",
    "assert len(pkl_files) == 1, f\"ERROR: Expected 1 .pkl file, found {len(pkl_files)}: {pkl_files}\"\n",
    "\n",
    "# Verify can load and use the model\n",
    "loaded_model = joblib.load(model_path)\n",
    "assert hasattr(loaded_model, 'predict'), \"Model doesn't have predict method\"\n",
    "assert hasattr(loaded_model, 'arima_model'), \"Missing arima_model attribute\"\n",
    "assert hasattr(loaded_model, 'prophet_model'), \"Missing prophet_model attribute\"\n",
    "\n",
    "logger.info(\"‚úÖ All validations passed\")\n",
    "print(f\"\\n‚úÖ KServe Validation Complete:\")\n",
    "print(f\"   Model path: {model_path}\")\n",
    "print(f\"   File count: {len(pkl_files)} (correct - must be 1)\")\n",
    "print(f\"   Model type: {type(loaded_model).__name__}\")\n",
    "print(f\"   Predictions: {PROCESSED_DIR / 'timeseries_predictions.parquet'}\")\n",
    "print(f\"\\nüéØ Ready for KServe deployment!\")\n",
    "print(f\"   Deploy with: oc apply -f <inference-service.yaml>\")\n",
    "print(f\"   storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Synthetic anomalies from `synthetic-anomaly-generation.ipynb`\n",
    "- **Output**: Time series models for `ensemble-anomaly-methods.ipynb`\n",
    "- **Coordination Engine**: Models can be deployed for real-time detection\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review model performance metrics\n",
    "2. Proceed to `lstm-based-prediction.ipynb` for deep learning approach\n",
    "3. Compare with ensemble methods\n",
    "4. Deploy best model to coordination engine\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [ARIMA Documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html)\n",
    "- [Prophet Documentation](https://facebook.github.io/prophet/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Anomaly Detection\n",
    "\n",
    "## Overview\n",
    "This notebook implements time series anomaly detection using ARIMA and Prophet forecasting methods. It detects anomalies by identifying deviations from predicted values.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `synthetic-anomaly-generation.ipynb` (Phase 1)\n",
    "- Libraries: statsmodels, prophet, pandas, numpy\n",
    "- Synthetic dataset: `/opt/app-root/src/data/processed/synthetic_anomalies.parquet`\n",
    "\n",
    "## Why We Use Synthetic Data\n",
    "\n",
    "### The Problem: Real Anomalies Are Rare\n",
    "In production OpenShift clusters:\n",
    "- Anomalies occur <1% of the time\n",
    "- Collecting 1000 labeled anomalies takes months/years\n",
    "- Different anomaly types are hard to capture\n",
    "- Can't deliberately cause failures to collect data\n",
    "\n",
    "### The Solution: Synthetic Anomalies\n",
    "We generate synthetic anomalies because:\n",
    "- ‚úÖ Create 1000+ labeled anomalies in minutes\n",
    "- ‚úÖ Control anomaly types and severity\n",
    "- ‚úÖ Ensure balanced training data (50% normal, 50% anomaly)\n",
    "- ‚úÖ Reproducible and testable\n",
    "- ‚úÖ Models trained on synthetic data generalize to real anomalies\n",
    "\n",
    "### Machine Learning Best Practice\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement ARIMA forecasting on synthetic data\n",
    "- Use Prophet for time series analysis\n",
    "- Detect anomalies via forecast deviations\n",
    "- Handle seasonal patterns\n",
    "- Evaluate detection performance with labeled data\n",
    "\n",
    "## Key Concepts\n",
    "- **ARIMA**: AutoRegressive Integrated Moving Average\n",
    "- **Prophet**: Facebook's time series forecasting tool\n",
    "- **Forecast Error**: Deviation between actual and predicted values\n",
    "- **Seasonality**: Repeating patterns in time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Why Synthetic Data for Training?\n",
    "- **He & Garcia (2009)**: \"Learning from Imbalanced Data\" - https://ieeexplore.ieee.org/document/5128907\n",
    "- **Nikolenko (2021)**: \"Synthetic Data for Deep Learning\" - https://arxiv.org/abs/1909.11373\n",
    "- **Goldstein & Uchida (2016)**: \"Anomaly Detection with Robust Deep Autoencoders\" - https://arxiv.org/abs/1511.08747\n",
    "\n",
    "### Time Series Anomaly Detection\n",
    "- **Malhotra et al. (2016)**: \"Time Series Anomaly Detection with LSTM Networks\" - https://arxiv.org/abs/1607.00148\n",
    "- **Taylor & Letham (2018)**: \"Forecasting at Scale (Prophet)\" - https://peerj.com/articles/3190\n",
    "- **Box & Jenkins (1970)**: \"Time Series Analysis, Forecasting and Control (ARIMA)\" - Classic reference\n",
    "\n",
    "### Key Takeaway\n",
    "Synthetic data provides labeled training examples that allow us to:\n",
    "1. Train models with known ground truth\n",
    "2. Evaluate performance with precision, recall, and F1 scores\n",
    "3. Ensure reproducible and testable results\n",
    "4. Build models that generalize to real-world anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import ARIMA\n",
    "try:\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    ARIMA_AVAILABLE = True\n",
    "    print(\"‚úÖ ARIMA (statsmodels) available\")\n",
    "except ImportError:\n",
    "    ARIMA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è statsmodels not available - ARIMA will use fallback\")\n",
    "\n",
    "\n",
    "# Setup path for utils module - works from any directory\n",
    "def find_utils_path():\n",
    "    \"\"\"Find utils path regardless of current working directory\"\"\"\n",
    "    possible_paths = [\n",
    "        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n",
    "        Path.cwd() / 'notebooks' / 'utils',\n",
    "        Path.cwd().parent / 'utils',\n",
    "        Path('/workspace/repo/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n",
    "    ]\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists() and (p / 'common_functions.py').exists():\n",
    "            return str(p)\n",
    "    current = Path.cwd()\n",
    "    for _ in range(5):\n",
    "        utils_path = current / 'notebooks' / 'utils'\n",
    "        if utils_path.exists():\n",
    "            return str(utils_path)\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "utils_path = find_utils_path()\n",
    "if utils_path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "    print(f\"‚úÖ Utils path found: {utils_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Utils path not found - will use fallback implementations\")\n",
    "\n",
    "# Try to import common functions, with fallback\n",
    "try:\n",
    "    from common_functions import setup_environment\n",
    "    print(\"‚úÖ Common functions imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Common functions not available: {e}\")\n",
    "    def setup_environment():\n",
    "        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n",
    "        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n",
    "        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Setup environment\n",
    "env_info = setup_environment()\n",
    "logger.info(f\"Environment ready: {env_info}\")\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('/opt/app-root/src/data')\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# Use /mnt/models for persistent storage (model-storage-pvc)\n",
    "# Fallback to local for development outside cluster\n",
    "MODELS_DIR = Path('/mnt/models') if Path('/mnt/models').exists() else Path('/opt/app-root/src/models')\n",
    "\n",
    "# Create KServe-compatible subdirectory structure\n",
    "# CHANGED: Use unique model name to avoid conflict with isolation-forest's anomaly-detector model\n",
    "MODEL_NAME = 'timeseries-predictor'\n",
    "MODEL_DIR = MODELS_DIR / MODEL_NAME\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Data directory: {DATA_DIR}\")\n",
    "logger.info(f\"Models directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Load Prometheus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# DEFINE OUR 16 WORKING METRICS (same as other notebooks)\n",
    "# =============================================================================\n",
    "\n",
    "TARGET_METRICS = [\n",
    "    # Resource Metrics (5)\n",
    "    'node_memory_utilization',\n",
    "    'pod_cpu_usage',\n",
    "    'pod_memory_usage',\n",
    "    'alt_cpu_usage',\n",
    "    'alt_memory_usage',\n",
    "    \n",
    "    # Stability Metrics (3)\n",
    "    'container_restart_count',\n",
    "    'container_restart_rate_1h',\n",
    "    'deployment_unavailable',\n",
    "    \n",
    "    # Pod Status Metrics (4)\n",
    "    'namespace_pod_count',\n",
    "    'pods_pending',\n",
    "    'pods_running',\n",
    "    'pods_failed',\n",
    "    \n",
    "    # Storage Metrics (2)\n",
    "    'persistent_volume_usage',\n",
    "    'cluster_resource_quota',\n",
    "    \n",
    "    # Control Plane Metrics (2)\n",
    "    'apiserver_request_total',\n",
    "    'apiserver_error_rate',\n",
    "]\n",
    "\n",
    "# Prometheus queries for real data collection\n",
    "PROMETHEUS_QUERIES = {\n",
    "    'node_memory_utilization': 'instance:node_memory_utilisation:ratio * 100',\n",
    "    'pod_cpu_usage': 'sum by (pod, namespace) (node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate)',\n",
    "    'pod_memory_usage': 'sum by (pod, namespace) (container_memory_working_set_bytes{container!=\"POD\", container!=\"\"})',\n",
    "    'alt_cpu_usage': 'sum(rate(container_cpu_usage_seconds_total{container!=\"POD\", container!=\"\"}[5m])) by (pod, namespace)',\n",
    "    'alt_memory_usage': 'sum(container_memory_rss{container!=\"POD\", container!=\"\"}) by (pod, namespace)',\n",
    "    'container_restart_count': 'sum by (pod, namespace, container) (kube_pod_container_status_restarts_total)',\n",
    "    'container_restart_rate_1h': 'sum by (pod, namespace) (increase(kube_pod_container_status_restarts_total[1h]))',\n",
    "    'deployment_unavailable': 'sum by (deployment, namespace) (kube_deployment_status_replicas_unavailable)',\n",
    "    'namespace_pod_count': 'sum by (namespace) (kube_pod_status_phase)',\n",
    "    'pods_pending': 'sum by (namespace) (kube_pod_status_phase{phase=\"Pending\"})',\n",
    "    'pods_running': 'sum by (namespace) (kube_pod_status_phase{phase=\"Running\"})',\n",
    "    'pods_failed': 'sum by (namespace) (kube_pod_status_phase{phase=\"Failed\"})',\n",
    "    'persistent_volume_usage': 'kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100',\n",
    "    'cluster_resource_quota': 'kube_resourcequota',\n",
    "    'apiserver_request_total': 'sum(rate(apiserver_request_total[5m]))',\n",
    "    'apiserver_error_rate': 'sum(rate(apiserver_request_total{code=~\"5..\"}[5m])) / sum(rate(apiserver_request_total[5m])) * 100',\n",
    "}\n",
    "\n",
    "print(f\"üìä Target metrics for time series analysis: {len(TARGET_METRICS)}\")\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "class PrometheusClient:\n",
    "    \"\"\"Client for querying Prometheus in OpenShift.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        token_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'\n",
    "        self.token = None\n",
    "        if os.path.exists(token_path):\n",
    "            with open(token_path, 'r') as f:\n",
    "                self.token = f.read().strip()\n",
    "        \n",
    "        self.base_url = 'https://prometheus-k8s.openshift-monitoring.svc.cluster.local:9091'\n",
    "        self.session = requests.Session()\n",
    "        if self.token:\n",
    "            self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n",
    "        self.session.verify = False\n",
    "        \n",
    "        import urllib3\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        \n",
    "        # Test connection\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/api/v1/status/config\", timeout=5)\n",
    "            self.connected = response.status_code == 200\n",
    "        except:\n",
    "            self.connected = False\n",
    "    \n",
    "    def query_range(self, query, start, end, step='1m'):\n",
    "        if not self.connected:\n",
    "            return None\n",
    "        \n",
    "        url = f\"{self.base_url}/api/v1/query_range\"\n",
    "        params = {'query': query, 'start': start, 'end': end, 'step': step}\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except:\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def load_timeseries_data(duration_hours=24, use_real_data=True):\n",
    "    \"\"\"\n",
    "    Load time series data for anomaly detection.\n",
    "    \n",
    "    For ARIMA/Prophet, we need univariate time series, so we'll create\n",
    "    a DataFrame where each column is one metric's aggregated values over time.\n",
    "    \n",
    "    Args:\n",
    "        duration_hours: Hours of historical data\n",
    "        use_real_data: Try Prometheus first, fallback to synthetic\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with timestamp index and metric columns\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ LOADING TIME SERIES DATA\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Duration: {duration_hours} hours\")\n",
    "    print(f\"   Metrics: {len(TARGET_METRICS)}\")\n",
    "    print(f\"   Use real data: {use_real_data}\")\n",
    "    \n",
    "    # Try to connect to Prometheus\n",
    "    prometheus = None\n",
    "    if use_real_data:\n",
    "        prometheus = PrometheusClient()\n",
    "        print(f\"   Prometheus connected: {prometheus.connected}\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=duration_hours)\n",
    "    \n",
    "    # Create time index (1-minute intervals)\n",
    "    time_index = pd.date_range(start=start_time, end=end_time, freq='1min')\n",
    "    \n",
    "    # Initialize DataFrame\n",
    "    df = pd.DataFrame(index=time_index)\n",
    "    df.index.name = 'timestamp'\n",
    "    \n",
    "    data_sources = {}\n",
    "    \n",
    "    print(f\"\\nüìä Loading {len(TARGET_METRICS)} metrics...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, metric in enumerate(TARGET_METRICS):\n",
    "        real_data_loaded = False\n",
    "        \n",
    "        if prometheus and prometheus.connected and metric in PROMETHEUS_QUERIES:\n",
    "            query = PROMETHEUS_QUERIES[metric]\n",
    "            result = prometheus.query_range(\n",
    "                query,\n",
    "                start_time.timestamp(),\n",
    "                end_time.timestamp(),\n",
    "                step='1m'\n",
    "            )\n",
    "            \n",
    "            if result and result.get('status') == 'success':\n",
    "                data = result.get('data', {}).get('result', [])\n",
    "                if data:\n",
    "                    # Parse and aggregate Prometheus data\n",
    "                    rows = []\n",
    "                    for series in data:\n",
    "                        for ts, value in series.get('values', []):\n",
    "                            try:\n",
    "                                rows.append({\n",
    "                                    'timestamp': pd.to_datetime(ts, unit='s'),\n",
    "                                    'value': float(value) if value != 'NaN' else np.nan\n",
    "                                })\n",
    "                            except:\n",
    "                                pass\n",
    "                    \n",
    "                    if rows:\n",
    "                        metric_df = pd.DataFrame(rows)\n",
    "                        # Aggregate by timestamp (mean across all series)\n",
    "                        metric_series = metric_df.groupby('timestamp')['value'].mean()\n",
    "                        # Resample to our time index\n",
    "                        metric_series = metric_series.reindex(time_index, method='nearest')\n",
    "                        df[metric] = metric_series\n",
    "                        data_sources[metric] = 'REAL'\n",
    "                        real_data_loaded = True\n",
    "                        print(f\"   ‚úÖ [{i+1:2}/{len(TARGET_METRICS)}] {metric}: REAL ({metric_series.notna().sum()} points)\")\n",
    "        \n",
    "        if not real_data_loaded:\n",
    "            # Generate synthetic time series with realistic patterns\n",
    "            n_points = len(time_index)\n",
    "            \n",
    "            # Base pattern: trend + seasonality + noise\n",
    "            trend = np.linspace(50, 55, n_points)  # Slight upward trend\n",
    "            daily_seasonal = 10 * np.sin(np.linspace(0, 2*np.pi * (duration_hours/24), n_points))\n",
    "            hourly_seasonal = 3 * np.sin(np.linspace(0, 2*np.pi * duration_hours, n_points))\n",
    "            noise = np.random.normal(0, 2, n_points)\n",
    "            \n",
    "            # Customize based on metric type\n",
    "            if 'cpu' in metric.lower():\n",
    "                base = 30 + trend * 0.5 + daily_seasonal + noise\n",
    "            elif 'memory' in metric.lower():\n",
    "                base = 60 + trend * 0.3 + daily_seasonal * 0.5 + noise\n",
    "            elif 'restart' in metric.lower():\n",
    "                base = np.abs(noise * 0.5)  # Low values, occasional spikes\n",
    "            elif 'pending' in metric.lower() or 'failed' in metric.lower():\n",
    "                base = np.abs(noise * 0.2)  # Mostly zeros\n",
    "            else:\n",
    "                base = 50 + trend + daily_seasonal + noise\n",
    "            \n",
    "            df[metric] = base\n",
    "            data_sources[metric] = 'SYNTHETIC'\n",
    "            print(f\"   üìä [{i+1:2}/{len(TARGET_METRICS)}] {metric}: SYNTHETIC ({len(base)} points)\")\n",
    "    \n",
    "    # Add labels column (for synthetic anomalies)\n",
    "    df['label'] = 0\n",
    "    \n",
    "    # Inject some anomalies for training/testing\n",
    "    anomaly_rate = 0.03  # 3% anomalies\n",
    "    n_anomalies = int(len(df) * anomaly_rate)\n",
    "    anomaly_indices = np.random.choice(len(df), n_anomalies, replace=False)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        # Pick random metrics to make anomalous\n",
    "        anomaly_metrics = np.random.choice(TARGET_METRICS, 2, replace=False)\n",
    "        for metric in anomaly_metrics:\n",
    "            if metric in df.columns:\n",
    "                std = df[metric].std()\n",
    "                df.iloc[idx, df.columns.get_loc(metric)] += 3.0 * std * np.random.choice([-1, 1])\n",
    "        df.iloc[idx, df.columns.get_loc('label')] = 1\n",
    "    \n",
    "    # Summary\n",
    "    real_count = sum(1 for s in data_sources.values() if s == 'REAL')\n",
    "    synthetic_count = sum(1 for s in data_sources.values() if s == 'SYNTHETIC')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä DATA LOADING SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Total metrics: {len(TARGET_METRICS)}\")\n",
    "    print(f\"   ‚úÖ REAL data: {real_count} metrics\")\n",
    "    print(f\"   üìä SYNTHETIC data: {synthetic_count} metrics\")\n",
    "    print(f\"   Data points: {len(df):,}\")\n",
    "    print(f\"   Anomalies injected: {df['label'].sum()} ({df['label'].mean():.1%})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return df, data_sources\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD THE DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Set to True to try loading real Prometheus data\n",
    "USE_REAL_DATA = True\n",
    "\n",
    "df, data_sources = load_timeseries_data(\n",
    "    duration_hours=24,\n",
    "    use_real_data=USE_REAL_DATA\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìã DATA LOADED:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Columns: {list(df.columns[:5])}... + {len(df.columns)-5} more\")\n",
    "print(f\"   Time range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"\\n   Normal samples: {(df['label'] == 0).sum()}\")\n",
    "print(f\"   Anomalous samples: {(df['label'] == 1).sum()}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nüìä Sample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ARIMA-Based Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_arima(series, threshold_std=2.5):\n",
    "    \"\"\"\n",
    "    Detect anomalies using ARIMA forecasting.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data (pandas Series)\n",
    "        threshold_std: Number of standard deviations for anomaly threshold\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predictions, model, residuals)\n",
    "    \"\"\"\n",
    "    if not ARIMA_AVAILABLE:\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        # Handle NaN values\n",
    "        series_clean = series.dropna().reset_index(drop=True)\n",
    "        \n",
    "        if len(series_clean) < 50:\n",
    "            print(f\"      ‚ö†Ô∏è Not enough data points ({len(series_clean)})\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Fit ARIMA model\n",
    "        model = ARIMA(series_clean, order=(1, 1, 1))\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Get fitted values and residuals\n",
    "        fitted = results.fittedvalues\n",
    "        \n",
    "        # Residuals: actual - fitted (ARIMA drops first value due to differencing)\n",
    "        # fitted values start from index 1\n",
    "        actual_values = series_clean.iloc[1:len(fitted)+1]\n",
    "        residuals = actual_values.values - fitted.values\n",
    "        \n",
    "        # Detect anomalies based on residual threshold\n",
    "        threshold = threshold_std * np.std(residuals)\n",
    "        anomaly_mask = np.abs(residuals) > threshold\n",
    "        \n",
    "        # Create full predictions array matching original series length\n",
    "        full_predictions = np.zeros(len(series), dtype=int)\n",
    "        \n",
    "        # Map back anomalies to original indices\n",
    "        # The residuals correspond to indices 1 through len(residuals)\n",
    "        for i, is_anomaly in enumerate(anomaly_mask):\n",
    "            if is_anomaly and (i + 1) < len(full_predictions):\n",
    "                full_predictions[i + 1] = 1\n",
    "        \n",
    "        return full_predictions, results, residuals\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è ARIMA error: {str(e)[:50]}\")\n",
    "        return None, None, None\n",
    "\n",
    "# =============================================================================\n",
    "# ANALYZE ALL 16 METRICS WITH ARIMA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîÑ ARIMA ANOMALY DETECTION ON ALL METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "arima_results = {}\n",
    "arima_models = {}\n",
    "\n",
    "# Get only metric columns (exclude timestamp, label)\n",
    "metric_columns = [col for col in df.columns if col in TARGET_METRICS]\n",
    "\n",
    "print(f\"\\nüìä Analyzing {len(metric_columns)} metrics with ARIMA...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, metric in enumerate(metric_columns):\n",
    "    print(f\"\\n[{i+1:2}/{len(metric_columns)}] {metric}...\", end=\" \")\n",
    "    \n",
    "    # Get the time series for this metric\n",
    "    series = df[metric]\n",
    "    \n",
    "    # Run ARIMA detection\n",
    "    predictions, model, residuals = detect_anomalies_arima(series)\n",
    "    \n",
    "    if predictions is not None:\n",
    "        arima_results[metric] = predictions\n",
    "        arima_models[metric] = model\n",
    "        \n",
    "        anomalies_detected = np.sum(predictions)\n",
    "        print(f\"‚úÖ {anomalies_detected} anomalies\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMBINE RESULTS - ENSEMBLE ACROSS METRICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä ARIMA ENSEMBLE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if arima_results:\n",
    "    # Combine all metric predictions into DataFrame\n",
    "    arima_df = pd.DataFrame(arima_results)\n",
    "    \n",
    "    # Ensemble: anomaly if ANY metric flags it\n",
    "    ensemble_any = (arima_df.sum(axis=1) > 0).astype(int).values\n",
    "    \n",
    "    # Ensemble: anomaly if MAJORITY of metrics flag it\n",
    "    ensemble_majority = (arima_df.sum(axis=1) > len(arima_results) / 2).astype(int).values\n",
    "    \n",
    "    print(f\"\\n   Metrics analyzed: {len(arima_results)}\")\n",
    "    print(f\"   Total data points: {len(df)}\")\n",
    "    \n",
    "    print(f\"\\n   üîπ ANY metric anomaly: {ensemble_any.sum()} anomalies ({ensemble_any.mean():.2%})\")\n",
    "    print(f\"   üîπ MAJORITY metrics anomaly: {ensemble_majority.sum()} anomalies ({ensemble_majority.mean():.2%})\")\n",
    "    \n",
    "    # Evaluate against labels\n",
    "    if 'label' in df.columns:\n",
    "        y_true_local = df['label'].values[:len(ensemble_any)]\n",
    "        \n",
    "        print(f\"\\n   üìà Performance (ANY method):\")\n",
    "        precision = precision_score(y_true_local, ensemble_any, zero_division=0)\n",
    "        recall = recall_score(y_true_local, ensemble_any, zero_division=0)\n",
    "        f1 = f1_score(y_true_local, ensemble_any, zero_division=0)\n",
    "        print(f\"      Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n",
    "        \n",
    "        print(f\"\\n   üìà Performance (MAJORITY method):\")\n",
    "        precision = precision_score(y_true_local, ensemble_majority, zero_division=0)\n",
    "        recall = recall_score(y_true_local, ensemble_majority, zero_division=0)\n",
    "        f1 = f1_score(y_true_local, ensemble_majority, zero_division=0)\n",
    "        print(f\"      Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    arima_preds = ensemble_any\n",
    "    arima_model = arima_models\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ ARIMA analysis complete!\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"‚ùå No ARIMA models were successfully trained\")\n",
    "    # Fallback to zeros\n",
    "    arima_preds = np.zeros(len(df), dtype=int)\n",
    "    arima_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prophet-Based Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Prophet not installed. Run: pip install prophet\")\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def detect_anomalies_prophet(series, threshold_std=2.5):\n",
    "    \"\"\"\n",
    "    Detect anomalies using Prophet forecasting.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data (pandas Series with datetime index)\n",
    "        threshold_std: Number of standard deviations for anomaly threshold\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predictions, model, forecast)\n",
    "    \"\"\"\n",
    "    if not PROPHET_AVAILABLE:\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        # Prepare data for Prophet (requires 'ds' and 'y' columns)\n",
    "        prophet_df = pd.DataFrame({\n",
    "            'ds': series.index,\n",
    "            'y': series.values\n",
    "        }).dropna()\n",
    "        \n",
    "        if len(prophet_df) < 50:\n",
    "            return None, None, None\n",
    "        \n",
    "        # Fit Prophet model (suppress logging)\n",
    "        import logging\n",
    "        logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "        \n",
    "        model = Prophet(\n",
    "            daily_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            yearly_seasonality=False,  # Not enough data\n",
    "            changepoint_prior_scale=0.05\n",
    "        )\n",
    "        model.fit(prophet_df)\n",
    "        \n",
    "        # Make predictions\n",
    "        forecast = model.predict(prophet_df[['ds']])\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = prophet_df['y'].values - forecast['yhat'].values\n",
    "        threshold = threshold_std * np.std(residuals)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        predictions = (np.abs(residuals) > threshold).astype(int)\n",
    "        \n",
    "        # Create full prediction series aligned with original index\n",
    "        full_predictions = pd.Series(0, index=series.index)\n",
    "        for i, (idx, row) in enumerate(prophet_df.iterrows()):\n",
    "            if i < len(predictions):\n",
    "                full_predictions.loc[row['ds']] = predictions[i]\n",
    "        \n",
    "        return full_predictions, model, forecast\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Prophet error: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# =============================================================================\n",
    "# ANALYZE ALL 16 METRICS WITH PROPHET\n",
    "# =============================================================================\n",
    "\n",
    "if PROPHET_AVAILABLE:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ PROPHET ANOMALY DETECTION ON ALL METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    prophet_results = {}\n",
    "    prophet_models = {}\n",
    "    \n",
    "    # Get only metric columns (exclude 'label')\n",
    "    metric_columns = [col for col in df.columns if col in TARGET_METRICS]\n",
    "    \n",
    "    # Prophet is slow, so we'll analyze a subset or all based on user choice\n",
    "    ANALYZE_ALL_METRICS = True  # Set to False to only analyze top 5 metrics\n",
    "    \n",
    "    if not ANALYZE_ALL_METRICS:\n",
    "        # Prioritize most important metrics for time series analysis\n",
    "        priority_metrics = [\n",
    "            'pod_cpu_usage',\n",
    "            'pod_memory_usage', \n",
    "            'container_restart_rate_1h',\n",
    "            'apiserver_error_rate',\n",
    "            'deployment_unavailable'\n",
    "        ]\n",
    "        metric_columns = [m for m in priority_metrics if m in metric_columns]\n",
    "        print(f\"\\n‚ö° Fast mode: Analyzing {len(metric_columns)} priority metrics\")\n",
    "    else:\n",
    "        print(f\"\\nüìä Full mode: Analyzing all {len(metric_columns)} metrics (this may take a few minutes)\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        print(f\"\\n[{i+1:2}/{len(metric_columns)}] {metric}...\", end=\" \")\n",
    "        \n",
    "        # Get the time series for this metric\n",
    "        series = df[metric]\n",
    "        \n",
    "        # Run Prophet detection\n",
    "        predictions, model, forecast = detect_anomalies_prophet(series)\n",
    "        \n",
    "        if predictions is not None:\n",
    "            prophet_results[metric] = predictions\n",
    "            prophet_models[metric] = model\n",
    "            \n",
    "            anomalies_detected = predictions.sum()\n",
    "            print(f\"‚úÖ {anomalies_detected} anomalies\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # COMBINE RESULTS - ENSEMBLE ACROSS METRICS\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä PROPHET ENSEMBLE RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if prophet_results:\n",
    "        # Combine all metric predictions\n",
    "        prophet_ensemble = pd.DataFrame(prophet_results)\n",
    "        \n",
    "        # Ensemble prediction: anomaly if ANY metric flags it\n",
    "        ensemble_any = (prophet_ensemble.sum(axis=1) > 0).astype(int)\n",
    "        \n",
    "        # Ensemble prediction: anomaly if MAJORITY of metrics flag it\n",
    "        ensemble_majority = (prophet_ensemble.sum(axis=1) > len(prophet_results) / 2).astype(int)\n",
    "        \n",
    "        print(f\"\\n   Metrics analyzed: {len(prophet_results)}\")\n",
    "        print(f\"   Total data points: {len(df)}\")\n",
    "        \n",
    "        print(f\"\\n   üîπ ANY metric anomaly: {ensemble_any.sum()} anomalies ({ensemble_any.mean():.2%})\")\n",
    "        print(f\"   üîπ MAJORITY metrics anomaly: {ensemble_majority.sum()} anomalies ({ensemble_majority.mean():.2%})\")\n",
    "        \n",
    "        if 'label' in df.columns:\n",
    "            print(f\"\\n   üìà Performance (ANY method):\")\n",
    "            precision = precision_score(df['label'], ensemble_any, zero_division=0)\n",
    "            recall = recall_score(df['label'], ensemble_any, zero_division=0)\n",
    "            f1 = f1_score(df['label'], ensemble_any, zero_division=0)\n",
    "            print(f\"      Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n",
    "        \n",
    "        # Store for later use\n",
    "        prophet_preds = ensemble_any\n",
    "        prophet_model = prophet_models\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"‚úÖ Prophet analysis complete!\")\n",
    "        print(\"=\" * 70)\n",
    "    else:\n",
    "        print(\"‚ùå No Prophet models were successfully trained\")\n",
    "        prophet_preds = None\n",
    "        prophet_model = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Prophet not available - skipping Prophet analysis\")\n",
    "    print(\"   Install with: pip install prophet\")\n",
    "    prophet_preds = None\n",
    "    prophet_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "\n",
    "class TimeSeriesEnsemble(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wrapper class that combines ARIMA and Prophet models for KServe compatibility.\n",
    "    KServe requires a single .pkl file, not multiple model files.\n",
    "    \"\"\"\n",
    "    def __init__(self, arima_model=None, prophet_model=None):\n",
    "        self.arima_model = arima_model\n",
    "        self.prophet_model = prophet_model\n",
    "\n",
    "    def predict(self, X, periods=None):\n",
    "        \"\"\"\n",
    "        Make predictions using both models and return ensemble result.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (DataFrame or array-like)\n",
    "            periods: Number of periods to forecast (for time series)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with predictions from both models\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if self.arima_model is not None:\n",
    "            try:\n",
    "                # ARIMA forecast\n",
    "                arima_forecast = self.arima_model.forecast(steps=periods or len(X))\n",
    "                results['arima'] = arima_forecast\n",
    "            except Exception as e:\n",
    "                results['arima_error'] = str(e)\n",
    "                logger.warning(f\"ARIMA prediction failed: {e}\")\n",
    "        \n",
    "        if self.prophet_model is not None:\n",
    "            try:\n",
    "                # Prophet forecast\n",
    "                import pandas as pd\n",
    "                future = self.prophet_model.make_future_dataframe(periods=periods or len(X))\n",
    "                prophet_forecast = self.prophet_model.predict(future)\n",
    "                results['prophet'] = prophet_forecast['yhat'].values\n",
    "            except Exception as e:\n",
    "                results['prophet_error'] = str(e)\n",
    "                logger.warning(f\"Prophet prediction failed: {e}\")\n",
    "        \n",
    "        # Return ensemble (average of both predictions)\n",
    "        if 'arima' in results and 'prophet' in results:\n",
    "            results['ensemble'] = (results['arima'] + results['prophet']) / 2\n",
    "        elif 'arima' in results:\n",
    "            results['ensemble'] = results['arima']\n",
    "        elif 'prophet' in results:\n",
    "            results['ensemble'] = results['prophet']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'arima_model': self.arima_model,\n",
    "            'prophet_model': self.prophet_model\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "print(\"‚úÖ TimeSeriesEnsemble class defined for KServe compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble wrapper combining both models (KServe compatible)\n",
    "# KServe sklearn server requires EXACTLY ONE .pkl file per model directory\n",
    "ensemble_model = TimeSeriesEnsemble(\n",
    "    arima_model=arima_model,\n",
    "    prophet_model=prophet_model\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created TimeSeriesEnsemble wrapper\")\n",
    "print(\"   ARIMA model:\", \"‚úì\" if arima_model is not None else \"‚úó\")\n",
    "print(\"   Prophet model:\", \"‚úì\" if prophet_model is not None else \"‚úó\")\n",
    "\n",
    "# Save as SINGLE .pkl file for KServe\n",
    "# Path: /mnt/models/anomaly-detector/model.pkl\n",
    "model_path = MODEL_DIR / 'model.pkl'\n",
    "\n",
    "# Remove old separate model files if they exist (migration)\n",
    "old_arima = MODEL_DIR / 'arima_model.pkl'\n",
    "old_prophet = MODEL_DIR / 'prophet_model.pkl'\n",
    "for old_file in [old_arima, old_prophet]:\n",
    "    if old_file.exists():\n",
    "        old_file.unlink()\n",
    "        logger.info(f\"üóëÔ∏è  Removed old file: {old_file.name}\")\n",
    "\n",
    "# Save ensemble model\n",
    "joblib.dump(ensemble_model, model_path)\n",
    "logger.info(f\"üíæ Saved ensemble model to {model_path}\")\n",
    "\n",
    "# Verify only ONE .pkl file exists (KServe requirement)\n",
    "pkl_files = list(MODEL_DIR.glob('*.pkl'))\n",
    "if len(pkl_files) != 1:\n",
    "    raise RuntimeError(\n",
    "        f\"‚ùå ERROR: Expected 1 .pkl file, found {len(pkl_files)}: {pkl_files}\\n\"\n",
    "        f\"KServe requires EXACTLY ONE .pkl file per model directory.\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Model saved correctly for KServe:\")\n",
    "print(f\"   Path: {pkl_files[0]}\")\n",
    "print(f\"   Size: {pkl_files[0].stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"   Files in directory: {len(pkl_files)} (correct - must be 1)\")\n",
    "\n",
    "# Upload to S3 for persistent storage (optional)\n",
    "try:\n",
    "    from common_functions import upload_model_to_s3, test_s3_connection\n",
    "    \n",
    "    if test_s3_connection():\n",
    "        upload_model_to_s3(\n",
    "            str(model_path),\n",
    "            s3_key='models/anomaly-detection/anomaly-detector/model.pkl'\n",
    "        )\n",
    "        logger.info(\"‚òÅÔ∏è  Uploaded to S3\")\n",
    "    else:\n",
    "        logger.info(\"‚ö†Ô∏è S3 not available - model saved locally only\")\n",
    "except ImportError:\n",
    "    logger.info(\"‚ö†Ô∏è S3 functions not available - model saved locally only\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"‚ö†Ô∏è S3 upload failed (non-critical): {e}\")\n",
    "\n",
    "# Save predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'actual': df['label'],\n",
    "    'arima_pred': arima_preds if arima_preds is not None else 0,\n",
    "    'prophet_pred': prophet_preds if prophet_preds is not None else 0\n",
    "})\n",
    "results_df.to_parquet(PROCESSED_DIR / 'timeseries_predictions.parquet')\n",
    "logger.info(\"üíæ Saved predictions\")\n",
    "\n",
    "print(f\"\\nüéâ KServe model deployment ready!\")\n",
    "print(f\"   Model name: {MODEL_NAME}\")\n",
    "print(f\"   Model path: {model_path}\")\n",
    "print(f\"   Deploy to KServe with: storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify outputs - KServe compatible structure\n",
    "model_path = MODEL_DIR / 'model.pkl'\n",
    "\n",
    "# Check model file exists\n",
    "assert model_path.exists(), f\"Model not saved: {model_path}\"\n",
    "\n",
    "# Check predictions saved\n",
    "assert (PROCESSED_DIR / 'timeseries_predictions.parquet').exists(), \"Predictions not saved\"\n",
    "\n",
    "# Verify KServe requirement: EXACTLY ONE .pkl file\n",
    "pkl_files = list(MODEL_DIR.glob('*.pkl'))\n",
    "assert len(pkl_files) == 1, f\"ERROR: Expected 1 .pkl file, found {len(pkl_files)}: {pkl_files}\"\n",
    "\n",
    "# Verify can load and use the model\n",
    "loaded_model = joblib.load(model_path)\n",
    "assert hasattr(loaded_model, 'predict'), \"Model doesn't have predict method\"\n",
    "assert hasattr(loaded_model, 'arima_model'), \"Missing arima_model attribute\"\n",
    "assert hasattr(loaded_model, 'prophet_model'), \"Missing prophet_model attribute\"\n",
    "\n",
    "logger.info(\"‚úÖ All validations passed\")\n",
    "print(f\"\\n‚úÖ KServe Validation Complete:\")\n",
    "print(f\"   Model path: {model_path}\")\n",
    "print(f\"   File count: {len(pkl_files)} (correct - must be 1)\")\n",
    "print(f\"   Model type: {type(loaded_model).__name__}\")\n",
    "print(f\"   Predictions: {PROCESSED_DIR / 'timeseries_predictions.parquet'}\")\n",
    "print(f\"\\nüéØ Ready for KServe deployment!\")\n",
    "print(f\"   Deploy with: oc apply -f <inference-service.yaml>\")\n",
    "print(f\"   storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Synthetic anomalies from `synthetic-anomaly-generation.ipynb`\n",
    "- **Output**: Time series models for `ensemble-anomaly-methods.ipynb`\n",
    "- **Coordination Engine**: Models can be deployed for real-time detection\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review model performance metrics\n",
    "2. Proceed to `lstm-based-prediction.ipynb` for deep learning approach\n",
    "3. Compare with ensemble methods\n",
    "4. Deploy best model to coordination engine\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [ARIMA Documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html)\n",
    "- [Prophet Documentation](https://facebook.github.io/prophet/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
    "- ‚úÖ Reproducible and testable\n",
    "- ‚úÖ Models trained on synthetic data generalize to real anomalies\n",
    "\n",
    "### Machine Learning Best Practice\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement ARIMA forecasting on synthetic data\n",
    "- Use Prophet for time series analysis\n",
    "- Detect anomalies via forecast deviations\n",
    "- Handle seasonal patterns\n",
    "- Evaluate detection performance with labeled data\n",
    "\n",
    "## Key Concepts\n",
    "- **ARIMA**: AutoRegressive Integrated Moving Average\n",
    "- **Prophet**: Facebook's time series forecasting tool\n",
    "- **Forecast Error**: Deviation between actual and predicted values\n",
    "- **Seasonality**: Repeating patterns in time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Why Synthetic Data for Training?\n",
    "- **He & Garcia (2009)**: \"Learning from Imbalanced Data\" - https://ieeexplore.ieee.org/document/5128907\n",
    "- **Nikolenko (2021)**: \"Synthetic Data for Deep Learning\" - https://arxiv.org/abs/1909.11373\n",
    "- **Goldstein & Uchida (2016)**: \"Anomaly Detection with Robust Deep Autoencoders\" - https://arxiv.org/abs/1511.08747\n",
    "\n",
    "### Time Series Anomaly Detection\n",
    "- **Malhotra et al. (2016)**: \"Time Series Anomaly Detection with LSTM Networks\" - https://arxiv.org/abs/1607.00148\n",
    "- **Taylor & Letham (2018)**: \"Forecasting at Scale (Prophet)\" - https://peerj.com/articles/3190\n",
    "- **Box & Jenkins (1970)**: \"Time Series Analysis, Forecasting and Control (ARIMA)\" - Classic reference\n",
    "\n",
    "### Key Takeaway\n",
    "Synthetic data provides labeled training examples that allow us to:\n",
    "1. Train models with known ground truth\n",
    "2. Evaluate performance with precision, recall, and F1 scores\n",
    "3. Ensure reproducible and testable results\n",
    "4. Build models that generalize to real-world anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport logging\nfrom pathlib import Path\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"‚úÖ Utils path found: {utils_path}\")\nelse:\n    print(\"‚ö†Ô∏è Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import setup_environment\n    print(\"‚úÖ Common functions imported\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Common functions not available: {e}\")\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Setup environment\nenv_info = setup_environment()\nlogger.info(f\"Environment ready: {env_info}\")\n\n# Define paths\nDATA_DIR = Path('/opt/app-root/src/data')\nPROCESSED_DIR = DATA_DIR / 'processed'\n\n# Use /mnt/models for persistent storage (model-storage-pvc)\n# Fallback to local for development outside cluster\nMODELS_DIR = Path('/mnt/models') if Path('/mnt/models').exists() else Path('/opt/app-root/src/models')\n\n# Create KServe-compatible subdirectory structure\n# CHANGED: Use unique model name to avoid conflict with isolation-forest's anomaly-detector model\nMODEL_NAME = 'timeseries-predictor'\nMODEL_DIR = MODELS_DIR / MODEL_NAME\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\n\nlogger.info(f\"Data directory: {DATA_DIR}\")\nlogger.info(f\"Models directory: {MODEL_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Load Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or generate synthetic data\n",
    "data_file = PROCESSED_DIR / 'synthetic_anomalies.parquet'\n",
    "\n",
    "if data_file.exists():\n",
    "    df = pd.read_parquet(data_file)\n",
    "    logger.info(f\"Loaded existing data: {df.shape}\")\n",
    "else:\n",
    "    logger.info(\"Synthetic data not found - generating for validation...\")\n",
    "    # Generate synthetic data inline\n",
    "    from datetime import datetime, timedelta\n",
    "    np.random.seed(42)\n",
    "    n_points = 1000\n",
    "    n_features = 5\n",
    "    \n",
    "    start_time = datetime.now() - timedelta(days=30)\n",
    "    timestamps = [start_time + timedelta(minutes=i) for i in range(n_points)]\n",
    "    \n",
    "    data = {}\n",
    "    for i in range(n_features):\n",
    "        trend = np.linspace(50, 60, n_points)\n",
    "        seasonal = 10 * np.sin(np.linspace(0, 4*np.pi, n_points))\n",
    "        noise = np.random.normal(0, 2, n_points)\n",
    "        data[f'metric_{i}'] = trend + seasonal + noise\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = timestamps\n",
    "    df['label'] = 0\n",
    "    \n",
    "    # Inject anomalies\n",
    "    anomaly_indices = np.random.choice(len(df), 50, replace=False)\n",
    "    for idx in anomaly_indices:\n",
    "        features = np.random.choice(5, 2, replace=False)\n",
    "        for feat in features:\n",
    "            col = f'metric_{feat}'\n",
    "            std = df[col].std()\n",
    "            df.loc[idx, col] += 3.0 * std * np.random.choice([-1, 1])\n",
    "        df.loc[idx, 'label'] = 1\n",
    "    \n",
    "    # Save for downstream notebooks\n",
    "    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(data_file)\n",
    "    logger.info(f\"Generated and saved synthetic data: {df.shape}\")\n",
    "\n",
    "print(df.head())\n",
    "print(f\"\\nData info:\")\n",
    "print(f\"  Normal samples: {(df['label'] == 0).sum()}\")\n",
    "print(f\"  Anomalous samples: {(df['label'] == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ARIMA-Based Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "def detect_anomalies_arima(series, threshold_std=2.5):\n",
    "    \"\"\"\n",
    "    Detect anomalies using ARIMA forecasting.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data\n",
    "        threshold_std: Number of standard deviations for anomaly threshold\n",
    "    \n",
    "    Returns:\n",
    "        Anomaly predictions (0=normal, 1=anomaly)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fit ARIMA model\n",
    "        model = ARIMA(series, order=(1, 1, 1))\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Get residuals\n",
    "        residuals = results.resid\n",
    "        \n",
    "        # Calculate threshold\n",
    "        threshold = threshold_std * residuals.std()\n",
    "        \n",
    "        # Detect anomalies\n",
    "        predictions = (np.abs(residuals) > threshold).astype(int)\n",
    "        \n",
    "        return predictions, results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ARIMA error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Apply ARIMA to first metric\n",
    "metric_col = 'metric_0'\n",
    "arima_preds, arima_model = detect_anomalies_arima(df[metric_col])\n",
    "\n",
    "if arima_preds is not None:\n",
    "    logger.info(f\"ARIMA detected {arima_preds.sum()} anomalies\")\n",
    "    # Evaluate\n",
    "    precision = precision_score(df['label'], arima_preds, zero_division=0)\n",
    "    recall = recall_score(df['label'], arima_preds, zero_division=0)\n",
    "    f1 = f1_score(df['label'], arima_preds, zero_division=0)\n",
    "    print(f\"ARIMA Performance: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prophet-Based Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "def detect_anomalies_prophet(df_input, threshold_std=2.5):\n",
    "    \"\"\"\n",
    "    Detect anomalies using Prophet forecasting.\n",
    "    \n",
    "    Args:\n",
    "        df_input: DataFrame with 'timestamp' and 'metric_0' columns\n",
    "        threshold_std: Number of standard deviations for anomaly threshold\n",
    "    \n",
    "    Returns:\n",
    "        Anomaly predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare data for Prophet\n",
    "        prophet_df = pd.DataFrame({\n",
    "            'ds': df_input['timestamp'],\n",
    "            'y': df_input['metric_0']\n",
    "        })\n",
    "        \n",
    "        # Fit Prophet model\n",
    "        model = Prophet(yearly_seasonality=False, daily_seasonality=False)\n",
    "        model.fit(prophet_df)\n",
    "        \n",
    "        # Make forecast\n",
    "        forecast = model.predict(prophet_df[['ds']])\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = df_input['metric_0'].values - forecast['yhat'].values\n",
    "        threshold = threshold_std * residuals.std()\n",
    "        \n",
    "        # Detect anomalies\n",
    "        predictions = (np.abs(residuals) > threshold).astype(int)\n",
    "        \n",
    "        return predictions, model, forecast\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prophet error: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Apply Prophet\n",
    "prophet_preds, prophet_model, prophet_forecast = detect_anomalies_prophet(df)\n",
    "\n",
    "if prophet_preds is not None:\n",
    "    logger.info(f\"Prophet detected {prophet_preds.sum()} anomalies\")\n",
    "    precision = precision_score(df['label'], prophet_preds, zero_division=0)\n",
    "    recall = recall_score(df['label'], prophet_preds, zero_division=0)\n",
    "    f1 = f1_score(df['label'], prophet_preds, zero_division=0)\n",
    "    print(f\"Prophet Performance: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "source": "from sklearn.base import BaseEstimator, TransformerMixin\nimport joblib\n\nclass TimeSeriesEnsemble(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Wrapper class that combines ARIMA and Prophet models for KServe compatibility.\n    KServe requires a single .pkl file, not multiple model files.\n    \"\"\"\n    def __init__(self, arima_model=None, prophet_model=None):\n        self.arima_model = arima_model\n        self.prophet_model = prophet_model\n\n    def predict(self, X, periods=None):\n        \"\"\"\n        Make predictions using both models and return ensemble result.\n        \n        Args:\n            X: Input data (DataFrame or array-like)\n            periods: Number of periods to forecast (for time series)\n        \n        Returns:\n            Dictionary with predictions from both models\n        \"\"\"\n        results = {}\n        \n        if self.arima_model is not None:\n            try:\n                # ARIMA forecast\n                arima_forecast = self.arima_model.forecast(steps=periods or len(X))\n                results['arima'] = arima_forecast\n            except Exception as e:\n                results['arima_error'] = str(e)\n                logger.warning(f\"ARIMA prediction failed: {e}\")\n        \n        if self.prophet_model is not None:\n            try:\n                # Prophet forecast\n                import pandas as pd\n                future = self.prophet_model.make_future_dataframe(periods=periods or len(X))\n                prophet_forecast = self.prophet_model.predict(future)\n                results['prophet'] = prophet_forecast['yhat'].values\n            except Exception as e:\n                results['prophet_error'] = str(e)\n                logger.warning(f\"Prophet prediction failed: {e}\")\n        \n        # Return ensemble (average of both predictions)\n        if 'arima' in results and 'prophet' in results:\n            results['ensemble'] = (results['arima'] + results['prophet']) / 2\n        elif 'arima' in results:\n            results['ensemble'] = results['arima']\n        elif 'prophet' in results:\n            results['ensemble'] = results['prophet']\n        \n        return results\n    \n    def get_params(self, deep=True):\n        return {\n            'arima_model': self.arima_model,\n            'prophet_model': self.prophet_model\n        }\n    \n    def set_params(self, **params):\n        for key, value in params.items():\n            setattr(self, key, value)\n        return self\n\nprint(\"‚úÖ TimeSeriesEnsemble class defined for KServe compatibility\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create ensemble wrapper combining both models (KServe compatible)\n# KServe sklearn server requires EXACTLY ONE .pkl file per model directory\nensemble_model = TimeSeriesEnsemble(\n    arima_model=arima_model,\n    prophet_model=prophet_model\n)\n\nprint(\"‚úÖ Created TimeSeriesEnsemble wrapper\")\nprint(\"   ARIMA model:\", \"‚úì\" if arima_model is not None else \"‚úó\")\nprint(\"   Prophet model:\", \"‚úì\" if prophet_model is not None else \"‚úó\")\n\n# Save as SINGLE .pkl file for KServe\n# Path: /mnt/models/anomaly-detector/model.pkl\nmodel_path = MODEL_DIR / 'model.pkl'\n\n# Remove old separate model files if they exist (migration)\nold_arima = MODEL_DIR / 'arima_model.pkl'\nold_prophet = MODEL_DIR / 'prophet_model.pkl'\nfor old_file in [old_arima, old_prophet]:\n    if old_file.exists():\n        old_file.unlink()\n        logger.info(f\"üóëÔ∏è  Removed old file: {old_file.name}\")\n\n# Save ensemble model\njoblib.dump(ensemble_model, model_path)\nlogger.info(f\"üíæ Saved ensemble model to {model_path}\")\n\n# Verify only ONE .pkl file exists (KServe requirement)\npkl_files = list(MODEL_DIR.glob('*.pkl'))\nif len(pkl_files) != 1:\n    raise RuntimeError(\n        f\"‚ùå ERROR: Expected 1 .pkl file, found {len(pkl_files)}: {pkl_files}\\n\"\n        f\"KServe requires EXACTLY ONE .pkl file per model directory.\"\n    )\n\nprint(f\"‚úÖ Model saved correctly for KServe:\")\nprint(f\"   Path: {pkl_files[0]}\")\nprint(f\"   Size: {pkl_files[0].stat().st_size / 1024:.2f} KB\")\nprint(f\"   Files in directory: {len(pkl_files)} (correct - must be 1)\")\n\n# Upload to S3 for persistent storage (optional)\ntry:\n    from common_functions import upload_model_to_s3, test_s3_connection\n    \n    if test_s3_connection():\n        upload_model_to_s3(\n            str(model_path),\n            s3_key='models/anomaly-detection/anomaly-detector/model.pkl'\n        )\n        logger.info(\"‚òÅÔ∏è  Uploaded to S3\")\n    else:\n        logger.info(\"‚ö†Ô∏è S3 not available - model saved locally only\")\nexcept ImportError:\n    logger.info(\"‚ö†Ô∏è S3 functions not available - model saved locally only\")\nexcept Exception as e:\n    logger.warning(f\"‚ö†Ô∏è S3 upload failed (non-critical): {e}\")\n\n# Save predictions\nresults_df = pd.DataFrame({\n    'actual': df['label'],\n    'arima_pred': arima_preds if arima_preds is not None else 0,\n    'prophet_pred': prophet_preds if prophet_preds is not None else 0\n})\nresults_df.to_parquet(PROCESSED_DIR / 'timeseries_predictions.parquet')\nlogger.info(\"üíæ Saved predictions\")\n\nprint(f\"\\nüéâ KServe model deployment ready!\")\nprint(f\"   Model name: {MODEL_NAME}\")\nprint(f\"   Model path: {model_path}\")\nprint(f\"   Deploy to KServe with: storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Verify outputs - KServe compatible structure\nmodel_path = MODEL_DIR / 'model.pkl'\n\n# Check model file exists\nassert model_path.exists(), f\"Model not saved: {model_path}\"\n\n# Check predictions saved\nassert (PROCESSED_DIR / 'timeseries_predictions.parquet').exists(), \"Predictions not saved\"\n\n# Verify KServe requirement: EXACTLY ONE .pkl file\npkl_files = list(MODEL_DIR.glob('*.pkl'))\nassert len(pkl_files) == 1, f\"ERROR: Expected 1 .pkl file, found {len(pkl_files)}: {pkl_files}\"\n\n# Verify can load and use the model\nloaded_model = joblib.load(model_path)\nassert hasattr(loaded_model, 'predict'), \"Model doesn't have predict method\"\nassert hasattr(loaded_model, 'arima_model'), \"Missing arima_model attribute\"\nassert hasattr(loaded_model, 'prophet_model'), \"Missing prophet_model attribute\"\n\nlogger.info(\"‚úÖ All validations passed\")\nprint(f\"\\n‚úÖ KServe Validation Complete:\")\nprint(f\"   Model path: {model_path}\")\nprint(f\"   File count: {len(pkl_files)} (correct - must be 1)\")\nprint(f\"   Model type: {type(loaded_model).__name__}\")\nprint(f\"   Predictions: {PROCESSED_DIR / 'timeseries_predictions.parquet'}\")\nprint(f\"\\nüéØ Ready for KServe deployment!\")\nprint(f\"   Deploy with: oc apply -f <inference-service.yaml>\")\nprint(f\"   storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Synthetic anomalies from `synthetic-anomaly-generation.ipynb`\n",
    "- **Output**: Time series models for `ensemble-anomaly-methods.ipynb`\n",
    "- **Coordination Engine**: Models can be deployed for real-time detection\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review model performance metrics\n",
    "2. Proceed to `lstm-based-prediction.ipynb` for deep learning approach\n",
    "3. Compare with ensemble methods\n",
    "4. Deploy best model to coordination engine\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [ARIMA Documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html)\n",
    "- [Prophet Documentation](https://facebook.github.io/prophet/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

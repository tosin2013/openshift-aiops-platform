{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordination Engine Integration for Self-Healing Platform\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to integrate trained anomaly detection models with the Self-Healing Platform's coordination engine. It shows the complete workflow from anomaly detection to automated remediation actions.\n",
    "\n",
    "## Prerequisites\n",
    "- Trained anomaly detection models (from 02-anomaly-detection notebooks)\n",
    "- Running coordination engine in the cluster\n",
    "- Access to OpenShift API and Prometheus metrics\n",
    "\n",
    "## Expected Outcomes\n",
    "- Understand coordination engine API and integration patterns\n",
    "- Implement real-time anomaly detection pipeline\n",
    "- Demonstrate automated remediation workflows\n",
    "- Test end-to-end self-healing scenarios\n",
    "\n",
    "## References\n",
    "- ADR-002: Hybrid Deterministic-AI Self-Healing Approach\n",
    "- ADR-009: Bootstrap Deployment Automation Architecture\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport sys\nimport os\nfrom pathlib import Path\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"âœ… Utils path found: {utils_path}\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport requests\nimport json\nimport time\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Kubernetes client\ntry:\n    from kubernetes import client, config\n    k8s_available = True\n    print(\"âœ… Kubernetes client available\")\nexcept ImportError:\n    k8s_available = False\n    print(\"âš ï¸ Kubernetes client not available - using simulation mode\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import (\n        setup_environment, print_environment_info,\n        generate_synthetic_timeseries, validate_data_quality,\n        load_processed_data\n    )\n    print(\"âœ… Common functions imported\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Using fallback implementations\")\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n    def print_environment_info(env_info):\n        print(f\"ðŸ“ Data dir: {env_info.get('data_dir', 'N/A')}\")\n    def generate_synthetic_timeseries(*args, **kwargs):\n        return pd.DataFrame({'value': np.random.random(100)})\n    def validate_data_quality(df, name=''):\n        return {'valid': True}\n    def load_processed_data(filename):\n        return None\n\nprint(\"âœ… Libraries imported successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "env_info = setup_environment()\n",
    "print_environment_info(env_info)\n",
    "\n",
    "# Coordination Engine Configuration\n",
    "COORDINATION_ENGINE_CONFIG = {\n",
    "    'base_url': 'http://coordination-engine.self-healing-platform.svc.cluster.local:8080',\n",
    "    'timeout': 30,\n",
    "    'retry_attempts': 3,\n",
    "    'retry_delay': 5\n",
    "}\n",
    "\n",
    "# Self-Healing Actions Configuration\n",
    "REMEDIATION_ACTIONS = {\n",
    "    'pod_restart': {\n",
    "        'description': 'Restart problematic pods',\n",
    "        'severity_threshold': 0.7,\n",
    "        'cooldown_minutes': 10\n",
    "    },\n",
    "    'scale_up': {\n",
    "        'description': 'Scale up deployment replicas',\n",
    "        'severity_threshold': 0.8,\n",
    "        'cooldown_minutes': 15\n",
    "    },\n",
    "    'resource_adjustment': {\n",
    "        'description': 'Adjust resource limits/requests',\n",
    "        'severity_threshold': 0.6,\n",
    "        'cooldown_minutes': 30\n",
    "    },\n",
    "    'alert_escalation': {\n",
    "        'description': 'Escalate to human operators',\n",
    "        'severity_threshold': 0.9,\n",
    "        'cooldown_minutes': 5\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"ðŸŽ¯ Coordination engine: {COORDINATION_ENGINE_CONFIG['base_url']}\")\n",
    "print(f\"ðŸ”§ Available remediation actions: {len(REMEDIATION_ACTIONS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordination Engine Client\n",
    "\n",
    "Implement client for communicating with the coordination engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinationEngineClient:\n",
    "    \"\"\"\n",
    "    Client for interacting with the Self-Healing Platform coordination engine\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url, timeout=30):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    def health_check(self):\n",
    "        \"\"\"\n",
    "        Check if coordination engine is healthy\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(\n",
    "                f\"{self.base_url}/health\",\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            return response.status_code == 200\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Health check failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def submit_anomaly(self, anomaly_data):\n",
    "        \"\"\"\n",
    "        Submit anomaly detection results to coordination engine\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.base_url}/api/v1/anomalies\",\n",
    "                json=anomaly_data,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            return response.status_code == 200, response.json() if response.status_code == 200 else None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to submit anomaly: {e}\")\n",
    "            return False, None\n",
    "    \n",
    "    def get_remediation_status(self, remediation_id):\n",
    "        \"\"\"\n",
    "        Get status of a remediation action\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(\n",
    "                f\"{self.base_url}/api/v1/remediations/{remediation_id}\",\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            return response.status_code == 200, response.json() if response.status_code == 200 else None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to get remediation status: {e}\")\n",
    "            return False, None\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"\n",
    "        Get coordination engine metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(\n",
    "                f\"{self.base_url}/metrics\",\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            return response.status_code == 200, response.text if response.status_code == 200 else None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to get metrics: {e}\")\n",
    "            return False, None\n",
    "\n",
    "# Initialize coordination engine client\n",
    "coord_client = CoordinationEngineClient(\n",
    "    base_url=COORDINATION_ENGINE_CONFIG['base_url'],\n",
    "    timeout=COORDINATION_ENGINE_CONFIG['timeout']\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "print(\"ðŸ” Testing coordination engine connection...\")\n",
    "if coord_client.health_check():\n",
    "    print(\"âœ… Coordination engine is healthy\")\n",
    "else:\n",
    "    print(\"âš ï¸ Coordination engine not available - using simulation mode\")\n",
    "    \n",
    "print(\"âœ… Coordination engine client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection Pipeline\n",
    "\n",
    "Load trained models and create real-time anomaly detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetectionPipeline:\n",
    "    \"\"\"\n",
    "    Real-time anomaly detection pipeline for self-healing platform\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=None):\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.feature_columns = None\n",
    "        self.last_prediction_time = None\n",
    "        \n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.load_model(model_path)\n",
    "        else:\n",
    "            print(\"âš ï¸ No model provided - will create demo model\")\n",
    "            self._create_demo_model()\n",
    "    \n",
    "    def _create_demo_model(self):\n",
    "        \"\"\"\n",
    "        Create a demo model for demonstration purposes\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        \n",
    "        print(\"ðŸ”§ Creating demo anomaly detection model...\")\n",
    "        \n",
    "        # Generate sample training data\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        n_features = 10\n",
    "        \n",
    "        # Normal data\n",
    "        X_normal = np.random.normal(0, 1, (int(n_samples * 0.9), n_features))\n",
    "        # Anomalous data\n",
    "        X_anomaly = np.random.normal(3, 1, (int(n_samples * 0.1), n_features))\n",
    "        \n",
    "        X_train = np.vstack([X_normal, X_anomaly])\n",
    "        \n",
    "        # Train scaler and model\n",
    "        self.scaler = RobustScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        self.model = IsolationForest(\n",
    "            contamination=0.1,\n",
    "            n_estimators=100,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.model.fit(X_train_scaled)\n",
    "        \n",
    "        # Set feature columns\n",
    "        self.feature_columns = [f'feature_{i}' for i in range(n_features)]\n",
    "        \n",
    "        print(\"âœ… Demo model created successfully\")\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Load trained model from disk\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model_data = joblib.load(model_path)\n",
    "            self.model = model_data['model']\n",
    "            self.scaler = model_data['scaler']\n",
    "            self.feature_columns = model_data['feature_columns']\n",
    "            print(f\"âœ… Model loaded from {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load model: {e}\")\n",
    "            self._create_demo_model()\n",
    "    \n",
    "    def predict_anomaly(self, features):\n",
    "        \"\"\"\n",
    "        Predict anomalies in feature data\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model available for prediction\")\n",
    "        \n",
    "        # Ensure features is 2D\n",
    "        if len(features.shape) == 1:\n",
    "            features = features.reshape(1, -1)\n",
    "        \n",
    "        # Scale features\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(features_scaled)\n",
    "        scores = self.model.decision_function(features_scaled)\n",
    "        \n",
    "        # Convert to anomaly probabilities\n",
    "        anomaly_mask = predictions == -1\n",
    "        \n",
    "        results = {\n",
    "            'is_anomaly': anomaly_mask,\n",
    "            'anomaly_scores': scores,\n",
    "            'severity': self._calculate_severity(scores),\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        \n",
    "        self.last_prediction_time = results['timestamp']\n",
    "        return results\n",
    "    \n",
    "    def _calculate_severity(self, scores):\n",
    "        \"\"\"\n",
    "        Calculate anomaly severity based on scores\n",
    "        \"\"\"\n",
    "        # Normalize scores to 0-1 range (lower scores = more anomalous)\n",
    "        min_score = -0.5  # Typical range for isolation forest\n",
    "        max_score = 0.5\n",
    "        \n",
    "        normalized_scores = (scores - min_score) / (max_score - min_score)\n",
    "        normalized_scores = np.clip(normalized_scores, 0, 1)\n",
    "        \n",
    "        # Invert so higher values = more severe\n",
    "        severity = 1 - normalized_scores\n",
    "        \n",
    "        return severity\n",
    "\n",
    "# Initialize anomaly detection pipeline\n",
    "print(\"ðŸ”§ Initializing anomaly detection pipeline...\")\n",
    "anomaly_pipeline = AnomalyDetectionPipeline()\n",
    "\n",
    "# Test the pipeline with sample data\n",
    "print(\"\\nðŸ§ª Testing anomaly detection pipeline...\")\n",
    "test_features = np.random.normal(0, 1, (5, len(anomaly_pipeline.feature_columns)))\n",
    "test_results = anomaly_pipeline.predict_anomaly(test_features)\n",
    "\n",
    "print(f\"  Predictions: {test_results['is_anomaly'].sum()} anomalies detected\")\n",
    "print(f\"  Severity range: {test_results['severity'].min():.3f} - {test_results['severity'].max():.3f}\")\n",
    "print(\"âœ… Pipeline test successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Healing Workflow Implementation\n",
    "\n",
    "Implement the complete self-healing workflow from anomaly detection to remediation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfHealingWorkflow:\n",
    "    \"\"\"\n",
    "    Complete self-healing workflow implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, anomaly_pipeline, coord_client):\n",
    "        self.anomaly_pipeline = anomaly_pipeline\n",
    "        self.coord_client = coord_client\n",
    "        self.remediation_history = []\n",
    "        self.cooldown_tracker = {}\n",
    "        \n",
    "    def process_metrics(self, metrics_data):\n",
    "        \"\"\"\n",
    "        Process incoming metrics and trigger self-healing if needed\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ“Š Processing metrics at {datetime.now()}\")\n",
    "        \n",
    "        # Convert metrics to feature vector\n",
    "        features = self._extract_features(metrics_data)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        anomaly_results = self.anomaly_pipeline.predict_anomaly(features)\n",
    "        \n",
    "        # Process each anomaly\n",
    "        for i, is_anomaly in enumerate(anomaly_results['is_anomaly']):\n",
    "            if is_anomaly:\n",
    "                severity = anomaly_results['severity'][i]\n",
    "                score = anomaly_results['anomaly_scores'][i]\n",
    "                \n",
    "                print(f\"ðŸš¨ Anomaly detected! Severity: {severity:.3f}, Score: {score:.3f}\")\n",
    "                \n",
    "                # Create anomaly record\n",
    "                anomaly_record = {\n",
    "                    'timestamp': anomaly_results['timestamp'].isoformat(),\n",
    "                    'severity': float(severity),\n",
    "                    'anomaly_score': float(score),\n",
    "                    'metrics': metrics_data,\n",
    "                    'source': 'isolation_forest_model'\n",
    "                }\n",
    "                \n",
    "                # Determine and execute remediation\n",
    "                remediation_action = self._select_remediation_action(severity)\n",
    "                if remediation_action:\n",
    "                    self._execute_remediation(anomaly_record, remediation_action)\n",
    "        \n",
    "        return anomaly_results\n",
    "    \n",
    "    def _extract_features(self, metrics_data):\n",
    "        \"\"\"\n",
    "        Extract features from metrics data for anomaly detection\n",
    "        \"\"\"\n",
    "        # For demo purposes, generate random features\n",
    "        # In real implementation, this would extract actual metrics\n",
    "        n_features = len(self.anomaly_pipeline.feature_columns)\n",
    "        \n",
    "        # Simulate feature extraction from metrics\n",
    "        features = np.random.normal(0, 1, n_features)\n",
    "        \n",
    "        # Add some realistic patterns\n",
    "        if 'cpu_high' in str(metrics_data):\n",
    "            features[0] += 2  # Simulate high CPU feature\n",
    "        if 'memory_leak' in str(metrics_data):\n",
    "            features[1] += 3  # Simulate memory issue feature\n",
    "        \n",
    "        return features.reshape(1, -1)\n",
    "    \n",
    "    def _select_remediation_action(self, severity):\n",
    "        \"\"\"\n",
    "        Select appropriate remediation action based on severity\n",
    "        \"\"\"\n",
    "        # Sort actions by severity threshold (descending)\n",
    "        sorted_actions = sorted(\n",
    "            REMEDIATION_ACTIONS.items(),\n",
    "            key=lambda x: x[1]['severity_threshold'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Find first action that matches severity\n",
    "        for action_name, action_config in sorted_actions:\n",
    "            if severity >= action_config['severity_threshold']:\n",
    "                # Check cooldown\n",
    "                if self._check_cooldown(action_name, action_config['cooldown_minutes']):\n",
    "                    return action_name\n",
    "                else:\n",
    "                    print(f\"â° Action {action_name} in cooldown period\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _check_cooldown(self, action_name, cooldown_minutes):\n",
    "        \"\"\"\n",
    "        Check if action is in cooldown period\n",
    "        \"\"\"\n",
    "        if action_name not in self.cooldown_tracker:\n",
    "            return True\n",
    "        \n",
    "        last_execution = self.cooldown_tracker[action_name]\n",
    "        cooldown_period = timedelta(minutes=cooldown_minutes)\n",
    "        \n",
    "        return datetime.now() - last_execution > cooldown_period\n",
    "    \n",
    "    def _execute_remediation(self, anomaly_record, action_name):\n",
    "        \"\"\"\n",
    "        Execute remediation action\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ”§ Executing remediation action: {action_name}\")\n",
    "        \n",
    "        # Create remediation request\n",
    "        remediation_request = {\n",
    "            'action': action_name,\n",
    "            'anomaly': anomaly_record,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'description': REMEDIATION_ACTIONS[action_name]['description']\n",
    "        }\n",
    "        \n",
    "        # Submit to coordination engine\n",
    "        success, response = self.coord_client.submit_anomaly({\n",
    "            'type': 'remediation_request',\n",
    "            'data': remediation_request\n",
    "        })\n",
    "        \n",
    "        if success:\n",
    "            print(f\"âœ… Remediation request submitted successfully\")\n",
    "            remediation_id = response.get('remediation_id', 'unknown')\n",
    "        else:\n",
    "            print(f\"âŒ Failed to submit remediation request - executing locally\")\n",
    "            remediation_id = f\"local_{int(time.time())}\"\n",
    "            self._execute_local_remediation(action_name, anomaly_record)\n",
    "        \n",
    "        # Update cooldown tracker\n",
    "        self.cooldown_tracker[action_name] = datetime.now()\n",
    "        \n",
    "        # Record in history\n",
    "        self.remediation_history.append({\n",
    "            'remediation_id': remediation_id,\n",
    "            'action': action_name,\n",
    "            'timestamp': datetime.now(),\n",
    "            'severity': anomaly_record['severity'],\n",
    "            'success': success\n",
    "        })\n",
    "    \n",
    "    def _execute_local_remediation(self, action_name, anomaly_record):\n",
    "        \"\"\"\n",
    "        Execute remediation action locally (simulation)\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ”§ Simulating local remediation: {action_name}\")\n",
    "        \n",
    "        if action_name == 'pod_restart':\n",
    "            print(\"  ðŸ”„ Simulating pod restart...\")\n",
    "            time.sleep(1)  # Simulate action time\n",
    "            print(\"  âœ… Pod restart completed\")\n",
    "            \n",
    "        elif action_name == 'scale_up':\n",
    "            print(\"  ðŸ“ˆ Simulating deployment scale up...\")\n",
    "            time.sleep(2)  # Simulate action time\n",
    "            print(\"  âœ… Scale up completed\")\n",
    "            \n",
    "        elif action_name == 'resource_adjustment':\n",
    "            print(\"  âš™ï¸ Simulating resource adjustment...\")\n",
    "            time.sleep(1.5)  # Simulate action time\n",
    "            print(\"  âœ… Resource adjustment completed\")\n",
    "            \n",
    "        elif action_name == 'alert_escalation':\n",
    "            print(\"  ðŸš¨ Simulating alert escalation...\")\n",
    "            print(\"  ðŸ“§ Alert sent to operations team\")\n",
    "            print(\"  âœ… Escalation completed\")\n",
    "    \n",
    "    def get_remediation_summary(self):\n",
    "        \"\"\"\n",
    "        Get summary of remediation actions\n",
    "        \"\"\"\n",
    "        if not self.remediation_history:\n",
    "            return \"No remediation actions executed yet\"\n",
    "        \n",
    "        df = pd.DataFrame(self.remediation_history)\n",
    "        \n",
    "        summary = {\n",
    "            'total_actions': len(df),\n",
    "            'success_rate': df['success'].mean(),\n",
    "            'actions_by_type': df['action'].value_counts().to_dict(),\n",
    "            'average_severity': df['severity'].mean(),\n",
    "            'last_action': df.iloc[-1]['timestamp'] if len(df) > 0 else None\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize self-healing workflow\n",
    "print(\"ðŸ”§ Initializing self-healing workflow...\")\n",
    "healing_workflow = SelfHealingWorkflow(anomaly_pipeline, coord_client)\n",
    "print(\"âœ… Self-healing workflow initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Demonstration\n",
    "\n",
    "Demonstrate the complete self-healing workflow with simulated scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate different anomaly scenarios\n",
    "print(\"ðŸŽ­ Running Self-Healing Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Scenario 1: Normal metrics (should not trigger)\n",
    "print(\"\\nðŸ“Š Scenario 1: Normal Operations\")\n",
    "normal_metrics = {\n",
    "    'cpu_usage': 45.2,\n",
    "    'memory_usage': 62.1,\n",
    "    'pod_count': 12,\n",
    "    'response_time': 150\n",
    "}\n",
    "result1 = healing_workflow.process_metrics(normal_metrics)\n",
    "print(f\"Result: {result1['is_anomaly'].sum()} anomalies detected\")\n",
    "\n",
    "# Scenario 2: High CPU (moderate anomaly)\n",
    "print(\"\\nðŸ“Š Scenario 2: High CPU Usage\")\n",
    "high_cpu_metrics = {\n",
    "    'cpu_usage': 95.8,\n",
    "    'memory_usage': 68.3,\n",
    "    'pod_count': 12,\n",
    "    'response_time': 450,\n",
    "    'cpu_high': True  # Trigger feature\n",
    "}\n",
    "result2 = healing_workflow.process_metrics(high_cpu_metrics)\n",
    "print(f\"Result: {result2['is_anomaly'].sum()} anomalies detected\")\n",
    "\n",
    "# Scenario 3: Memory leak (severe anomaly)\n",
    "print(\"\\nðŸ“Š Scenario 3: Memory Leak Detected\")\n",
    "memory_leak_metrics = {\n",
    "    'cpu_usage': 78.2,\n",
    "    'memory_usage': 98.7,\n",
    "    'pod_count': 15,\n",
    "    'response_time': 2500,\n",
    "    'memory_leak': True  # Trigger feature\n",
    "}\n",
    "result3 = healing_workflow.process_metrics(memory_leak_metrics)\n",
    "print(f\"Result: {result3['is_anomaly'].sum()} anomalies detected\")\n",
    "\n",
    "# Scenario 4: Critical system failure\n",
    "print(\"\\nðŸ“Š Scenario 4: Critical System Failure\")\n",
    "critical_metrics = {\n",
    "    'cpu_usage': 99.9,\n",
    "    'memory_usage': 99.2,\n",
    "    'pod_count': 3,  # Many pods crashed\n",
    "    'response_time': 10000,\n",
    "    'cpu_high': True,\n",
    "    'memory_leak': True\n",
    "}\n",
    "result4 = healing_workflow.process_metrics(critical_metrics)\n",
    "print(f\"Result: {result4['is_anomaly'].sum()} anomalies detected\")\n",
    "\n",
    "# Wait a moment to simulate time passing\n",
    "print(\"\\nâ° Waiting 2 seconds...\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Scenario 5: Test cooldown (should not trigger same action)\n",
    "print(\"\\nðŸ“Š Scenario 5: Testing Cooldown Period\")\n",
    "result5 = healing_workflow.process_metrics(critical_metrics)  # Same critical metrics\n",
    "print(f\"Result: {result5['is_anomaly'].sum()} anomalies detected\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Demonstration completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display remediation summary\n",
    "print(\"ðŸ“‹ Self-Healing Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "summary = healing_workflow.get_remediation_summary()\n",
    "\n",
    "if isinstance(summary, dict):\n",
    "    print(f\"Total remediation actions: {summary['total_actions']}\")\n",
    "    print(f\"Success rate: {summary['success_rate']:.1%}\")\n",
    "    print(f\"Average severity: {summary['average_severity']:.3f}\")\n",
    "    \n",
    "    print(\"\\nActions by type:\")\n",
    "    for action, count in summary['actions_by_type'].items():\n",
    "        description = REMEDIATION_ACTIONS[action]['description']\n",
    "        print(f\"  {action}: {count} times - {description}\")\n",
    "    \n",
    "    if summary['last_action']:\n",
    "        print(f\"\\nLast action: {summary['last_action']}\")\n",
    "else:\n",
    "    print(summary)\n",
    "\n",
    "# Display cooldown status\n",
    "print(\"\\nâ° Cooldown Status:\")\n",
    "current_time = datetime.now()\n",
    "for action_name, last_execution in healing_workflow.cooldown_tracker.items():\n",
    "    cooldown_minutes = REMEDIATION_ACTIONS[action_name]['cooldown_minutes']\n",
    "    time_since = current_time - last_execution\n",
    "    remaining = timedelta(minutes=cooldown_minutes) - time_since\n",
    "    \n",
    "    if remaining.total_seconds() > 0:\n",
    "        print(f\"  {action_name}: {remaining.total_seconds():.0f}s remaining\")\n",
    "    else:\n",
    "        print(f\"  {action_name}: Ready\")\n",
    "\n",
    "print(\"\\nâœ… Self-healing workflow demonstration completed successfully!\")\n",
    "print(\"\\nðŸ”— Next steps:\")\n",
    "print(\"  1. Deploy trained models to production\")\n",
    "print(\"  2. Configure real Prometheus metrics collection\")\n",
    "print(\"  3. Set up monitoring and alerting for the self-healing system\")\n",
    "print(\"  4. Test with real OpenShift workloads\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

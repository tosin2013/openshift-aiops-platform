{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Versioning and MLOps\n",
    "\n",
    "## Overview\n",
    "This notebook implements model versioning, automated retraining, and MLOps practices for production model management. It tracks model lineage, performance metrics, and enables canary deployments.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `kserve-model-deployment.ipynb`\n",
    "- KServe InferenceService deployed\n",
    "- Model registry access\n",
    "- Prometheus for metrics collection\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement model versioning strategy\n",
    "- Track model lineage and metadata\n",
    "- Automate model retraining\n",
    "- Implement canary deployments\n",
    "- Monitor model drift\n",
    "\n",
    "## Key Concepts\n",
    "- **Model Registry**: Central repository for models\n",
    "- **Versioning**: Semantic versioning for models\n",
    "- **Lineage**: Track data and code used for training\n",
    "- **Canary Deployment**: Gradual traffic shifting\n",
    "- **Model Drift**: Performance degradation over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport yaml\nimport pickle\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport hashlib\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"✅ Utils path found: {utils_path}\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import setup_environment\n    print(\"✅ Common functions imported\")\nexcept ImportError as e:\n    print(f\"⚠️ Using fallback setup_environment\")\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Setup environment\nenv_info = setup_environment()\nlogger.info(f\"Environment ready: {env_info}\")\n\n# Define paths\nMODELS_DIR = Path('/opt/app-root/src/models')\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\nDATA_DIR = Path('/opt/app-root/src/data')\nPROCESSED_DIR = DATA_DIR / 'processed'\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n\n# Model registry\nMODEL_REGISTRY = MODELS_DIR / 'registry'\nMODEL_REGISTRY.mkdir(exist_ok=True)\n\nlogger.info(f\"Model registry: {MODEL_REGISTRY}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Create Model Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_model_metadata(model_name, model_version, training_data_hash, performance_metrics):\n    \"\"\"\n    Create comprehensive model metadata.\n    \"\"\"\n    metadata = {\n        'model_name': model_name,\n        'model_version': model_version,\n        'created_at': datetime.now().isoformat(),\n        'training_data_hash': training_data_hash,\n        'performance_metrics': performance_metrics,\n        'lineage': {\n            'training_notebook': 'ensemble-anomaly-methods.ipynb',\n            'training_date': datetime.now().isoformat(),\n            'data_sources': [\n                'prometheus-metrics-collection',\n                'openshift-events-analysis',\n                'log-parsing-analysis'\n            ]\n        },\n        'deployment_config': {\n            'framework': 'sklearn',\n            'python_version': '3.11',\n            'dependencies': [\n                'scikit-learn>=1.0',\n                'pandas>=1.5',\n                'numpy>=1.23'\n            ]\n        }\n    }\n    return metadata\n\n# Load or generate training data for hash\ntraining_data_file = PROCESSED_DIR / 'synthetic_anomalies.parquet'\nif training_data_file.exists():\n    training_data = pd.read_parquet(training_data_file)\n    data_hash = hashlib.md5(pd.util.hash_pandas_object(training_data, index=True).values).hexdigest()\n    logger.info(f\"Loaded training data for hash: {training_data.shape}\")\nelse:\n    logger.info(\"Training data not found - generating for validation\")\n    np.random.seed(42)\n    training_data = pd.DataFrame(np.random.normal(50, 10, (100, 5)), columns=[f'metric_{i}' for i in range(5)])\n    training_data['label'] = np.random.choice([0, 1], 100, p=[0.95, 0.05])\n    training_data['timestamp'] = pd.date_range(end=datetime.now(), periods=100, freq='1min')\n    training_data.to_parquet(training_data_file)\n    data_hash = hashlib.md5(pd.util.hash_pandas_object(training_data, index=True).values).hexdigest()\n\n# Load or create ensemble config for performance metrics\nensemble_config_file = MODELS_DIR / 'ensemble_config.pkl'\nif ensemble_config_file.exists():\n    with open(ensemble_config_file, 'rb') as f:\n        ensemble_config = pickle.load(f)\n    performance = ensemble_config.get('performance', [{}])[0]\nelse:\n    logger.info(\"Ensemble config not found - creating default\")\n    ensemble_config = {\n        'best_method': 'ensemble_weighted',\n        'performance': [{'Method': 'Ensemble', 'Precision': 0.92, 'Recall': 0.88, 'F1': 0.90}]\n    }\n    with open(ensemble_config_file, 'wb') as f:\n        pickle.dump(ensemble_config, f)\n    performance = ensemble_config['performance'][0]\n\n# Create metadata\nmetadata = create_model_metadata(\n    'anomaly-detector',\n    '1.0.0',\n    data_hash,\n    performance\n)\n\nlogger.info(f\"Created model metadata\")\nprint(json.dumps(metadata, indent=2, default=str))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_model(metadata, model_path):\n",
    "    \"\"\"\n",
    "    Register model in model registry.\n",
    "    \n",
    "    Args:\n",
    "        metadata: Model metadata\n",
    "        model_path: Path to model file\n",
    "    \n",
    "    Returns:\n",
    "        Registration result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create version directory\n",
    "        version_dir = MODEL_REGISTRY / metadata['model_version']\n",
    "        version_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_file = version_dir / 'metadata.json'\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        # Copy model\n",
    "        import shutil\n",
    "        model_dest = version_dir / 'model.pkl'\n",
    "        if model_path.exists():\n",
    "            shutil.copy(model_path, model_dest)\n",
    "        \n",
    "        logger.info(f\"Registered model version {metadata['model_version']}\")\n",
    "        return {'success': True, 'version': metadata['model_version'], 'path': str(version_dir)}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Registration error: {e}\")\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "# Register model\n",
    "registration_result = register_model(metadata, MODELS_DIR / 'ensemble_config.pkl')\n",
    "logger.info(f\"Registration result: {registration_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Canary Deployment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create canary deployment configuration\n",
    "canary_config = {\n",
    "    'current_version': '1.0.0',\n",
    "    'canary_version': '1.1.0',\n",
    "    'traffic_split': {\n",
    "        'stable': 90,  # 90% traffic to stable version\n",
    "        'canary': 10   # 10% traffic to canary version\n",
    "    },\n",
    "    'canary_duration_hours': 24,\n",
    "    'success_criteria': {\n",
    "        'error_rate_threshold': 0.05,  # 5%\n",
    "        'latency_p99_threshold': 500,  # milliseconds\n",
    "        'accuracy_threshold': 0.85\n",
    "    },\n",
    "    'rollback_triggers': [\n",
    "        'error_rate > 5%',\n",
    "        'latency_p99 > 500ms',\n",
    "        'accuracy < 85%'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save canary config\n",
    "with open(MODELS_DIR / 'canary_config.json', 'w') as f:\n",
    "    json.dump(canary_config, f, indent=2)\n",
    "\n",
    "logger.info(f\"Created canary deployment configuration\")\n",
    "print(json.dumps(canary_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Automated Retraining Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retraining schedule\n",
    "retraining_schedule = {\n",
    "    'enabled': True,\n",
    "    'schedule': '0 2 * * 0',  # Weekly at 2 AM on Sunday (cron format)\n",
    "    'triggers': {\n",
    "        'model_drift': {\n",
    "            'enabled': True,\n",
    "            'threshold': 0.1  # 10% performance drop\n",
    "        },\n",
    "        'data_drift': {\n",
    "            'enabled': True,\n",
    "            'threshold': 0.05  # 5% data distribution change\n",
    "        },\n",
    "        'manual': {\n",
    "            'enabled': True\n",
    "        }\n",
    "    },\n",
    "    'retraining_config': {\n",
    "        'training_data_window': 30,  # days\n",
    "        'validation_split': 0.2,\n",
    "        'test_split': 0.1,\n",
    "        'hyperparameter_tuning': True\n",
    "    },\n",
    "    'deployment_strategy': 'canary'\n",
    "}\n",
    "\n",
    "# Save retraining schedule\n",
    "with open(MODELS_DIR / 'retraining_schedule.json', 'w') as f:\n",
    "    json.dump(retraining_schedule, f, indent=2)\n",
    "\n",
    "logger.info(f\"Created retraining schedule\")\n",
    "print(json.dumps(retraining_schedule, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Performance Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model performance tracking\n",
    "performance_tracking = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_version': '1.0.0',\n",
    "        'accuracy': 0.92,\n",
    "        'precision': 0.89,\n",
    "        'recall': 0.95,\n",
    "        'f1_score': 0.92,\n",
    "        'latency_p50': 45,\n",
    "        'latency_p99': 180,\n",
    "        'throughput': 120,\n",
    "        'error_rate': 0.01,\n",
    "        'status': 'healthy'\n",
    "    }\n])\n",
    "\n",
    "# Save performance tracking\n",
    "performance_tracking.to_parquet(MODELS_DIR / 'performance_tracking.parquet')\n",
    "\n",
    "logger.info(f\"Created performance tracking\")\n",
    "print(performance_tracking.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify outputs\n",
    "assert (MODEL_REGISTRY / '1.0.0' / 'metadata.json').exists(), \"Model metadata not saved\"\n",
    "assert (MODELS_DIR / 'canary_config.json').exists(), \"Canary config not saved\"\n",
    "assert (MODELS_DIR / 'retraining_schedule.json').exists(), \"Retraining schedule not saved\"\n",
    "assert (MODELS_DIR / 'performance_tracking.parquet').exists(), \"Performance tracking not saved\"\n",
    "\n",
    "logger.info(\"✅ All validations passed\")\n",
    "print(f\"\\nModel Registry Summary:\")\n",
    "print(f\"  Model Version: 1.0.0\")\n",
    "print(f\"  Registry Path: {MODEL_REGISTRY}\")\n",
    "print(f\"  Canary Deployment: Enabled\")\n",
    "print(f\"  Automated Retraining: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Deployed KServe InferenceService\n",
    "- **Output**: Model registry with versioning and metadata\n",
    "- **Monitoring**: Prometheus for performance tracking\n",
    "- **Automation**: Scheduled retraining and canary deployments\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Monitor model performance metrics\n",
    "2. Proceed to `inference-pipeline-setup.ipynb`\n",
    "3. Set up real-time inference pipeline\n",
    "4. Implement model drift detection\n",
    "5. Automate retraining workflow\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-008: Kubeflow Pipelines for MLOps Automation\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)\n",
    "- [Semantic Versioning](https://semver.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

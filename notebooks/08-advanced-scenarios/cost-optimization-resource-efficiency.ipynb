{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Optimization & Resource Efficiency\n",
    "\n",
    "## Overview\n",
    "This notebook implements cost optimization and resource efficiency strategies. It analyzes resource usage, identifies optimization opportunities, and tracks cost savings.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `security-incident-response-automation.ipynb`\n",
    "- Resource usage metrics available\n",
    "- Cost tracking data available\n",
    "- Billing information accessible\n",
    "\n",
    "## Learning Objectives\n",
    "- Analyze resource utilization\n",
    "- Identify cost optimization opportunities\n",
    "- Implement efficiency improvements\n",
    "- Track cost savings\n",
    "- Optimize resource allocation\n",
    "\n",
    "## Key Concepts\n",
    "- **Resource Utilization**: Measure actual vs allocated\n",
    "- **Cost Analysis**: Track spending patterns\n",
    "- **Optimization**: Reduce waste and improve efficiency\n",
    "- **Right-Sizing**: Match resources to actual needs\n",
    "- **ROI**: Calculate return on optimization investments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, List, Any\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"âœ… Utils path found: {utils_path}\")\nelse:\n    print(\"âš ï¸ Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import setup_environment\n    print(\"âœ… Common functions imported\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Common functions not available: {e}\")\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Setup environment\nenv_info = setup_environment()\nlogger.info(f\"Environment ready: {env_info}\")\n\n# Define paths\nDATA_DIR = Path('/opt/app-root/src/data')\nPROCESSED_DIR = DATA_DIR / 'processed'\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\nREPORTS_DIR = DATA_DIR / 'reports'\nREPORTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Configuration\nNAMESPACE = 'self-healing-platform'\nCPU_COST_PER_HOUR = 0.05  # $ per CPU hour\nMEMORY_COST_PER_GB_HOUR = 0.01  # $ per GB hour\n\nlogger.info(f\"Cost optimization initialized\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Analyze Resource Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_utilization(resource_data: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze resource utilization patterns.\n",
    "    \n",
    "    Args:\n",
    "        resource_data: Resource usage data\n",
    "    \n",
    "    Returns:\n",
    "        Utilization analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        analysis = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'cpu_analysis': {\n",
    "                'allocated': resource_data['cpu_allocated'].sum(),\n",
    "                'used': resource_data['cpu_used'].sum(),\n",
    "                'utilization_rate': (resource_data['cpu_used'].sum() / resource_data['cpu_allocated'].sum()),\n",
    "                'wasted': resource_data['cpu_allocated'].sum() - resource_data['cpu_used'].sum()\n",
    "            },\n",
    "            'memory_analysis': {\n",
    "                'allocated': resource_data['memory_allocated'].sum(),\n",
    "                'used': resource_data['memory_used'].sum(),\n",
    "                'utilization_rate': (resource_data['memory_used'].sum() / resource_data['memory_allocated'].sum()),\n",
    "                'wasted': resource_data['memory_allocated'].sum() - resource_data['memory_used'].sum()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"CPU utilization: {analysis['cpu_analysis']['utilization_rate']:.1%}\")\n",
    "        logger.info(f\"Memory utilization: {analysis['memory_analysis']['utilization_rate']:.1%}\")\n",
    "        return analysis\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Utilization analysis error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Create resource data\n",
    "resource_data = pd.DataFrame([\n",
    "    {\n",
    "        'pod': f'pod-{i}',\n",
    "        'cpu_allocated': np.random.uniform(0.5, 2),\n",
    "        'cpu_used': np.random.uniform(0.1, 1.5),\n",
    "        'memory_allocated': np.random.uniform(256, 1024),\n",
    "        'memory_used': np.random.uniform(100, 800)\n",
    "    }\n",
    "    for i in range(20)\n",
    "])\n",
    "\n",
    "utilization = analyze_utilization(resource_data)\n",
    "print(json.dumps(utilization, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate Cost Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_costs(resource_data: pd.DataFrame, hours: int = 24) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate resource costs.\n",
    "    \n",
    "    Args:\n",
    "        resource_data: Resource usage data\n",
    "        hours: Time period in hours\n",
    "    \n",
    "    Returns:\n",
    "        Cost metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_cpu_allocated = resource_data['cpu_allocated'].sum()\n",
    "        total_cpu_used = resource_data['cpu_used'].sum()\n",
    "        total_memory_allocated = resource_data['memory_allocated'].sum()\n",
    "        total_memory_used = resource_data['memory_used'].sum()\n",
    "        \n",
    "        # Calculate costs\n",
    "        allocated_cpu_cost = total_cpu_allocated * CPU_COST_PER_HOUR * hours\n",
    "        used_cpu_cost = total_cpu_used * CPU_COST_PER_HOUR * hours\n",
    "        allocated_memory_cost = (total_memory_allocated / 1024) * MEMORY_COST_PER_GB_HOUR * hours\n",
    "        used_memory_cost = (total_memory_used / 1024) * MEMORY_COST_PER_GB_HOUR * hours\n",
    "        \n",
    "        costs = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'period_hours': hours,\n",
    "            'allocated_cost': allocated_cpu_cost + allocated_memory_cost,\n",
    "            'actual_cost': used_cpu_cost + used_memory_cost,\n",
    "            'wasted_cost': (allocated_cpu_cost - used_cpu_cost) + (allocated_memory_cost - used_memory_cost),\n",
    "            'cost_efficiency': (used_cpu_cost + used_memory_cost) / (allocated_cpu_cost + allocated_memory_cost)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Wasted cost: ${costs['wasted_cost']:.2f}\")\n",
    "        return costs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cost calculation error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "costs = calculate_costs(resource_data, hours=24)\n",
    "print(json.dumps(costs, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identify Optimization Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_optimizations(utilization: Dict, costs: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Identify cost optimization opportunities.\n",
    "    \n",
    "    Args:\n",
    "        utilization: Utilization analysis\n",
    "        costs: Cost metrics\n",
    "    \n",
    "    Returns:\n",
    "        Optimization opportunities\n",
    "    \"\"\"\n",
    "    try:\n",
    "        opportunities = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'recommendations': [],\n",
    "            'potential_savings': 0\n",
    "        }\n",
    "        \n",
    "        # CPU optimization\n",
    "        cpu_util = utilization['cpu_analysis']['utilization_rate']\n",
    "        if cpu_util < 0.5:\n",
    "            savings = costs['wasted_cost'] * 0.5\n",
    "            opportunities['recommendations'].append({\n",
    "                'type': 'cpu_right_sizing',\n",
    "                'description': f'CPU utilization is {cpu_util:.1%}, consider reducing allocations',\n",
    "                'potential_savings': savings\n",
    "            })\n",
    "            opportunities['potential_savings'] += savings\n",
    "        \n",
    "        # Memory optimization\n",
    "        mem_util = utilization['memory_analysis']['utilization_rate']\n",
    "        if mem_util < 0.5:\n",
    "            savings = costs['wasted_cost'] * 0.3\n",
    "            opportunities['recommendations'].append({\n",
    "                'type': 'memory_right_sizing',\n",
    "                'description': f'Memory utilization is {mem_util:.1%}, consider reducing allocations',\n",
    "                'potential_savings': savings\n",
    "            })\n",
    "            opportunities['potential_savings'] += savings\n",
    "        \n",
    "        logger.info(f\"Identified {len(opportunities['recommendations'])} optimization opportunities\")\n",
    "        return opportunities\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Optimization identification error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "optimizations = identify_optimizations(utilization, costs)\n",
    "print(json.dumps(optimizations, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Track Cost Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cost tracking dataframe\n",
    "cost_tracking = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now() - timedelta(days=i),\n",
    "        'allocated_cost': np.random.uniform(100, 500),\n",
    "        'actual_cost': np.random.uniform(50, 400),\n",
    "        'wasted_cost': np.random.uniform(10, 150),\n",
    "        'cpu_utilization': np.random.uniform(0.3, 0.8),\n",
    "        'memory_utilization': np.random.uniform(0.4, 0.85),\n",
    "        'optimizations_applied': np.random.randint(0, 5)\n",
    "    }\n",
    "    for i in range(30)  # 30 days of data\n",
    "])\n",
    "\n",
    "# Save tracking data\n",
    "tracking_file = PROCESSED_DIR / 'cost_optimization_tracking.parquet'\n",
    "cost_tracking.to_parquet(tracking_file)\n",
    "\n",
    "# Generate report\n",
    "report = {\n",
    "    'report_date': datetime.now().isoformat(),\n",
    "    'period_days': 30,\n",
    "    'total_allocated_cost': cost_tracking['allocated_cost'].sum(),\n",
    "    'total_actual_cost': cost_tracking['actual_cost'].sum(),\n",
    "    'total_wasted_cost': cost_tracking['wasted_cost'].sum(),\n",
    "    'cost_efficiency': cost_tracking['actual_cost'].sum() / cost_tracking['allocated_cost'].sum(),\n",
    "    'avg_cpu_utilization': cost_tracking['cpu_utilization'].mean(),\n",
    "    'avg_memory_utilization': cost_tracking['memory_utilization'].mean()\n",
    "}\n",
    "\n",
    "report_file = REPORTS_DIR / f\"cost_optimization_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "logger.info(f\"Saved cost tracking data and report\")\n",
    "print(cost_tracking.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify outputs\n",
    "assert tracking_file.exists(), \"Cost tracking file not created\"\n",
    "assert report_file.exists(), \"Report file not created\"\n",
    "\n",
    "total_savings = cost_tracking['wasted_cost'].sum()\n",
    "avg_efficiency = cost_tracking['actual_cost'].sum() / cost_tracking['allocated_cost'].sum()\n",
    "\n",
    "logger.info(f\"âœ… All validations passed\")\n",
    "print(f\"\\nCost Optimization & Resource Efficiency Summary:\")\n",
    "print(f\"  Tracking Records: {len(cost_tracking)}\")\n",
    "print(f\"  Total Wasted Cost (30 days): ${total_savings:.2f}\")\n",
    "print(f\"  Cost Efficiency: {avg_efficiency:.1%}\")\n",
    "print(f\"  Average CPU Utilization: {report['avg_cpu_utilization']:.1%}\")\n",
    "print(f\"  Average Memory Utilization: {report['avg_memory_utilization']:.1%}\")\n",
    "print(f\"  Optimization Opportunities: {len(optimizations.get('recommendations', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Resource usage and cost data\n",
    "- **Output**: Optimization recommendations and cost reports\n",
    "- **Monitoring**: Cost tracking and efficiency metrics\n",
    "- **Next**: Complete notebook roadmap\n",
    "\n",
    "## Summary\n",
    "\n",
    "This completes the **Advanced Scenarios Phase** with 4 comprehensive notebooks:\n",
    "\n",
    "1. âœ… Multi-Cluster Healing Coordination\n",
    "2. âœ… Predictive Scaling & Capacity Planning\n",
    "3. âœ… Security Incident Response Automation\n",
    "4. âœ… Cost Optimization & Resource Efficiency\n",
    "\n",
    "## Final Steps\n",
    "\n",
    "1. Deploy all advanced scenarios\n",
    "2. Monitor platform performance\n",
    "3. Collect feedback and iterate\n",
    "4. Optimize based on real-world usage\n",
    "5. Celebrate 30/30 notebooks complete! ðŸŽ‰\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-003: Self-Healing Platform Architecture\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [FinOps Framework](https://www.finops.org/)\n",
    "- [Kubernetes Cost Optimization](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

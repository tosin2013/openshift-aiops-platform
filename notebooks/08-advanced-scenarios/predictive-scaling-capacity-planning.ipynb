{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# Predictive Scaling & Capacity Planning\n",
        "\n",
        "## Overview\n",
        "This notebook implements predictive scaling and capacity planning. It forecasts resource demand, triggers proactive scaling, and optimizes resource allocation across the platform.\n",
        "\n",
        "**Model Serving**: This notebook trains and saves a sklearn model to the shared PVC (`/mnt/models/`) for the `predictive-analytics` InferenceService. The model is saved during both validation and manual execution.\n",
        "\n",
        "## Prerequisites\n",
        "- Completed: `multi-cluster-healing-coordination.ipynb`\n",
        "- Historical resource usage data\n",
        "- Kubernetes metrics available\n",
        "- HPA (Horizontal Pod Autoscaler) configured\n",
        "\n",
        "## Learning Objectives\n",
        "- Forecast resource demand\n",
        "- Implement predictive scaling\n",
        "- Plan capacity requirements\n",
        "- Optimize resource allocation\n",
        "- Prevent resource exhaustion\n",
        "\n",
        "## Key Concepts\n",
        "- **Demand Forecasting**: Predict future resource needs\n",
        "- **Predictive Scaling**: Scale before demand spike\n",
        "- **Capacity Planning**: Allocate resources efficiently\n",
        "- **Resource Optimization**: Minimize waste\n",
        "- **Cost Efficiency**: Balance performance and cost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "## Setup Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# \u2728 Import PredictiveAnalytics module (updated for KServe compatibility)\n",
        "# Add src/models to path\n",
        "sys.path.insert(0, '/workspace/repo/src/models')\n",
        "sys.path.insert(0, '/opt/app-root/src/openshift-aiops-platform/src/models')\n",
        "sys.path.insert(0, str(Path.cwd().parent.parent / 'src' / 'models'))\n",
        "\n",
        "try:\n",
        "    from predictive_analytics import PredictiveAnalytics, generate_sample_timeseries_data\n",
        "    print(\"\u2705 PredictiveAnalytics module imported successfully\")\n",
        "    USING_MODULE = True\n",
        "except ImportError as e:\n",
        "    print(f\"\u26a0\ufe0f Could not import PredictiveAnalytics module: {e}\")\n",
        "    print(\"   Will use simplified inline implementation\")\n",
        "    USING_MODULE = False\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\ud83d\udce6 All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths for model storage and data\n",
        "import os\n",
        "\n",
        "# Use relative paths from current working directory (safe in both OpenShift and local)\n",
        "# OpenShift notebooks run from /opt/app-root/src (writable)\n",
        "# Local runs from project root\n",
        "BASE_DIR = Path.cwd()\n",
        "\n",
        "# Data directory (writable in both environments)\n",
        "DATA_DIR = BASE_DIR / 'data'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Model storage paths\n",
        "# Primary: PVC mount for KServe InferenceService (predictive-analytics)\n",
        "# Fallback: Local models directory\n",
        "PVC_MODELS_DIR = Path('/mnt/models')\n",
        "LOCAL_MODELS_DIR = BASE_DIR / 'models' / 'local'\n",
        "LOCAL_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Model name must match InferenceService name\n",
        "MODEL_NAME = 'predictive-analytics'\n",
        "\n",
        "# Configuration\n",
        "NAMESPACE = 'self-healing-platform'\n",
        "FORECAST_HORIZON = 24  # 24 hours ahead\n",
        "SCALING_THRESHOLD = 0.80  # Scale at 80% capacity\n",
        "\n",
        "print(f\"\ud83d\udcc1 Storage Configuration:\")\n",
        "print(f\"   Base directory: {BASE_DIR}\")\n",
        "print(f\"   Data directory: {DATA_DIR}\")\n",
        "print(f\"   PVC models directory: {PVC_MODELS_DIR} (exists: {PVC_MODELS_DIR.exists()})\")\n",
        "print(f\"   Local models directory: {LOCAL_MODELS_DIR}\")\n",
        "print(f\"   Model will save to: {PVC_MODELS_DIR if PVC_MODELS_DIR.exists() else LOCAL_MODELS_DIR}/{MODEL_NAME}/model.pkl\")\n",
        "\n",
        "logger.info(f\"Predictive scaling initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {},
      "source": [
        "## Implementation Section\n",
        "\n",
        "### 1. Generate Training Data and Train Predictive Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b371q6p2rhw",
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE: Removed custom PredictiveScalingEnsemble class\n",
        "# Using sklearn's MultiOutputRegressor instead for KServe compatibility\n",
        "# KServe can load standard sklearn estimators but not custom classes\n",
        "\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "print(\"\u2705 Using sklearn MultiOutputRegressor for KServe compatibility\")\n",
        "print(\"   This allows a single model to predict both CPU and Memory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate realistic historical resource usage data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Create 7 days of hourly data with realistic patterns\n",
        "hours = 168  # 7 days\n",
        "timestamps = [datetime.now() - timedelta(hours=i) for i in range(hours, 0, -1)]\n",
        "\n",
        "# Simulate daily patterns (higher during business hours)\n",
        "hour_of_day = np.array([t.hour for t in timestamps])\n",
        "day_of_week = np.array([t.weekday() for t in timestamps])\n",
        "\n",
        "# Base load + daily pattern + weekly pattern + noise\n",
        "base_cpu = 0.4\n",
        "daily_pattern = 0.2 * np.sin(2 * np.pi * (hour_of_day - 6) / 24)  # Peak at 2pm\n",
        "weekly_pattern = 0.1 * (1 - day_of_week / 7)  # Higher on weekdays\n",
        "noise = np.random.normal(0, 0.05, hours)\n",
        "\n",
        "cpu_usage = np.clip(base_cpu + daily_pattern + weekly_pattern + noise, 0.1, 0.95)\n",
        "memory_usage = np.clip(cpu_usage * 1.1 + np.random.normal(0, 0.03, hours), 0.2, 0.95)\n",
        "\n",
        "historical_data = pd.DataFrame({\n",
        "    'timestamp': timestamps,\n",
        "    'hour_of_day': hour_of_day,\n",
        "    'day_of_week': day_of_week,\n",
        "    'cpu_usage': cpu_usage,\n",
        "    'memory_usage': memory_usage\n",
        "})\n",
        "\n",
        "print(f\"Generated {len(historical_data)} hours of historical data\")\n",
        "print(f\"CPU usage range: {cpu_usage.min():.2%} - {cpu_usage.max():.2%}\")\n",
        "print(f\"Memory usage range: {memory_usage.min():.2%} - {memory_usage.max():.2%}\")\n",
        "\n",
        "# Train a predictive model for resource forecasting\n",
        "# Features: hour_of_day, day_of_week, rolling averages\n",
        "historical_data['cpu_rolling_mean'] = historical_data['cpu_usage'].rolling(window=24, min_periods=1).mean()\n",
        "historical_data['memory_rolling_mean'] = historical_data['memory_usage'].rolling(window=24, min_periods=1).mean()\n",
        "\n",
        "# Prepare features and targets (both CPU and Memory)\n",
        "feature_cols = ['hour_of_day', 'day_of_week', 'cpu_rolling_mean', 'memory_rolling_mean']\n",
        "X = historical_data[feature_cols].values\n",
        "# Stack both targets for MultiOutputRegressor\n",
        "y = np.column_stack([historical_data['cpu_usage'].values, historical_data['memory_usage'].values])\n",
        "\n",
        "# \u2728 Create SINGLE sklearn Pipeline with MultiOutputRegressor (KServe compatible!)\n",
        "# This predicts both CPU and Memory in one model\n",
        "predictive_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', MultiOutputRegressor(\n",
        "        RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"\u2705 Training multi-output prediction model (CPU + Memory)...\")\n",
        "predictive_model.fit(X, y)\n",
        "\n",
        "print(f\"\u2705 Model trained with {len(historical_data)} samples\")\n",
        "print(f\"   Input features: {len(feature_cols)}\")\n",
        "print(f\"   Output predictions: 2 (CPU, Memory)\")\n",
        "print(f\"   Model type: sklearn Pipeline (KServe compatible)\")\n",
        "\n",
        "logger.info(f\"\u2705 Trained predictive scaling model with {len(historical_data)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {},
      "source": [
        "### 2. Save Model to PVC for KServe InferenceService"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {},
      "outputs": [],
      "source": "# \u2728 Train and Save Model using PredictiveAnalytics module (KServe-compatible)\n\nif USING_MODULE:\n    # Use the full PredictiveAnalytics class with KServe-compatible saving\n    print(\"\ud83d\udd2c Training PredictiveAnalytics model...\")\n    print(f\"   Forecast horizon: 12 hours\")\n    print(f\"   Lookback window: 24 hours\")\n    \n    # Initialize model\n    predictor = PredictiveAnalytics(forecast_horizon=12, lookback_window=24)\n    \n    # Train on historical data\n    training_results = predictor.train(historical_data)\n    \n    print(f\"\\n\u2705 Training completed:\")\n    print(f\"   Models trained: {training_results['models_trained']}\")\n    print(f\"   Features: {training_results['feature_count']}\")\n    \n    # Print metrics\n    for metric_name, results in training_results['metrics'].items():\n        print(f\"\\n   {metric_name}:\")\n        print(f\"     MAE:  {results['mae']:.4f}\")\n        print(f\"     RMSE: {results['rmse']:.4f}\")\n        print(f\"     R\u00b2:   {results['r2']:.4f}\")\n    \n    # Save model with KServe-compatible structure\n    print(f\"\\n\ud83d\udcbe Saving model in KServe-compatible format...\")\n    \n    # Determine model directory (PVC or local)\n    if PVC_MODELS_DIR.exists():\n        model_base_dir = str(PVC_MODELS_DIR)\n        print(f\"   Using PVC: {model_base_dir}\")\n    else:\n        model_base_dir = str(LOCAL_MODELS_DIR)\n        print(f\"   Using local: {model_base_dir}\")\n    \n    # Save with automatic KServe compatibility\n    predictor.save_models(model_base_dir, kserve_compatible=True)\n    \n    # Verify the saved model\n    expected_path = Path(model_base_dir) / 'predictive-analytics' / 'model.pkl'\n    if expected_path.exists():\n        size_kb = expected_path.stat().st_size / 1024\n        print(f\"\\n\u2705 Model saved successfully!\")\n        print(f\"   Location: {expected_path}\")\n        print(f\"   Size: {size_kb:.2f} KB\")\n        print(f\"\\n\ud83d\udce1 KServe InferenceService will:\")\n        print(f\"   1. Mount: pvc://model-storage-pvc/predictive-analytics\")\n        print(f\"   2. Load: /mnt/models/predictive-analytics/model.pkl\")\n        print(f\"   3. Register as: 'predictive-analytics'\")\n        print(f\"   4. Endpoint: /v1/models/predictive-analytics:predict\")\n        \n        save_results = {\n            'saved_to': [str(expected_path)],\n            'errors': [],\n            'kserve_compatible': True\n        }\n    else:\n        print(f\"\u274c Model not found at expected location: {expected_path}\")\n        save_results = {\n            'saved_to': [],\n            'errors': ['Model file not created'],\n            'kserve_compatible': False\n        }\n    \n    # Store for use in predictions\n    predictive_model = predictor\n    \nelse:\n    # Fallback: Use simplified sklearn pipeline\n    print(\"\u26a0\ufe0f Using simplified model (PredictiveAnalytics module not available)\")\n    \n    # Create features\n    X = historical_data[['cpu_usage', 'memory_usage']].values\n    y = historical_data[['cpu_usage', 'memory_usage']].values\n    \n    # Train pipeline\n    predictive_model = Pipeline([\n        ('scaler', StandardScaler()),\n        ('regressor', MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42)))\n    ])\n    predictive_model.fit(X, y)\n    \n    # Save manually\n    model_dir = (PVC_MODELS_DIR if PVC_MODELS_DIR.exists() else LOCAL_MODELS_DIR) / 'predictive-analytics'\n    model_dir.mkdir(parents=True, exist_ok=True)\n    model_path = model_dir / 'model.pkl'\n    joblib.dump(predictive_model, model_path)\n    \n    save_results = {'saved_to': [str(model_path)], 'errors': [], 'kserve_compatible': True}\n    print(f\"\u2705 Model saved to: {model_path}\")\n\nprint(f\"\\n\ud83d\udcca Save Results: {save_results}\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {},
      "source": [
        "### 3. Forecast Resource Demand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def forecast_demand(model, current_data: pd.DataFrame, horizon: int = 24) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Forecast resource demand using trained model.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model (sklearn Pipeline or PredictiveAnalytics)\n",
        "        current_data: Current resource usage data\n",
        "        horizon: Forecast horizon in hours\n",
        "    \n",
        "    Returns:\n",
        "        Demand forecast\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if model is PredictiveAnalytics wrapper\n",
        "        if hasattr(model, 'predict') and hasattr(model, 'is_trained'):\n",
        "            # Use PredictiveAnalytics wrapper\n",
        "            predictions = model.predict(current_data)\n",
        "            \n",
        "            # Extract forecasts\n",
        "            cpu_forecast = np.array(predictions.get('cpu_usage', {}).get('forecast', []))\n",
        "            memory_forecast = np.array(predictions.get('memory_usage', {}).get('forecast', []))\n",
        "            \n",
        "            forecast = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'horizon_hours': len(cpu_forecast),\n",
        "                'cpu_forecast': cpu_forecast.tolist(),\n",
        "                'memory_forecast': memory_forecast.tolist(),\n",
        "                'peak_cpu': float(np.max(cpu_forecast)) if len(cpu_forecast) > 0 else 0.0,\n",
        "                'peak_memory': float(np.max(memory_forecast)) if len(memory_forecast) > 0 else 0.0,\n",
        "                'avg_cpu': float(np.mean(cpu_forecast)) if len(cpu_forecast) > 0 else 0.0,\n",
        "                'avg_memory': float(np.mean(memory_forecast)) if len(memory_forecast) > 0 else 0.0\n",
        "            }\n",
        "        else:\n",
        "            # Use sklearn Pipeline (original logic)\n",
        "            future_timestamps = [datetime.now() + timedelta(hours=i) for i in range(1, horizon + 1)]\n",
        "            \n",
        "            # Create features for prediction\n",
        "            future_features = []\n",
        "            last_cpu_mean = current_data['cpu_usage'].tail(24).mean()\n",
        "            last_memory_mean = current_data['memory_usage'].tail(24).mean()\n",
        "            \n",
        "            for ts in future_timestamps:\n",
        "                future_features.append([\n",
        "                    ts.hour,\n",
        "                    ts.weekday(),\n",
        "                    last_cpu_mean,\n",
        "                    last_memory_mean\n",
        "                ])\n",
        "            \n",
        "            X_future = np.array(future_features)\n",
        "            predictions = model.predict(X_future)\n",
        "            \n",
        "            cpu_forecast = predictions[:, 0]\n",
        "            memory_forecast = predictions[:, 1]\n",
        "            \n",
        "            forecast = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'horizon_hours': horizon,\n",
        "                'cpu_forecast': cpu_forecast.tolist(),\n",
        "                'memory_forecast': memory_forecast.tolist(),\n",
        "                'peak_cpu': float(np.max(cpu_forecast)),\n",
        "                'peak_memory': float(np.max(memory_forecast)),\n",
        "                'avg_cpu': float(np.mean(cpu_forecast)),\n",
        "                'avg_memory': float(np.mean(memory_forecast))\n",
        "            }\n",
        "        \n",
        "        logger.info(f\"Demand forecast: Peak CPU {forecast['peak_cpu']:.1%}, Peak Memory {forecast['peak_memory']:.1%}\")\n",
        "        return forecast\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Demand forecasting error: {e}\")\n",
        "        return {'error': str(e)}\n",
        "\n",
        "# Generate forecast\n",
        "forecast = forecast_demand(predictive_model, historical_data, FORECAST_HORIZON)\n",
        "print(json.dumps({k: v for k, v in forecast.items() if k not in ['cpu_forecast', 'memory_forecast']}, indent=2, default=str))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {},
      "source": [
        "### 4. Trigger Predictive Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10",
      "metadata": {},
      "outputs": [],
      "source": [
        "def trigger_predictive_scaling(forecast: Dict[str, Any], current_replicas: int) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Trigger predictive scaling based on forecast.\n",
        "    \n",
        "    Args:\n",
        "        forecast: Demand forecast\n",
        "        current_replicas: Current number of replicas\n",
        "    \n",
        "    Returns:\n",
        "        Scaling decision\n",
        "    \"\"\"\n",
        "    try:\n",
        "        peak_cpu = forecast.get('peak_cpu', 0)\n",
        "        \n",
        "        # Calculate required replicas\n",
        "        if peak_cpu > SCALING_THRESHOLD:\n",
        "            # Scale up: add 20% more replicas\n",
        "            required_replicas = int(current_replicas * (peak_cpu / SCALING_THRESHOLD))\n",
        "            scaling_action = 'scale_up'\n",
        "        elif peak_cpu < 0.5:\n",
        "            # Scale down: reduce by 20%\n",
        "            required_replicas = max(1, int(current_replicas * 0.8))\n",
        "            scaling_action = 'scale_down'\n",
        "        else:\n",
        "            required_replicas = current_replicas\n",
        "            scaling_action = 'no_change'\n",
        "        \n",
        "        scaling_decision = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'current_replicas': current_replicas,\n",
        "            'required_replicas': required_replicas,\n",
        "            'scaling_action': scaling_action,\n",
        "            'peak_cpu_forecast': peak_cpu,\n",
        "            'scaling_triggered': scaling_action != 'no_change',\n",
        "            'estimated_cost_savings': (current_replicas - required_replicas) * 100 if scaling_action == 'scale_down' else 0\n",
        "        }\n",
        "        \n",
        "        logger.info(f\"Scaling decision: {scaling_action} ({current_replicas} -> {required_replicas})\")\n",
        "        return scaling_decision\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Predictive scaling error: {e}\")\n",
        "        return {'error': str(e)}\n",
        "\n",
        "# Test scaling decision\n",
        "scaling_decision = trigger_predictive_scaling(forecast, current_replicas=5)\n",
        "print(json.dumps(scaling_decision, indent=2, default=str))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {},
      "source": [
        "### 5. Capacity Planning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plan_capacity(forecast: Dict[str, Any], current_capacity: Dict[str, float]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Plan capacity requirements based on forecast.\n",
        "    \n",
        "    Args:\n",
        "        forecast: Demand forecast\n",
        "        current_capacity: Current capacity allocation\n",
        "    \n",
        "    Returns:\n",
        "        Capacity plan\n",
        "    \"\"\"\n",
        "    try:\n",
        "        peak_cpu = forecast.get('peak_cpu', 0)\n",
        "        peak_memory = forecast.get('peak_memory', 0)\n",
        "        \n",
        "        # Calculate required capacity with 20% headroom\n",
        "        required_cpu = peak_cpu * 1.2\n",
        "        required_memory = peak_memory * 1.2\n",
        "        \n",
        "        capacity_plan = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'current_cpu_capacity': current_capacity.get('cpu', 0),\n",
        "            'required_cpu_capacity': required_cpu,\n",
        "            'cpu_headroom': required_cpu - current_capacity.get('cpu', 0),\n",
        "            'current_memory_capacity': current_capacity.get('memory', 0),\n",
        "            'required_memory_capacity': required_memory,\n",
        "            'memory_headroom': required_memory - current_capacity.get('memory', 0),\n",
        "            'capacity_sufficient': (required_cpu <= current_capacity.get('cpu', 0) and \n",
        "                                   required_memory <= current_capacity.get('memory', 0)),\n",
        "            'recommendations': []\n",
        "        }\n",
        "        \n",
        "        # Generate recommendations\n",
        "        if capacity_plan['cpu_headroom'] > 0:\n",
        "            capacity_plan['recommendations'].append(\n",
        "                f\"Add {capacity_plan['cpu_headroom']:.1f} CPU cores\"\n",
        "            )\n",
        "        if capacity_plan['memory_headroom'] > 0:\n",
        "            capacity_plan['recommendations'].append(\n",
        "                f\"Add {capacity_plan['memory_headroom']:.1f}GB memory\"\n",
        "            )\n",
        "        \n",
        "        logger.info(f\"Capacity plan: {len(capacity_plan['recommendations'])} recommendations\")\n",
        "        return capacity_plan\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Capacity planning error: {e}\")\n",
        "        return {'error': str(e)}\n",
        "\n",
        "# Test capacity planning\n",
        "current_capacity = {'cpu': 16, 'memory': 64}\n",
        "capacity_plan = plan_capacity(forecast, current_capacity)\n",
        "print(json.dumps(capacity_plan, indent=2, default=str))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {},
      "source": [
        "### 6. Track Scaling History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create scaling tracking dataframe\n",
        "scaling_tracking = pd.DataFrame([\n",
        "    {\n",
        "        'timestamp': datetime.now() - timedelta(hours=i),\n",
        "        'current_replicas': np.random.randint(3, 10),\n",
        "        'target_replicas': np.random.randint(3, 10),\n",
        "        'scaling_action': np.random.choice(['scale_up', 'scale_down', 'no_change']),\n",
        "        'forecast_accuracy': np.random.uniform(0.75, 0.95),\n",
        "        'cost_savings': np.random.uniform(0, 500),\n",
        "        'performance_impact': np.random.choice(['positive', 'neutral', 'negative'])\n",
        "    }\n",
        "    for i in range(168)  # 7 days of data\n",
        "])\n",
        "\n",
        "# Save tracking data\n",
        "tracking_file = PROCESSED_DIR / 'predictive_scaling_tracking.parquet'\n",
        "scaling_tracking.to_parquet(tracking_file)\n",
        "\n",
        "logger.info(f\"Saved predictive scaling tracking data\")\n",
        "print(f\"Tracking data saved to: {tracking_file}\")\n",
        "print(f\"Records: {len(scaling_tracking)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {},
      "source": [
        "## Validation Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify outputs\n",
        "assert tracking_file.exists(), \"Scaling tracking file not created\"\n",
        "assert 'peak_cpu' in forecast, \"No CPU forecast\"\n",
        "assert 'scaling_action' in scaling_decision, \"No scaling decision\"\n",
        "assert len(save_results['saved_to']) > 0, \"Model not saved to any location\"\n",
        "\n",
        "# Check if model was saved to PVC (critical for KServe)\n",
        "pvc_model_dir = PVC_MODELS_DIR / MODEL_NAME if PVC_MODELS_DIR.exists() else None\n",
        "pvc_model_exists = (pvc_model_dir / 'model.pkl').exists() if pvc_model_dir else False\n",
        "\n",
        "local_model_dir = LOCAL_MODELS_DIR / MODEL_NAME\n",
        "local_model_exists = (local_model_dir / 'model.pkl').exists()\n",
        "\n",
        "kserve_restarted = save_results.get('kserve_restarted', False)\n",
        "\n",
        "avg_forecast_accuracy = scaling_tracking['forecast_accuracy'].mean()\n",
        "total_cost_savings = scaling_tracking['cost_savings'].sum()\n",
        "scale_up_count = (scaling_tracking['scaling_action'] == 'scale_up').sum()\n",
        "\n",
        "logger.info(f\"\u2705 All validations passed\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Predictive Scaling & Capacity Planning Summary\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\n\ud83d\udcca Training Data:\")\n",
        "print(f\"   Historical Records: {len(historical_data)}\")\n",
        "print(f\"   Features: {len(feature_cols)}\")\n",
        "print(f\"\\n\ud83e\udd16 Model Status:\")\n",
        "print(f\"   Model Type: sklearn Pipeline with MultiOutputRegressor\")\n",
        "print(f\"   PVC Model: {'\u2705 Saved' if pvc_model_exists else '\u274c Not saved (PVC not mounted)'}\")\n",
        "print(f\"   Local Model: {'\u2705 Saved' if local_model_exists else '\u274c Not saved'}\")\n",
        "print(f\"   KServe Predictor Restarted: {'\u2705 Yes' if kserve_restarted else '\u26a0\ufe0f No (deployment may not exist yet)'}\")\n",
        "print(f\"   KServe Compatible: \u2705 Yes (sklearn Pipeline)\")\n",
        "print(f\"\\n\ud83d\udcc8 Forecast Results:\")\n",
        "print(f\"   Forecast Horizon: {FORECAST_HORIZON} hours\")\n",
        "print(f\"   Peak CPU: {forecast['peak_cpu']:.1%}\")\n",
        "print(f\"   Peak Memory: {forecast['peak_memory']:.1%}\")\n",
        "print(f\"\\n\u26a1 Scaling Decision:\")\n",
        "print(f\"   Action: {scaling_decision['scaling_action']}\")\n",
        "print(f\"   Replicas: {scaling_decision['current_replicas']} -> {scaling_decision['required_replicas']}\")\n",
        "print(f\"\\n\ud83d\udcc9 Historical Stats:\")\n",
        "print(f\"   Tracking Records: {len(scaling_tracking)}\")\n",
        "print(f\"   Average Forecast Accuracy: {avg_forecast_accuracy:.1%}\")\n",
        "print(f\"   Total Cost Savings: ${total_cost_savings:.0f}\")\n",
        "print(f\"   Scale-Up Events: {scale_up_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {},
      "source": [
        "## Integration Section\n",
        "\n",
        "This notebook integrates with:\n",
        "- **Input**: Historical resource usage and forecasts\n",
        "- **Output**: \n",
        "  - Trained sklearn model saved to PVC (`/mnt/models/model.pkl`)\n",
        "  - Model served by `predictive-analytics` InferenceService\n",
        "  - Scaling decisions and capacity plans\n",
        "- **Monitoring**: Forecast accuracy and cost savings\n",
        "- **Next**: Security incident response automation\n",
        "\n",
        "### Model Serving\n",
        "\n",
        "The trained model is automatically available to the `predictive-analytics` InferenceService:\n",
        "\n",
        "```yaml\n",
        "apiVersion: serving.kserve.io/v1beta1\n",
        "kind: InferenceService\n",
        "metadata:\n",
        "  name: predictive-analytics\n",
        "spec:\n",
        "  predictor:\n",
        "    model:\n",
        "      modelFormat:\n",
        "        name: sklearn\n",
        "      storageUri: \"pvc://model-storage-pvc\"\n",
        "```\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. \u2705 Model deployed to PVC for KServe\n",
        "2. Verify `predictive-analytics` InferenceService is ready\n",
        "3. Proceed to `security-incident-response-automation.ipynb`\n",
        "4. Implement security incident handling\n",
        "5. Automate response procedures\n",
        "\n",
        "## References\n",
        "\n",
        "- ADR-003: Self-Healing Platform Architecture\n",
        "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
        "- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n",
        "- [Capacity Planning](https://en.wikipedia.org/wiki/Capacity_planning)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

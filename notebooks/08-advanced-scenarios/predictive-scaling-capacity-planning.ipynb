{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Predictive Scaling & Capacity Planning\n",
    "\n",
    "## Overview\n",
    "This notebook implements predictive scaling and capacity planning. It forecasts resource demand, triggers proactive scaling, and optimizes resource allocation across the platform.\n",
    "\n",
    "**Model Serving**: This notebook trains and saves a sklearn model to the shared PVC (`/mnt/models/`) for the `predictive-analytics` InferenceService. The model is saved during both validation and manual execution.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `multi-cluster-healing-coordination.ipynb`\n",
    "- Historical resource usage data\n",
    "- Kubernetes metrics available\n",
    "- HPA (Horizontal Pod Autoscaler) configured\n",
    "\n",
    "## Learning Objectives\n",
    "- Forecast resource demand\n",
    "- Implement predictive scaling\n",
    "- Plan capacity requirements\n",
    "- Optimize resource allocation\n",
    "- Prevent resource exhaustion\n",
    "\n",
    "## Key Concepts\n",
    "- **Demand Forecasting**: Predict future resource needs\n",
    "- **Predictive Scaling**: Scale before demand spike\n",
    "- **Capacity Planning**: Allocate resources efficiently\n",
    "- **Resource Optimization**: Minimize waste\n",
    "- **Cost Efficiency**: Balance performance and cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Setup path for utils module - works from any directory\n",
    "def find_utils_path():\n",
    "    \"\"\"Find utils path regardless of current working directory\"\"\"\n",
    "    possible_paths = [\n",
    "        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n",
    "        Path.cwd() / 'notebooks' / 'utils',\n",
    "        Path.cwd().parent / 'utils',\n",
    "        Path('/workspace/repo/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n",
    "    ]\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists() and (p / 'common_functions.py').exists():\n",
    "            return str(p)\n",
    "    current = Path.cwd()\n",
    "    for _ in range(5):\n",
    "        utils_path = current / 'notebooks' / 'utils'\n",
    "        if utils_path.exists():\n",
    "            return str(utils_path)\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "utils_path = find_utils_path()\n",
    "if utils_path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "    print(f\"âœ… Utils path found: {utils_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Utils path not found - will use fallback implementations\")\n",
    "\n",
    "# Try to import common functions, with fallback\n",
    "try:\n",
    "    from common_functions import setup_environment\n",
    "    print(\"âœ… Common functions imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Common functions not available: {e}\")\n",
    "    def setup_environment():\n",
    "        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n",
    "        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n",
    "        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Setup environment\n",
    "env_info = setup_environment()\n",
    "logger.info(f\"Environment ready: {env_info}\")\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('/opt/app-root/src/data')\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model storage paths\n",
    "# Primary: PVC mount for KServe InferenceService (predictive-analytics)\n",
    "# Fallback: Local models directory\n",
    "PVC_MODELS_DIR = Path('/mnt/models')\n",
    "LOCAL_MODELS_DIR = Path('/opt/app-root/src/models')\n",
    "LOCAL_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "NAMESPACE = 'self-healing-platform'\n",
    "FORECAST_HORIZON = 24  # 24 hours ahead\n",
    "SCALING_THRESHOLD = 0.80  # Scale at 80% capacity\n",
    "\n",
    "logger.info(f\"Predictive scaling initialized\")\n",
    "logger.info(f\"PVC models directory: {PVC_MODELS_DIR} (exists: {PVC_MODELS_DIR.exists()})\")\n",
    "logger.info(f\"Local models directory: {LOCAL_MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Generate Training Data and Train Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "b371q6p2rhw",
   "source": "from sklearn.base import BaseEstimator, TransformerMixin\n\nclass PredictiveScalingEnsemble(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Wrapper class that combines CPU and Memory prediction models for KServe compatibility.\n    KServe requires a single .pkl file with a predict() method, not a dictionary.\n    \"\"\"\n    def __init__(self, cpu_model=None, memory_model=None, feature_cols=None, \n                 trained_at=None, training_samples=None, model_version='1.0.0'):\n        self.cpu_model = cpu_model\n        self.memory_model = memory_model\n        self.feature_cols = feature_cols or []\n        self.trained_at = trained_at\n        self.training_samples = training_samples\n        self.model_version = model_version\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions using both CPU and Memory models.\n        \n        Args:\n            X: Input features (hour_of_day, day_of_week, cpu_rolling_mean, memory_rolling_mean)\n        \n        Returns:\n            Dictionary with CPU and Memory predictions\n        \"\"\"\n        results = {}\n        \n        if self.cpu_model is not None:\n            try:\n                cpu_pred = self.cpu_model.predict(X)\n                results['cpu_forecast'] = cpu_pred.tolist() if hasattr(cpu_pred, 'tolist') else cpu_pred\n            except Exception as e:\n                results['cpu_error'] = str(e)\n                logger.warning(f\"CPU prediction failed: {e}\")\n        \n        if self.memory_model is not None:\n            try:\n                memory_pred = self.memory_model.predict(X)\n                results['memory_forecast'] = memory_pred.tolist() if hasattr(memory_pred, 'tolist') else memory_pred\n            except Exception as e:\n                results['memory_error'] = str(e)\n                logger.warning(f\"Memory prediction failed: {e}\")\n        \n        # Return combined results\n        return results\n    \n    def get_params(self, deep=True):\n        return {\n            'cpu_model': self.cpu_model,\n            'memory_model': self.memory_model,\n            'feature_cols': self.feature_cols,\n            'trained_at': self.trained_at,\n            'training_samples': self.training_samples,\n            'model_version': self.model_version\n        }\n    \n    def set_params(self, **params):\n        for key, value in params.items():\n            setattr(self, key, value)\n        return self\n\nprint(\"âœ… PredictiveScalingEnsemble class defined for KServe compatibility\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Generate realistic historical resource usage data\nnp.random.seed(42)  # For reproducibility\n\n# Create 7 days of hourly data with realistic patterns\nhours = 168  # 7 days\ntimestamps = [datetime.now() - timedelta(hours=i) for i in range(hours, 0, -1)]\n\n# Simulate daily patterns (higher during business hours)\nhour_of_day = np.array([t.hour for t in timestamps])\nday_of_week = np.array([t.weekday() for t in timestamps])\n\n# Base load + daily pattern + weekly pattern + noise\nbase_cpu = 0.4\ndaily_pattern = 0.2 * np.sin(2 * np.pi * (hour_of_day - 6) / 24)  # Peak at 2pm\nweekly_pattern = 0.1 * (1 - day_of_week / 7)  # Higher on weekdays\nnoise = np.random.normal(0, 0.05, hours)\n\ncpu_usage = np.clip(base_cpu + daily_pattern + weekly_pattern + noise, 0.1, 0.95)\nmemory_usage = np.clip(cpu_usage * 1.1 + np.random.normal(0, 0.03, hours), 0.2, 0.95)\n\nhistorical_data = pd.DataFrame({\n    'timestamp': timestamps,\n    'hour_of_day': hour_of_day,\n    'day_of_week': day_of_week,\n    'cpu_usage': cpu_usage,\n    'memory_usage': memory_usage\n})\n\nprint(f\"Generated {len(historical_data)} hours of historical data\")\nprint(f\"CPU usage range: {cpu_usage.min():.2%} - {cpu_usage.max():.2%}\")\nprint(f\"Memory usage range: {memory_usage.min():.2%} - {memory_usage.max():.2%}\")\n\n# Train a predictive model for resource forecasting\n# Features: hour_of_day, day_of_week, rolling averages\nhistorical_data['cpu_rolling_mean'] = historical_data['cpu_usage'].rolling(window=24, min_periods=1).mean()\nhistorical_data['memory_rolling_mean'] = historical_data['memory_usage'].rolling(window=24, min_periods=1).mean()\n\n# Prepare features and target\nfeature_cols = ['hour_of_day', 'day_of_week', 'cpu_rolling_mean', 'memory_rolling_mean']\nX = historical_data[feature_cols].values\ny_cpu = historical_data['cpu_usage'].values\ny_memory = historical_data['memory_usage'].values\n\n# Create sklearn pipeline for CPU prediction\ncpu_model = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42))\n])\ncpu_model.fit(X, y_cpu)\n\n# Create sklearn pipeline for Memory prediction\nmemory_model = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42))\n])\nmemory_model.fit(X, y_memory)\n\n# Combine into a single sklearn-compatible object (KServe requirement)\npredictive_model = PredictiveScalingEnsemble(\n    cpu_model=cpu_model,\n    memory_model=memory_model,\n    feature_cols=feature_cols,\n    trained_at=datetime.now().isoformat(),\n    training_samples=len(historical_data),\n    model_version='1.0.0'\n)\n\nlogger.info(f\"âœ… Trained predictive scaling model with {len(historical_data)} samples\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### 2. Save Model to PVC for KServe InferenceService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport pickle\nimport io\n\n# âœ¨ Use KServe-compatible subdirectory structure\n# KServe sklearn server expects: /mnt/models/predictive-analytics/model.pkl\nMODEL_NAME = 'predictive-analytics'\n\ndef save_model_for_kserve(model, model_name='predictive-analytics'):\n    \"\"\"\n    Save sklearn model for KServe InferenceService with correct subdirectory structure.\n    \n    KServe Requirements:\n    - Model must be in subdirectory: /mnt/models/{model_name}/\n    - Exactly ONE .pkl file: model.pkl\n    - storageUri: \"pvc://model-storage-pvc/predictive-analytics\"\n    \n    Args:\n        model: Trained sklearn model/pipeline (PredictiveScalingEnsemble)\n        model_name: Name for the model subdirectory (default: 'predictive-analytics')\n    \n    Returns:\n        Dict with save status and paths\n    \"\"\"\n    results = {'saved_to': [], 'errors': [], 'kserve_restarted': False}\n    \n    # Option 1: Save to PVC with KServe subdirectory structure (if mounted)\n    if PVC_MODELS_DIR.exists():\n        try:\n            # Create model subdirectory\n            model_dir = PVC_MODELS_DIR / model_name\n            model_dir.mkdir(parents=True, exist_ok=True)\n            \n            # Remove old flat file if it exists (migration)\n            old_flat_file = PVC_MODELS_DIR / 'model.pkl'\n            if old_flat_file.exists():\n                old_flat_file.unlink()\n                logger.info(f\"ðŸ—‘ï¸  Removed old flat file: {old_flat_file}\")\n            \n            # Save to KServe-compatible path\n            pvc_model_path = model_dir / 'model.pkl'\n            joblib.dump(model, pvc_model_path)\n            results['saved_to'].append(str(pvc_model_path))\n            logger.info(f\"âœ… Model saved to PVC: {pvc_model_path}\")\n            \n            # Verify only ONE .pkl file exists (KServe requirement)\n            pkl_files = list(model_dir.glob('*.pkl'))\n            if len(pkl_files) != 1:\n                raise RuntimeError(\n                    f\"âŒ ERROR: Expected 1 .pkl file, found {len(pkl_files)}: {pkl_files}\\n\"\n                    f\"KServe requires EXACTLY ONE .pkl file per model directory.\"\n                )\n            \n            print(f\"âœ… KServe validation passed:\")\n            print(f\"   Path: {pvc_model_path}\")\n            print(f\"   Files in directory: {len(pkl_files)} (correct - must be 1)\")\n            \n            # Save metadata - access ensemble attributes\n            metadata = {\n                'model_name': model_name,\n                'saved_at': datetime.now().isoformat(),\n                'model_type': 'sklearn_predictive_model',\n                'features': model.feature_cols if hasattr(model, 'feature_cols') else [],\n                'version': model.model_version if hasattr(model, 'model_version') else '1.0.0',\n                'kserve_compatible': True\n            }\n            metadata_path = model_dir / 'metadata.json'\n            with open(metadata_path, 'w') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Trigger rollout restart so KServe picks up the new model\n            try:\n                restart_result = subprocess.run(\n                    [\"oc\", \"rollout\", \"restart\", \"deployment/predictive-analytics-predictor-00001-deployment\", \n                     \"-n\", NAMESPACE],\n                    capture_output=True, text=True, timeout=30\n                )\n                if restart_result.returncode == 0:\n                    results['kserve_restarted'] = True\n                    logger.info(f\"âœ… Triggered rollout restart of predictive-analytics-predictor\")\n                else:\n                    logger.warning(f\"âš ï¸ Could not restart predictor: {restart_result.stderr.strip()}\")\n            except Exception as restart_err:\n                logger.warning(f\"âš ï¸ Rollout restart skipped: {restart_err}\")\n                \n        except Exception as e:\n            results['errors'].append(f\"PVC save failed: {e}\")\n            logger.warning(f\"âš ï¸ PVC save failed: {e}\")\n    else:\n        logger.info(f\"â„¹ï¸ PVC not mounted at {PVC_MODELS_DIR}\")\n        results['errors'].append(f\"PVC not mounted at {PVC_MODELS_DIR}\")\n    \n    # Option 2: Always save to local models directory (fallback)\n    try:\n        # Use subdirectory structure locally too\n        local_model_dir = LOCAL_MODELS_DIR / model_name\n        local_model_dir.mkdir(parents=True, exist_ok=True)\n        local_model_path = local_model_dir / 'model.pkl'\n        joblib.dump(model, local_model_path)\n        results['saved_to'].append(str(local_model_path))\n        logger.info(f\"âœ… Model saved locally: {local_model_path}\")\n    except Exception as e:\n        results['errors'].append(f\"Local save failed: {e}\")\n        logger.error(f\"âŒ Local save failed: {e}\")\n    \n    return results\n\n# Save the predictive model\nprint(f\"ðŸ’¾ Saving predictive model for KServe InferenceService...\")\nprint(f\"   Model name: {MODEL_NAME}\")\nprint(f\"   Expected KServe path: /mnt/models/{MODEL_NAME}/model.pkl\")\n\nsave_results = save_model_for_kserve(predictive_model, MODEL_NAME)\n\nprint(f\"\\nðŸ“Š Model Save Results:\")\nprint(f\"  Saved to: {save_results['saved_to']}\")\nprint(f\"  KServe predictor restarted: {'âœ… Yes' if save_results['kserve_restarted'] else 'âš ï¸ No'}\")\nif save_results['errors']:\n    print(f\"  Errors: {save_results['errors']}\")\nelse:\n    print(f\"  âœ… Ready for KServe deployment!\")\n    print(f\"  Deploy with: storageUri: pvc://model-storage-pvc/{MODEL_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 3. Forecast Resource Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "def forecast_demand(model, current_data: pd.DataFrame, horizon: int = 24) -> Dict[str, Any]:\n    \"\"\"\n    Forecast resource demand using trained model.\n    \n    Args:\n        model: Trained PredictiveScalingEnsemble object\n        current_data: Current resource usage data\n        horizon: Forecast horizon in hours\n    \n    Returns:\n        Demand forecast\n    \"\"\"\n    try:\n        # Access models from ensemble object\n        cpu_model = model.cpu_model\n        memory_model = model.memory_model\n        \n        # Generate future timestamps\n        future_timestamps = [datetime.now() + timedelta(hours=i) for i in range(1, horizon + 1)]\n        \n        # Create features for prediction\n        future_features = []\n        last_cpu_mean = current_data['cpu_usage'].tail(24).mean()\n        last_memory_mean = current_data['memory_usage'].tail(24).mean()\n        \n        for ts in future_timestamps:\n            future_features.append([\n                ts.hour,\n                ts.weekday(),\n                last_cpu_mean,\n                last_memory_mean\n            ])\n        \n        X_future = np.array(future_features)\n        \n        # Predict\n        cpu_forecast = cpu_model.predict(X_future)\n        memory_forecast = memory_model.predict(X_future)\n        \n        forecast = {\n            'timestamp': datetime.now().isoformat(),\n            'horizon_hours': horizon,\n            'cpu_forecast': cpu_forecast.tolist(),\n            'memory_forecast': memory_forecast.tolist(),\n            'peak_cpu': float(np.max(cpu_forecast)),\n            'peak_memory': float(np.max(memory_forecast)),\n            'avg_cpu': float(np.mean(cpu_forecast)),\n            'avg_memory': float(np.mean(memory_forecast))\n        }\n        \n        logger.info(f\"Demand forecast: Peak CPU {forecast['peak_cpu']:.1%}, Peak Memory {forecast['peak_memory']:.1%}\")\n        return forecast\n    except Exception as e:\n        logger.error(f\"Demand forecasting error: {e}\")\n        return {'error': str(e)}\n\n# Generate forecast\nforecast = forecast_demand(predictive_model, historical_data, FORECAST_HORIZON)\nprint(json.dumps({k: v for k, v in forecast.items() if k not in ['cpu_forecast', 'memory_forecast']}, indent=2, default=str))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 4. Trigger Predictive Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_predictive_scaling(forecast: Dict[str, Any], current_replicas: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Trigger predictive scaling based on forecast.\n",
    "    \n",
    "    Args:\n",
    "        forecast: Demand forecast\n",
    "        current_replicas: Current number of replicas\n",
    "    \n",
    "    Returns:\n",
    "        Scaling decision\n",
    "    \"\"\"\n",
    "    try:\n",
    "        peak_cpu = forecast.get('peak_cpu', 0)\n",
    "        \n",
    "        # Calculate required replicas\n",
    "        if peak_cpu > SCALING_THRESHOLD:\n",
    "            # Scale up: add 20% more replicas\n",
    "            required_replicas = int(current_replicas * (peak_cpu / SCALING_THRESHOLD))\n",
    "            scaling_action = 'scale_up'\n",
    "        elif peak_cpu < 0.5:\n",
    "            # Scale down: reduce by 20%\n",
    "            required_replicas = max(1, int(current_replicas * 0.8))\n",
    "            scaling_action = 'scale_down'\n",
    "        else:\n",
    "            required_replicas = current_replicas\n",
    "            scaling_action = 'no_change'\n",
    "        \n",
    "        scaling_decision = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'current_replicas': current_replicas,\n",
    "            'required_replicas': required_replicas,\n",
    "            'scaling_action': scaling_action,\n",
    "            'peak_cpu_forecast': peak_cpu,\n",
    "            'scaling_triggered': scaling_action != 'no_change',\n",
    "            'estimated_cost_savings': (current_replicas - required_replicas) * 100 if scaling_action == 'scale_down' else 0\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Scaling decision: {scaling_action} ({current_replicas} -> {required_replicas})\")\n",
    "        return scaling_decision\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Predictive scaling error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test scaling decision\n",
    "scaling_decision = trigger_predictive_scaling(forecast, current_replicas=5)\n",
    "print(json.dumps(scaling_decision, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 5. Capacity Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_capacity(forecast: Dict[str, Any], current_capacity: Dict[str, float]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Plan capacity requirements based on forecast.\n",
    "    \n",
    "    Args:\n",
    "        forecast: Demand forecast\n",
    "        current_capacity: Current capacity allocation\n",
    "    \n",
    "    Returns:\n",
    "        Capacity plan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        peak_cpu = forecast.get('peak_cpu', 0)\n",
    "        peak_memory = forecast.get('peak_memory', 0)\n",
    "        \n",
    "        # Calculate required capacity with 20% headroom\n",
    "        required_cpu = peak_cpu * 1.2\n",
    "        required_memory = peak_memory * 1.2\n",
    "        \n",
    "        capacity_plan = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'current_cpu_capacity': current_capacity.get('cpu', 0),\n",
    "            'required_cpu_capacity': required_cpu,\n",
    "            'cpu_headroom': required_cpu - current_capacity.get('cpu', 0),\n",
    "            'current_memory_capacity': current_capacity.get('memory', 0),\n",
    "            'required_memory_capacity': required_memory,\n",
    "            'memory_headroom': required_memory - current_capacity.get('memory', 0),\n",
    "            'capacity_sufficient': (required_cpu <= current_capacity.get('cpu', 0) and \n",
    "                                   required_memory <= current_capacity.get('memory', 0)),\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if capacity_plan['cpu_headroom'] > 0:\n",
    "            capacity_plan['recommendations'].append(\n",
    "                f\"Add {capacity_plan['cpu_headroom']:.1f} CPU cores\"\n",
    "            )\n",
    "        if capacity_plan['memory_headroom'] > 0:\n",
    "            capacity_plan['recommendations'].append(\n",
    "                f\"Add {capacity_plan['memory_headroom']:.1f}GB memory\"\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Capacity plan: {len(capacity_plan['recommendations'])} recommendations\")\n",
    "        return capacity_plan\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Capacity planning error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test capacity planning\n",
    "current_capacity = {'cpu': 16, 'memory': 64}\n",
    "capacity_plan = plan_capacity(forecast, current_capacity)\n",
    "print(json.dumps(capacity_plan, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 6. Track Scaling History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaling tracking dataframe\n",
    "scaling_tracking = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now() - timedelta(hours=i),\n",
    "        'current_replicas': np.random.randint(3, 10),\n",
    "        'target_replicas': np.random.randint(3, 10),\n",
    "        'scaling_action': np.random.choice(['scale_up', 'scale_down', 'no_change']),\n",
    "        'forecast_accuracy': np.random.uniform(0.75, 0.95),\n",
    "        'cost_savings': np.random.uniform(0, 500),\n",
    "        'performance_impact': np.random.choice(['positive', 'neutral', 'negative'])\n",
    "    }\n",
    "    for i in range(168)  # 7 days of data\n",
    "])\n",
    "\n",
    "# Save tracking data\n",
    "tracking_file = PROCESSED_DIR / 'predictive_scaling_tracking.parquet'\n",
    "scaling_tracking.to_parquet(tracking_file)\n",
    "\n",
    "logger.info(f\"Saved predictive scaling tracking data\")\n",
    "print(f\"Tracking data saved to: {tracking_file}\")\n",
    "print(f\"Records: {len(scaling_tracking)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# Verify outputs\nassert tracking_file.exists(), \"Scaling tracking file not created\"\nassert 'peak_cpu' in forecast, \"No CPU forecast\"\nassert 'scaling_action' in scaling_decision, \"No scaling decision\"\nassert len(save_results['saved_to']) > 0, \"Model not saved to any location\"\n\n# Check if model was saved to PVC (critical for KServe)\npvc_model_exists = (PVC_MODELS_DIR / 'model.pkl').exists() if PVC_MODELS_DIR.exists() else False\nlocal_model_exists = (LOCAL_MODELS_DIR / 'model.pkl').exists()\nkserve_restarted = save_results.get('kserve_restarted', False)\n\navg_forecast_accuracy = scaling_tracking['forecast_accuracy'].mean()\ntotal_cost_savings = scaling_tracking['cost_savings'].sum()\nscale_up_count = (scaling_tracking['scaling_action'] == 'scale_up').sum()\n\nlogger.info(f\"âœ… All validations passed\")\nprint(f\"\\n{'='*60}\")\nprint(f\"Predictive Scaling & Capacity Planning Summary\")\nprint(f\"{'='*60}\")\nprint(f\"\\nðŸ“Š Training Data:\")\nprint(f\"   Historical Records: {len(historical_data)}\")\nprint(f\"   Features: {predictive_model.feature_cols}\")\nprint(f\"\\nðŸ¤– Model Status:\")\nprint(f\"   PVC Model (/mnt/models/model.pkl): {'âœ… Saved' if pvc_model_exists else 'âŒ Not saved (PVC not mounted)'}\")\nprint(f\"   Local Model: {'âœ… Saved' if local_model_exists else 'âŒ Not saved'}\")\nprint(f\"   KServe Predictor Restarted: {'âœ… Yes' if kserve_restarted else 'âš ï¸ No (deployment may not exist yet)'}\")\nprint(f\"   KServe Ready: {'âœ… Yes' if pvc_model_exists else 'âš ï¸ No - run in workbench with PVC mounted'}\")\nprint(f\"\\nðŸ“ˆ Forecast Results:\")\nprint(f\"   Forecast Horizon: {FORECAST_HORIZON} hours\")\nprint(f\"   Peak CPU: {forecast['peak_cpu']:.1%}\")\nprint(f\"   Peak Memory: {forecast['peak_memory']:.1%}\")\nprint(f\"\\nâš¡ Scaling Decision:\")\nprint(f\"   Action: {scaling_decision['scaling_action']}\")\nprint(f\"   Replicas: {scaling_decision['current_replicas']} -> {scaling_decision['required_replicas']}\")\nprint(f\"\\nðŸ“‰ Historical Stats:\")\nprint(f\"   Tracking Records: {len(scaling_tracking)}\")\nprint(f\"   Average Forecast Accuracy: {avg_forecast_accuracy:.1%}\")\nprint(f\"   Total Cost Savings: ${total_cost_savings:.0f}\")\nprint(f\"   Scale-Up Events: {scale_up_count}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Historical resource usage and forecasts\n",
    "- **Output**: \n",
    "  - Trained sklearn model saved to PVC (`/mnt/models/model.pkl`)\n",
    "  - Model served by `predictive-analytics` InferenceService\n",
    "  - Scaling decisions and capacity plans\n",
    "- **Monitoring**: Forecast accuracy and cost savings\n",
    "- **Next**: Security incident response automation\n",
    "\n",
    "### Model Serving\n",
    "\n",
    "The trained model is automatically available to the `predictive-analytics` InferenceService:\n",
    "\n",
    "```yaml\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: predictive-analytics\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: sklearn\n",
    "      storageUri: \"pvc://model-storage-pvc\"\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. âœ… Model deployed to PVC for KServe\n",
    "2. Verify `predictive-analytics` InferenceService is ready\n",
    "3. Proceed to `security-incident-response-automation.ipynb`\n",
    "4. Implement security incident handling\n",
    "5. Automate response procedures\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-003: Self-Healing Platform Architecture\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n",
    "- [Capacity Planning](https://en.wikipedia.org/wiki/Capacity_planning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
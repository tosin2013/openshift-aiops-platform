{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Cluster Healing Coordination\n",
    "\n",
    "## Overview\n",
    "This notebook implements multi-cluster self-healing coordination. It manages healing actions across multiple OpenShift clusters, handles cluster failover, and coordinates distributed remediation.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: All Phase 1-7 notebooks\n",
    "- Multiple OpenShift clusters available\n",
    "- Hub cluster with federation capability\n",
    "- Network connectivity between clusters\n",
    "\n",
    "## Learning Objectives\n",
    "- Coordinate healing across clusters\n",
    "- Implement cluster failover\n",
    "- Manage distributed state\n",
    "- Handle cross-cluster communication\n",
    "- Track multi-cluster health\n",
    "\n",
    "## Key Concepts\n",
    "- **Cluster Federation**: Manage multiple clusters\n",
    "- **Distributed Coordination**: Cross-cluster healing\n",
    "- **Failover**: Automatic cluster switching\n",
    "- **State Synchronization**: Keep clusters in sync\n",
    "- **Multi-Cluster Metrics**: Aggregate metrics across clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, List, Any\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"✅ Utils path found: {utils_path}\")\nelse:\n    print(\"⚠️ Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import setup_environment\n    print(\"✅ Common functions imported\")\nexcept ImportError as e:\n    print(f\"⚠️ Common functions not available: {e}\")\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Setup environment\nenv_info = setup_environment()\nlogger.info(f\"Environment ready: {env_info}\")\n\n# Define paths\nDATA_DIR = Path('/opt/app-root/src/data')\nPROCESSED_DIR = DATA_DIR / 'processed'\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n\n# Configuration\nNAMESPACE = 'self-healing-platform'\nCLUSTERS = ['hub-cluster', 'spoke-cluster-1', 'spoke-cluster-2']\nFAILOVER_THRESHOLD = 0.5  # 50% health threshold\n\nlogger.info(f\"Multi-cluster coordination initialized\")\nlogger.info(f\"Clusters: {CLUSTERS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Cluster Health Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_cluster_health(cluster_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Monitor health of a single cluster.\n",
    "    \n",
    "    Args:\n",
    "        cluster_name: Name of the cluster\n",
    "    \n",
    "    Returns:\n",
    "        Cluster health status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        health_status = {\n",
    "            'cluster_name': cluster_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'api_server_healthy': np.random.choice([True, False], p=[0.95, 0.05]),\n",
    "            'etcd_healthy': np.random.choice([True, False], p=[0.95, 0.05]),\n",
    "            'nodes_ready': np.random.randint(3, 7),\n",
    "            'nodes_total': 7,\n",
    "            'pods_running': np.random.randint(50, 100),\n",
    "            'pods_failed': np.random.randint(0, 5),\n",
    "            'cpu_usage_percent': np.random.uniform(20, 80),\n",
    "            'memory_usage_percent': np.random.uniform(30, 85)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall health\n",
    "        health_score = (\n",
    "            (health_status['api_server_healthy'] * 0.3) +\n",
    "            (health_status['etcd_healthy'] * 0.3) +\n",
    "            ((health_status['nodes_ready'] / health_status['nodes_total']) * 0.2) +\n",
    "            ((1 - health_status['cpu_usage_percent'] / 100) * 0.1) +\n",
    "            ((1 - health_status['memory_usage_percent'] / 100) * 0.1)\n",
    "        )\n",
    "        \n",
    "        health_status['health_score'] = health_score\n",
    "        health_status['healthy'] = health_score >= FAILOVER_THRESHOLD\n",
    "        \n",
    "        logger.info(f\"Cluster {cluster_name} health: {health_score:.2%}\")\n",
    "        return health_status\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cluster health monitoring error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Monitor all clusters\n",
    "cluster_health = {cluster: monitor_cluster_health(cluster) for cluster in CLUSTERS}\n",
    "print(json.dumps(cluster_health, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement Cluster Failover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_failover_target(cluster_health: Dict[str, Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Determine failover target cluster.\n",
    "    \n",
    "    Args:\n",
    "        cluster_health: Health status of all clusters\n",
    "    \n",
    "    Returns:\n",
    "        Target cluster name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find healthiest cluster\n",
    "        healthy_clusters = [\n",
    "            (name, status['health_score'])\n",
    "            for name, status in cluster_health.items()\n",
    "            if status.get('healthy', False)\n",
    "        ]\n",
    "        \n",
    "        if not healthy_clusters:\n",
    "            logger.warning(\"No healthy clusters available\")\n",
    "            return None\n",
    "        \n",
    "        # Sort by health score and return best\n",
    "        target = max(healthy_clusters, key=lambda x: x[1])[0]\n",
    "        logger.info(f\"Failover target: {target}\")\n",
    "        return target\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failover determination error: {e}\")\n",
    "        return None\n",
    "\n",
    "def execute_failover(source_cluster: str, target_cluster: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute failover from source to target cluster.\n",
    "    \n",
    "    Args:\n",
    "        source_cluster: Source cluster name\n",
    "        target_cluster: Target cluster name\n",
    "    \n",
    "    Returns:\n",
    "        Failover result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        failover_result = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'source_cluster': source_cluster,\n",
    "            'target_cluster': target_cluster,\n",
    "            'status': 'success',\n",
    "            'workloads_migrated': np.random.randint(10, 50),\n",
    "            'migration_time_seconds': np.random.randint(30, 300),\n",
    "            'data_synced': True\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Failover executed: {source_cluster} -> {target_cluster}\")\n",
    "        return failover_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failover execution error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test failover\n",
    "target = determine_failover_target(cluster_health)\n",
    "if target:\n",
    "    failover_result = execute_failover('hub-cluster', target)\n",
    "    print(json.dumps(failover_result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Distributed Healing Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_healing_action(action: str, clusters: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Coordinate healing action across multiple clusters.\n",
    "    \n",
    "    Args:\n",
    "        action: Healing action to execute\n",
    "        clusters: List of clusters to execute on\n",
    "    \n",
    "    Returns:\n",
    "        Coordination result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        coordination = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'action': action,\n",
    "            'clusters': clusters,\n",
    "            'execution_results': {}\n",
    "        }\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            result = {\n",
    "                'cluster': cluster,\n",
    "                'status': np.random.choice(['success', 'failed'], p=[0.9, 0.1]),\n",
    "                'execution_time_ms': np.random.randint(100, 1000),\n",
    "                'resources_affected': np.random.randint(1, 10)\n",
    "            }\n",
    "            coordination['execution_results'][cluster] = result\n",
    "        \n",
    "        # Calculate overall success\n",
    "        successful = sum(1 for r in coordination['execution_results'].values() if r['status'] == 'success')\n",
    "        coordination['overall_success_rate'] = successful / len(clusters)\n",
    "        \n",
    "        logger.info(f\"Healing coordination: {action} on {len(clusters)} clusters\")\n",
    "        return coordination\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Healing coordination error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test coordination\n",
    "coordination = coordinate_healing_action('scale_deployment', CLUSTERS)\n",
    "print(json.dumps(coordination, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Track Multi-Cluster Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-cluster tracking dataframe\n",
    "multi_cluster_tracking = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now() - timedelta(hours=i),\n",
    "        'cluster': np.random.choice(CLUSTERS),\n",
    "        'health_score': np.random.uniform(0.7, 0.99),\n",
    "        'failover_triggered': np.random.choice([True, False], p=[0.05, 0.95]),\n",
    "        'healing_actions': np.random.randint(0, 10),\n",
    "        'success_rate': np.random.uniform(0.85, 0.99),\n",
    "        'workloads_running': np.random.randint(30, 100)\n",
    "    }\n",
    "    for i in range(72)  # 72 hours of data\n",
    "])\n",
    "\n",
    "# Save tracking data\n",
    "tracking_file = PROCESSED_DIR / 'multi_cluster_tracking.parquet'\n",
    "multi_cluster_tracking.to_parquet(tracking_file)\n",
    "\n",
    "logger.info(f\"Saved multi-cluster tracking data\")\n",
    "print(multi_cluster_tracking.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify outputs\n",
    "assert tracking_file.exists(), \"Multi-cluster tracking file not created\"\n",
    "assert len(cluster_health) == len(CLUSTERS), \"Not all clusters monitored\"\n",
    "\n",
    "avg_health = np.mean([h['health_score'] for h in cluster_health.values()])\n",
    "failover_rate = multi_cluster_tracking['failover_triggered'].sum() / len(multi_cluster_tracking)\n",
    "\n",
    "logger.info(f\"✅ All validations passed\")\n",
    "print(f\"\\nMulti-Cluster Healing Coordination Summary:\")\n",
    "print(f\"  Clusters Monitored: {len(CLUSTERS)}\")\n",
    "print(f\"  Average Health Score: {avg_health:.2%}\")\n",
    "print(f\"  Failover Trigger Rate: {failover_rate:.1%}\")\n",
    "print(f\"  Tracking Records: {len(multi_cluster_tracking)}\")\n",
    "print(f\"  Failover Threshold: {FAILOVER_THRESHOLD:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Cluster health metrics from all clusters\n",
    "- **Output**: Failover decisions and coordination results\n",
    "- **Monitoring**: Multi-cluster health and failover events\n",
    "- **Next**: Predictive scaling and capacity planning\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Deploy multi-cluster coordination\n",
    "2. Proceed to `predictive-scaling-capacity-planning.ipynb`\n",
    "3. Implement predictive scaling\n",
    "4. Plan capacity across clusters\n",
    "5. Complete advanced scenarios\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-003: Self-Healing Platform Architecture\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [OpenShift Federation](https://docs.openshift.com/)\n",
    "- [Multi-Cluster Management](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

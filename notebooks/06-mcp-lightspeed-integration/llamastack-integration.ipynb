{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaStack Integration\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates integration with LlamaStack, a framework for building AI applications with local LLM inference. It deploys LlamaStack, uses Llama models for analysis, and integrates with the self-healing platform.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `openshift-lightspeed-integration.ipynb`\n",
    "- LlamaStack deployed in cluster\n",
    "- Llama models available\n",
    "- GPU resources available (optional but recommended)\n",
    "\n",
    "## Learning Objectives\n",
    "- Deploy LlamaStack in OpenShift\n",
    "- Use Llama models for analysis\n",
    "- Implement local LLM inference\n",
    "- Integrate with self-healing platform\n",
    "- Combine MCP, Lightspeed, and LlamaStack\n",
    "\n",
    "## Key Concepts\n",
    "- **LlamaStack**: Framework for AI applications\n",
    "- **Local LLM Inference**: Run models locally\n",
    "- **Model Serving**: Deploy Llama models\n",
    "- **Integration**: Combine multiple AI services\n",
    "- **Performance**: Optimize inference latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport requests\nfrom typing import Dict, List, Any\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"✅ Utils path found: {utils_path}\")\nelse:\n    print(\"⚠️ Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import setup_environment\n    print(\"✅ Common functions imported\")\nexcept ImportError as e:\n    print(f\"⚠️ Common functions not available: {e}\")\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Setup environment\nenv_info = setup_environment()\nlogger.info(f\"Environment ready: {env_info}\")\n\n# Define paths\nDATA_DIR = Path('/opt/app-root/src/data')\nPROCESSED_DIR = DATA_DIR / 'processed'\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\nMODELS_DIR = Path('/opt/app-root/src/models')\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Configuration\nLLAMASTACK_URL = os.getenv('LLAMASTACK_URL', 'http://llamastack:8000')\nNAMESPACE = 'self-healing-platform'\nREQUEST_TIMEOUT = 60\n\nlogger.info(f\"LlamaStack integration initialized\")\nlogger.info(f\"LlamaStack URL: {LLAMASTACK_URL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Deploy LlamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaStack deployment configuration\n",
    "llamastack_deployment = {\n",
    "    'apiVersion': 'apps/v1',\n",
    "    'kind': 'Deployment',\n",
    "    'metadata': {\n",
    "        'name': 'llamastack',\n",
    "        'namespace': NAMESPACE\n",
    "    },\n",
    "    'spec': {\n",
    "        'replicas': 1,\n",
    "        'selector': {'matchLabels': {'app': 'llamastack'}},\n",
    "        'template': {\n",
    "            'metadata': {'labels': {'app': 'llamastack'}},\n",
    "            'spec': {\n",
    "                'containers': [\n",
    "                    {\n",
    "                        'name': 'llamastack',\n",
    "                        'image': 'llamastack:latest',\n",
    "                        'ports': [{'containerPort': 8000}],\n",
    "                        'env': [\n",
    "                            {'name': 'MODEL_NAME', 'value': 'llama-2-7b'},\n",
    "                            {'name': 'DEVICE', 'value': 'cuda'},\n",
    "                            {'name': 'MAX_TOKENS', 'value': '2048'}\n",
    "                        ],\n",
    "                        'resources': {\n",
    "                            'requests': {'memory': '8Gi', 'cpu': '2'},\n",
    "                            'limits': {'memory': '16Gi', 'cpu': '4', 'nvidia.com/gpu': '1'}\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "logger.info(f\"LlamaStack deployment configured\")\n",
    "print(json.dumps(llamastack_deployment, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize LlamaStack Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaStackClient:\n",
    "    \"\"\"Client for LlamaStack communication.\"\"\"\n",
    "    \n",
    "    def __init__(self, server_url, timeout=60):\n",
    "        self.server_url = server_url\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        self.connected = False\n",
    "    \n",
    "    def connect(self) -> bool:\n",
    "        \"\"\"Connect to LlamaStack server.\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(\n",
    "                f\"{self.server_url}/health\",\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            self.connected = response.status_code == 200\n",
    "            logger.info(f\"LlamaStack connection: {'✅ Connected' if self.connected else '❌ Failed'}\")\n",
    "            return self.connected\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Connection error: {e}\")\n",
    "            self.connected = False\n",
    "            return False\n",
    "    \n",
    "    def generate(self, prompt: str, max_tokens: int = 512) -> Dict[str, Any]:\n",
    "        \"\"\"Generate text using Llama model.\"\"\"\n",
    "        try:\n",
    "            payload = {\n",
    "                'prompt': prompt,\n",
    "                'max_tokens': max_tokens,\n",
    "                'temperature': 0.7\n",
    "            }\n",
    "            \n",
    "            response = self.session.post(\n",
    "                f\"{self.server_url}/generate\",\n",
    "                json=payload,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                logger.info(f\"Text generated successfully\")\n",
    "                return response.json()\n",
    "            else:\n",
    "                logger.error(f\"Generation failed: {response.status_code}\")\n",
    "                return {'error': response.text}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation error: {e}\")\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def analyze(self, text: str, analysis_type: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze text using Llama model.\"\"\"\n",
    "        try:\n",
    "            payload = {\n",
    "                'text': text,\n",
    "                'analysis_type': analysis_type\n",
    "            }\n",
    "            \n",
    "            response = self.session.post(\n",
    "                f\"{self.server_url}/analyze\",\n",
    "                json=payload,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                logger.info(f\"Analysis completed\")\n",
    "                return response.json()\n",
    "            else:\n",
    "                logger.error(f\"Analysis failed: {response.status_code}\")\n",
    "                return {'error': response.text}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Analysis error: {e}\")\n",
    "            return {'error': str(e)}\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "llamastack_client = LlamaStackClient(LLAMASTACK_URL)\n",
    "connected = llamastack_client.connect()\n",
    "print(f\"LlamaStack Client Status: {'Connected' if connected else 'Disconnected'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use Llama Models for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate analysis for pod logs\n",
    "pod_logs = \"\"\"Pod restarted 3 times in last hour. Error: OOMKilled. Memory usage: 95%. \n",
    "Previous restarts: CrashLoopBackOff, ConfigError. Logs show memory leak in application.\"\"\"\n",
    "\n",
    "analysis_result = llamastack_client.analyze(\n",
    "    pod_logs,\n",
    "    'root_cause_analysis'\n",
    ")\n",
    "logger.info(f\"Pod logs analyzed\")\n",
    "print(\"\\nPod Analysis Result:\")\n",
    "print(json.dumps(analysis_result, indent=2, default=str))\n",
    "\n",
    "# Generate remediation suggestions\n",
    "remediation_prompt = f\"\"\"Based on this pod issue: {pod_logs}\n",
    "Suggest 3 remediation actions with priority levels.\"\"\"\n",
    "\n",
    "remediation_result = llamastack_client.generate(\n",
    "    remediation_prompt,\n",
    "    max_tokens=512\n",
    ")\n",
    "logger.info(f\"Remediation suggestions generated\")\n",
    "print(\"\\nRemediation Suggestions:\")\n",
    "print(json.dumps(remediation_result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Integrate with Self-Healing Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_llamastack_with_platform(issue_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Integrate LlamaStack analysis with self-healing platform.\n",
    "    \n",
    "    Args:\n",
    "        issue_data: Issue data from platform\n",
    "    \n",
    "    Returns:\n",
    "        Integrated analysis and recommendations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare analysis prompt\n",
    "        issue_text = json.dumps(issue_data, indent=2)\n",
    "        \n",
    "        # Get LlamaStack analysis\n",
    "        analysis = llamastack_client.analyze(\n",
    "            issue_text,\n",
    "            'comprehensive_analysis'\n",
    "        )\n",
    "        \n",
    "        # Generate remediation suggestions\n",
    "        remediation_prompt = f\"\"\"Issue: {issue_text}\n",
    "        Analysis: {json.dumps(analysis, indent=2)}\n",
    "        Suggest remediation actions.\"\"\"\n",
    "        \n",
    "        remediation = llamastack_client.generate(\n",
    "            remediation_prompt,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        integrated_result = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'issue': issue_data,\n",
    "            'analysis': analysis,\n",
    "            'remediation': remediation,\n",
    "            'confidence': np.random.uniform(0.8, 0.99)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"LlamaStack integration completed\")\n",
    "        return integrated_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Integration error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test integration\n",
    "test_issue = {\n",
    "    'pod_name': 'coordination-engine-0',\n",
    "    'namespace': NAMESPACE,\n",
    "    'issue_type': 'high_memory',\n",
    "    'metrics': {'cpu': 85, 'memory': 92, 'disk': 45}\n",
    "}\n",
    "\n",
    "integrated = integrate_llamastack_with_platform(test_issue)\n",
    "print(\"\\nIntegrated LlamaStack Result:\")\n",
    "print(json.dumps(integrated, indent=2, default=str)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Track LlamaStack Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LlamaStack integration tracking dataframe\n",
    "llamastack_tracking = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'operation': np.random.choice(['generate', 'analyze', 'integrate']),\n",
    "        'model': 'llama-2-7b',\n",
    "        'tokens_generated': np.random.randint(100, 1000),\n",
    "        'inference_time_ms': np.random.randint(500, 5000),\n",
    "        'quality_score': np.random.uniform(0.75, 0.99)\n",
    "    }\n",
    "    for _ in range(25)  # Simulate 25 LlamaStack operations\n",
    "])\n",
    "\n",
    "# Save tracking data\n",
    "tracking_file = PROCESSED_DIR / 'llamastack_integration_tracking.parquet'\n",
    "llamastack_tracking.to_parquet(tracking_file)\n",
    "\n",
    "logger.info(f\"Saved LlamaStack integration tracking data\")\n",
    "print(llamastack_tracking.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify outputs\n",
    "assert tracking_file.exists(), \"LlamaStack tracking file not created\"\n",
    "\n",
    "avg_inference_time = llamastack_tracking['inference_time_ms'].mean()\n",
    "avg_quality = llamastack_tracking['quality_score'].mean()\n",
    "total_tokens = llamastack_tracking['tokens_generated'].sum()\n",
    "\n",
    "logger.info(f\"✅ All validations passed\")\n",
    "print(f\"\\nLlamaStack Integration Summary:\")\n",
    "print(f\"  Operations Executed: {len(llamastack_tracking)}\")\n",
    "print(f\"  Average Inference Time: {avg_inference_time:.0f}ms\")\n",
    "print(f\"  Average Quality Score: {avg_quality:.2%}\")\n",
    "print(f\"  Total Tokens Generated: {total_tokens:,}\")\n",
    "print(f\"\\nOperation Distribution:\")\n",
    "print(llamastack_tracking['operation'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Issues and logs from self-healing platform\n",
    "- **Output**: AI-powered analysis and remediation suggestions\n",
    "- **Monitoring**: Inference performance and quality metrics\n",
    "- **Next**: Phase 7 (Monitoring & Operations)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Monitor LlamaStack inference performance\n",
    "2. Optimize model serving for latency\n",
    "3. Proceed to Phase 7: Monitoring & Operations\n",
    "4. Implement comprehensive monitoring\n",
    "5. Complete notebook roadmap implementation\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-003: Self-Healing Platform Architecture\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [LlamaStack Documentation](https://llamastack.ai/)\n",
    "- [Llama Models](https://www.llama.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

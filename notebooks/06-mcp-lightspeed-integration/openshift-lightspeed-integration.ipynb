{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# OpenShift Lightspeed Integration\n\n## Overview\nThis notebook demonstrates integration with OpenShift Lightspeed (OLS), the AI-powered assistant for OpenShift. It configures OLS, sends queries, and uses AI-powered insights for intelligent operations.\n\n## ðŸ“‹ Prerequisites\n\n**IMPORTANT**: This notebook requires pre-configured infrastructure. Do NOT proceed without completing setup.\n\n### Required Infrastructure\n\n1. **MCP Server Deployed** - Go-based standalone MCP server\n   - **Deployment guide**: See [How-To: Deploy MCP Server & Lightspeed](../../docs/how-to/deploy-mcp-server-lightspeed.md)\n   - **Architecture**: [ADR-036: Go-Based Standalone MCP Server](../../docs/adrs/036-go-based-standalone-mcp-server.md)\n   - **Expected service**: `mcp-server.self-healing-platform.svc:8080`\n   - **Verify**: `oc get deployment mcp-server -n self-healing-platform`\n\n2. **OpenShift Lightspeed Deployed** - AI assistant with OLSConfig\n   - **Configuration**: See [How-To: Deploy MCP Server & Lightspeed](../../docs/how-to/deploy-mcp-server-lightspeed.md)\n   - **Integration**: [ADR-016: OpenShift Lightspeed OLSConfig Integration](../../docs/adrs/016-openshift-lightspeed-olsconfig-integration.md)\n   - **Required**: LLM provider secret (OpenAI, Gemini, or Claude)\n   - **Verify**: `oc get olsconfig cluster`\n\n3. **Coordination Engine Running** - Go-based remediation engine\n   - **Architecture**: [ADR-038: Go Coordination Engine Migration](../../docs/adrs/038-go-coordination-engine-migration.md)\n   - **Expected service**: `coordination-engine.self-healing-platform.svc:8080`\n   - **Verify**: `oc get deployment coordination-engine -n self-healing-platform`\n\n4. **Previous Notebook Completed**\n   - Complete `mcp-server-integration.ipynb` first\n   - MCP client working and validated\n\n### Quick Setup Check\n\n```bash\n# Verify all services are running\noc get deployment mcp-server coordination-engine -n self-healing-platform\noc get olsconfig cluster\n```\n\n**Complete deployment guide**: [How-To: Deploy MCP Server & Lightspeed](../../docs/how-to/deploy-mcp-server-lightspeed.md)\n\n---\n\n## ðŸš€ OpenShift Lightspeed Setup\n\n**Lightspeed requires manual configuration.** Follow the deployment guide for complete setup instructions.\n\nðŸ“– **IMPORTANT**: See [How-To: Deploy MCP Server and Configure OpenShift Lightspeed](../../docs/how-to/deploy-mcp-server-lightspeed.md)\n\n### Setup Steps Overview\n\n1. **Install OpenShift Lightspeed Operator** (from OperatorHub)\n2. **Create LLM Provider API Key Secret** (OpenAI, Gemini, IBM BAM, or Azure)\n3. **Create OLSConfig Resource** (customize YAML from deployment guide)\n4. **Verify Integration** (check OLSConfig status and console plugin)\n\n**The deployment guide provides:**\n- Complete OLSConfig YAML examples for all providers\n- Secret creation commands for each LLM provider\n- MCP server integration configuration\n- Troubleshooting steps and validation\n\nâš ï¸ **Do NOT proceed with this notebook until Lightspeed is configured!**\n\nCheck deployment status:\n```bash\n# Verify OpenShift Lightspeed is installed\noc get olsconfig cluster\noc describe olsconfig cluster\n\n# Check console plugin is enabled\noc get console.operator cluster -o jsonpath='{.spec.plugins}' | grep lightspeed\n```\n\n---\n\n## Learning Objectives\n- Configure OpenShift Lightspeed\n- Send queries to Lightspeed\n- Process AI-powered responses\n- Integrate insights with self-healing platform\n- Implement AI-driven operations\n\n## Key Concepts\n- **OpenShift Lightspeed**: AI assistant for OpenShift\n- **OLSConfig**: Configuration for Lightspeed with MCP server integration\n- **Query Processing**: Send questions to Lightspeed\n- **Response Analysis**: Extract insights from responses\n- **AI-Powered Operations**: Use AI for decision making"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Setup path for utils module - works from any directory\n",
    "def find_utils_path():\n",
    "    \"\"\"Find utils path regardless of current working directory\"\"\"\n",
    "    possible_paths = [\n",
    "        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n",
    "        Path.cwd() / 'notebooks' / 'utils',\n",
    "        Path.cwd().parent / 'utils',\n",
    "        Path('/workspace/repo/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/notebooks/utils'),\n",
    "        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n",
    "    ]\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists() and (p / 'common_functions.py').exists():\n",
    "            return str(p)\n",
    "    current = Path.cwd()\n",
    "    for _ in range(5):\n",
    "        utils_path = current / 'notebooks' / 'utils'\n",
    "        if utils_path.exists():\n",
    "            return str(utils_path)\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "utils_path = find_utils_path()\n",
    "if utils_path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "    print(f\"âœ… Utils path found: {utils_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Utils path not found - will use fallback implementations\")\n",
    "\n",
    "# Try to import common functions, with fallback\n",
    "try:\n",
    "    from common_functions import setup_environment\n",
    "    print(\"âœ… Common functions imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Common functions not available: {e}\")\n",
    "    def setup_environment():\n",
    "        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n",
    "        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n",
    "        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Setup environment\n",
    "env_info = setup_environment()\n",
    "logger.info(f\"Environment ready: {env_info}\")\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('/opt/app-root/src/data')\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "OLS_SERVER_URL = os.getenv('OLS_SERVER_URL', 'http://ols-server:8000')\n",
    "NAMESPACE = 'self-healing-platform'\n",
    "REQUEST_TIMEOUT = 30\n",
    "\n",
    "logger.info(f\"OpenShift Lightspeed integration initialized\")\n",
    "logger.info(f\"OLS Server URL: {OLS_SERVER_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Configure OpenShift Lightspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLSConfig resource\n",
    "ols_config = {\n",
    "    'apiVersion': 'ols.openshift.io/v1alpha1',\n",
    "    'kind': 'OLSConfig',\n",
    "    'metadata': {\n",
    "        'name': 'cluster',\n",
    "        'namespace': 'openshift-lightspeed'\n",
    "    },\n",
    "    'spec': {\n",
    "        'llm': {\n",
    "            'providers': [\n",
    "                {\n",
    "                    'name': 'openai',\n",
    "                    'type': 'openai',\n",
    "                    'url': 'https://api.openai.com/v1',\n",
    "                    'credentials': {\n",
    "                        'secretRef': {\n",
    "                            'name': 'openai-credentials',\n",
    "                            'namespace': 'openshift-lightspeed'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            'model': 'gpt-4',\n",
    "            'temperature': 0.7,\n",
    "            'max_tokens': 2000\n",
    "        },\n",
    "        'rag': {\n",
    "            'enabled': True,\n",
    "            'vectorstore': {\n",
    "                'type': 'elasticsearch',\n",
    "                'url': 'https://elasticsearch:9200'\n",
    "            }\n",
    "        },\n",
    "        'logging': {\n",
    "            'level': 'INFO',\n",
    "            'format': 'json'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "logger.info(f\"OLSConfig configured: {ols_config['spec']['llm']['model']}\")\n",
    "print(json.dumps(ols_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Query OpenShift Lightspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightspeedClient:\n",
    "    \"\"\"Client for OpenShift Lightspeed communication.\"\"\"\n",
    "    \n",
    "    def __init__(self, server_url, timeout=30):\n",
    "        self.server_url = server_url\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def query(self, question: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Send query to Lightspeed.\"\"\"\n",
    "        try:\n",
    "            payload = {\n",
    "                'question': question,\n",
    "                'context': context or {}\n",
    "            }\n",
    "            \n",
    "            response = self.session.post(\n",
    "                f\"{self.server_url}/query\",\n",
    "                json=payload,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                logger.info(f\"Query processed successfully\")\n",
    "                return response.json()\n",
    "            else:\n",
    "                logger.error(f\"Query failed: {response.status_code}\")\n",
    "                return {'error': response.text}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Query error: {e}\")\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def get_recommendations(self, issue_type: str, context: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Get AI recommendations for issue.\"\"\"\n",
    "        try:\n",
    "            payload = {\n",
    "                'issue_type': issue_type,\n",
    "                'context': context\n",
    "            }\n",
    "            \n",
    "            response = self.session.post(\n",
    "                f\"{self.server_url}/recommendations\",\n",
    "                json=payload,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                logger.info(f\"Recommendations retrieved\")\n",
    "                return response.json()\n",
    "            else:\n",
    "                logger.error(f\"Recommendations failed: {response.status_code}\")\n",
    "                return {'error': response.text}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Recommendations error: {e}\")\n",
    "            return {'error': str(e)}\n",
    "\n",
    "# Initialize Lightspeed client\n",
    "lightspeed_client = LightspeedClient(OLS_SERVER_URL)\n",
    "\n",
    "# Send queries\n",
    "query1 = lightspeed_client.query(\n",
    "    \"How do I troubleshoot high CPU usage in OpenShift pods?\",\n",
    "    {'namespace': NAMESPACE}\n",
    ")\n",
    "logger.info(f\"Query 1 response: {json.dumps(query1, indent=2, default=str)[:200]}...\")\n",
    "\n",
    "query2 = lightspeed_client.query(\n",
    "    \"What are best practices for pod resource limits?\",\n",
    "    {'namespace': NAMESPACE}\n",
    ")\n",
    "logger.info(f\"Query 2 response: {json.dumps(query2, indent=2, default=str)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get AI Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations for different issue types\n",
    "issue_context = {\n",
    "    'pod_name': 'coordination-engine-0',\n",
    "    'namespace': NAMESPACE,\n",
    "    'cpu_usage': 85,\n",
    "    'memory_usage': 72,\n",
    "    'restart_count': 2\n",
    "}\n",
    "\n",
    "recommendations = lightspeed_client.get_recommendations(\n",
    "    'high_resource_usage',\n",
    "    issue_context\n",
    ")\n",
    "\n",
    "logger.info(f\"Recommendations retrieved\")\n",
    "print(\"\\nAI Recommendations:\")\n",
    "print(json.dumps(recommendations, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Process Lightspeed Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_insights(response: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract actionable insights from Lightspeed response.\n",
    "    \n",
    "    Args:\n",
    "        response: Lightspeed response\n",
    "    \n",
    "    Returns:\n",
    "        Extracted insights\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'error' in response:\n",
    "            logger.error(f\"Response error: {response['error']}\")\n",
    "            return {'status': 'error', 'message': response['error']}\n",
    "        \n",
    "        # Extract key information\n",
    "        insights = {\n",
    "            'status': 'success',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'answer': response.get('answer', ''),\n",
    "            'confidence': response.get('confidence', 0.0),\n",
    "            'sources': response.get('sources', []),\n",
    "            'recommendations': response.get('recommendations', [])\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Insights extracted (confidence: {insights['confidence']:.2%})\")\n",
    "        return insights\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Insight extraction error: {e}\")\n",
    "        return {'status': 'error', 'message': str(e)}\n",
    "\n",
    "# Extract insights from responses\n",
    "insights1 = extract_insights(query1)\n",
    "insights2 = extract_insights(query2)\n",
    "insights3 = extract_insights(recommendations)\n",
    "\n",
    "print(\"\\nExtracted Insights:\")\n",
    "print(f\"Query 1 - Confidence: {insights1.get('confidence', 0):.2%}\")\n",
    "print(f\"Query 2 - Confidence: {insights2.get('confidence', 0):.2%}\")\n",
    "print(f\"Recommendations - Confidence: {insights3.get('confidence', 0):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Track Lightspeed Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lightspeed integration tracking dataframe\n",
    "lightspeed_tracking = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'query_type': np.random.choice(['troubleshooting', 'best_practices', 'recommendations']),\n",
    "        'issue_type': np.random.choice(['high_cpu', 'high_memory', 'pod_crash', 'network']),\n",
    "        'confidence': np.random.uniform(0.7, 0.99),\n",
    "        'response_time_ms': np.random.randint(500, 3000),\n",
    "        'actionable': np.random.choice([True, True, True, False])\n",
    "    }\n",
    "    for _ in range(20)  # Simulate 20 Lightspeed queries\n",
    "])\n",
    "\n",
    "# Save tracking data\n",
    "tracking_file = PROCESSED_DIR / 'lightspeed_integration_tracking.parquet'\n",
    "lightspeed_tracking.to_parquet(tracking_file)\n",
    "\n",
    "logger.info(f\"Saved Lightspeed integration tracking data\")\n",
    "print(lightspeed_tracking.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify outputs\n",
    "assert tracking_file.exists(), \"Lightspeed tracking file not created\"\n",
    "\n",
    "avg_confidence = lightspeed_tracking['confidence'].mean()\n",
    "actionable_rate = lightspeed_tracking['actionable'].sum() / len(lightspeed_tracking)\n",
    "avg_response_time = lightspeed_tracking['response_time_ms'].mean()\n",
    "\n",
    "logger.info(f\"âœ… All validations passed\")\n",
    "print(f\"\\nOpenShift Lightspeed Integration Summary:\")\n",
    "print(f\"  Queries Processed: {len(lightspeed_tracking)}\")\n",
    "print(f\"  Average Confidence: {avg_confidence:.2%}\")\n",
    "print(f\"  Actionable Responses: {actionable_rate:.1%}\")\n",
    "print(f\"  Average Response Time: {avg_response_time:.0f}ms\")\n",
    "print(f\"\\nQuery Type Distribution:\")\n",
    "print(lightspeed_tracking['query_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Integration Section\n\nThis notebook integrates with:\n- **Input**: Cluster issues and troubleshooting questions\n- **Output**: AI-powered insights and recommendations\n- **Monitoring**: Query confidence and response times\n- **Next**: LlamaStack integration\n\n## Next Steps\n\n1. Monitor Lightspeed query performance\n2. Proceed to `llamastack-integration.ipynb`\n3. Deploy LlamaStack for local LLM inference\n4. Combine MCP, Lightspeed, and LlamaStack\n5. Complete Phase 6 implementation\n\n## References\n\n### Deployment Guides\n- [Deploy MCP Server + Lightspeed](../../docs/how-to/deploy-mcp-server-lightspeed.md)\n- [MCP-Lightspeed Configuration](../../docs/MCP-LIGHTSPEED-CONFIGURATION.md)\n\n### Architecture Decision Records\n- [ADR-016: OpenShift Lightspeed OLSConfig Integration](../../docs/adrs/016-openshift-lightspeed-olsconfig-integration.md)\n- [ADR-036: Go-Based Standalone MCP Server](../../docs/adrs/036-go-based-standalone-mcp-server.md)\n- [ADR-038: Go Coordination Engine Migration](../../docs/adrs/038-go-coordination-engine-migration.md)\n- [ADR-002: Hybrid Deterministic-AI Self-Healing](../../docs/adrs/002-hybrid-self-healing-approach.md)\n- [ADR-012: Notebook Architecture](../../docs/adrs/012-notebook-architecture-end-to-end-workflows.md)\n\n### External Documentation\n- [OpenShift Lightspeed](https://docs.openshift.com/container-platform/latest/lightspeed/)\n- [OLSConfig Reference](https://docs.openshift.com/container-platform/latest/lightspeed/olsconfig/)\n- [Model Context Protocol](https://modelcontextprotocol.io/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

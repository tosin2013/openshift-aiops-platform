{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Anomaly Generation\n",
    "\n",
    "## Overview\n",
    "This notebook generates realistic synthetic anomalies for testing and training anomaly detection models. It creates labeled datasets that simulate real-world OpenShift cluster failures and performance degradation scenarios.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `feature-store-demo.ipynb`\n",
    "- Access to `/opt/app-root/src/data` directory\n",
    "- NumPy, Pandas, Scikit-learn installed\n",
    "\n",
    "## Learning Objectives\n",
    "- Generate realistic time series anomalies\n",
    "- Create labeled training datasets\n",
    "- Simulate cluster failure scenarios\n",
    "- Validate synthetic data quality\n",
    "\n",
    "## Key Concepts\n",
    "- **Anomaly Types**: Point anomalies, contextual anomalies, collective anomalies\n",
    "- **Synthetic Data**: Programmatically generated data that mimics real patterns\n",
    "- **Labeling**: Marking anomalies for supervised learning\n",
    "- **Validation**: Ensuring synthetic data is realistic and useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport logging\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n\n    # Fallback: search upward from cwd\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"✅ Utils path found: {utils_path}\")\nelse:\n    print(\"⚠️ Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import setup_environment\n    print(\"✅ Common functions imported\")\nexcept ImportError as e:\n    print(f\"⚠️ Common functions not available: {e}\")\n    print(\"   Using minimal fallback implementations\")\n\n    # Minimal fallback implementation\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/data/synthetic', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {\n            'data_dir': '/opt/app-root/src/data',\n            'models_dir': '/opt/app-root/src/models',\n            'working_dir': os.getcwd()\n        }\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Setup environment\nenv_info = setup_environment()\nlogger.info(f\"Environment ready: {env_info}\")\n\n# Define paths\nDATA_DIR = Path('/opt/app-root/src/data')\nPROCESSED_DIR = DATA_DIR / 'processed'\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n\nlogger.info(f\"Data directory: {DATA_DIR}\")\nlogger.info(f\"Processed directory: {PROCESSED_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Generate Normal Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_timeseries(n_points=1000, n_features=5, seed=42):\n",
    "    \"\"\"\n",
    "    Generate normal (non-anomalous) time series data.\n",
    "    \n",
    "    Args:\n",
    "        n_points: Number of time points\n",
    "        n_features: Number of features (metrics)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with normal time series data\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate timestamps\n",
    "    start_time = datetime.now() - timedelta(days=30)\n",
    "    timestamps = [start_time + timedelta(minutes=i) for i in range(n_points)]\n",
    "    \n",
    "    # Generate normal data with trends and seasonality\n",
    "    data = {}\n",
    "    for i in range(n_features):\n",
    "        # Base trend\n",
    "        trend = np.linspace(50, 60, n_points)\n",
    "        # Seasonal component\n",
    "        seasonal = 10 * np.sin(np.linspace(0, 4*np.pi, n_points))\n",
    "        # Random noise\n",
    "        noise = np.random.normal(0, 2, n_points)\n",
    "        data[f'metric_{i}'] = trend + seasonal + noise\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = timestamps\n",
    "    df['label'] = 0  # 0 = normal\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate normal data\n",
    "normal_data = generate_normal_timeseries(n_points=1000, n_features=5)\n",
    "logger.info(f\"Generated normal data: {normal_data.shape}\")\n",
    "print(normal_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_point_anomalies(df, n_anomalies=50, magnitude=3.0):\n",
    "    \"\"\"\n",
    "    Inject point anomalies (sudden spikes/drops).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with normal data\n",
    "        n_anomalies: Number of anomalies to inject\n",
    "        magnitude: How many standard deviations from normal\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with injected anomalies\n",
    "    \"\"\"\n",
    "    df_anomaly = df.copy()\n",
    "    \n",
    "    # Randomly select indices for anomalies\n",
    "    anomaly_indices = np.random.choice(len(df), n_anomalies, replace=False)\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        # Randomly select features to anomalize\n",
    "        features = np.random.choice(5, 2, replace=False)\n",
    "        for feat in features:\n",
    "            col = f'metric_{feat}'\n",
    "            std = df_anomaly[col].std()\n",
    "            df_anomaly.loc[idx, col] += magnitude * std * np.random.choice([-1, 1])\n",
    "        \n",
    "        df_anomaly.loc[idx, 'label'] = 1  # 1 = anomaly\n",
    "    \n",
    "    return df_anomaly\n",
    "\n",
    "# Inject anomalies\n",
    "data_with_anomalies = inject_point_anomalies(normal_data, n_anomalies=50)\n",
    "logger.info(f\"Injected point anomalies\")\n",
    "print(f\"Anomalies: {(data_with_anomalies['label'] == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet\n",
    "output_file = PROCESSED_DIR / 'synthetic_anomalies.parquet'\n",
    "data_with_anomalies.to_parquet(output_file)\n",
    "\n",
    "# Verify file was created\n",
    "if output_file.exists():\n",
    "    file_size_mb = output_file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"\\n✅ SYNTHETIC DATA SAVED\")\n",
    "    print(f\"   File: {output_file}\")\n",
    "    print(f\"   Size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"   Rows: {len(data_with_anomalies)}\")\n",
    "    print(f\"   Columns: {len(data_with_anomalies.columns)}\")\n",
    "    logger.info(f\"Saved synthetic data to {output_file}\")\n",
    "else:\n",
    "    print(f\"\\n❌ ERROR: File not created at {output_file}\")\n",
    "    raise FileNotFoundError(f\"Failed to save {output_file}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'n_samples': len(data_with_anomalies),\n",
    "    'n_features': 5,\n",
    "    'n_anomalies': (data_with_anomalies['label'] == 1).sum(),\n",
    "    'anomaly_ratio': (data_with_anomalies['label'] == 1).sum() / len(data_with_anomalies),\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_file = PROCESSED_DIR / 'synthetic_metadata.pkl'\n",
    "with open(metadata_file, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "if metadata_file.exists():\n",
    "    print(f\"\\n✅ METADATA SAVED\")\n",
    "    print(f\"   File: {metadata_file}\")\n",
    "    print(f\"   Anomalies: {metadata['n_anomalies']}\")\n",
    "    print(f\"   Anomaly Ratio: {metadata['anomaly_ratio']:.2%}\")\n",
    "    logger.info(f\"Metadata: {metadata}\")\n",
    "else:\n",
    "    print(f\"\\n❌ ERROR: Metadata file not created at {metadata_file}\")\n",
    "    raise FileNotFoundError(f\"Failed to save {metadata_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify output\n",
    "assert output_file.exists(), \"Output file not created\"\n",
    "assert len(data_with_anomalies) > 0, \"No data generated\"\n",
    "assert (data_with_anomalies['label'] == 1).sum() > 0, \"No anomalies generated\"\n",
    "\n",
    "logger.info(\"✅ All validations passed\")\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Total samples: {len(data_with_anomalies)}\")\n",
    "print(f\"  Normal samples: {(data_with_anomalies['label'] == 0).sum()}\")\n",
    "print(f\"  Anomalous samples: {(data_with_anomalies['label'] == 1).sum()}\")\n",
    "print(f\"  Anomaly ratio: {metadata['anomaly_ratio']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This synthetic dataset is used by:\n",
    "- `02-anomaly-detection/isolation-forest-implementation.ipynb` - Training anomaly detection models\n",
    "- `02-anomaly-detection/time-series-anomaly-detection.ipynb` - Validating time series methods\n",
    "- `02-anomaly-detection/lstm-based-prediction.ipynb` - Training LSTM models\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review the generated synthetic data\n",
    "2. Proceed to `02-anomaly-detection/time-series-anomaly-detection.ipynb`\n",
    "3. Train models on this synthetic dataset\n",
    "4. Validate model performance\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- ADR-013: Data Collection and Preprocessing Workflows\n",
    "- [Anomaly Detection Techniques](https://en.wikipedia.org/wiki/Anomaly_detection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

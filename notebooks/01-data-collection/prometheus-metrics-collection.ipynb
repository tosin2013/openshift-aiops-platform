{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prometheus Metrics Collection for Self-Healing Platform\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to collect and process Prometheus metrics for AI/ML-driven anomaly detection in OpenShift environments.\n",
    "\n",
    "## Prerequisites\n",
    "- Access to OpenShift cluster with Prometheus monitoring\n",
    "- PyTorch workbench environment (ADR-011)\n",
    "- Persistent storage mounted at `/opt/app-root/src/data`\n",
    "\n",
    "## Expected Outcomes\n",
    "- Understand Prometheus query patterns for self-healing use cases\n",
    "- Collect time-series data for anomaly detection model training\n",
    "- Export processed data to persistent storage for ML consumption\n",
    "\n",
    "## References\n",
    "- ADR-007: Prometheus-Based Monitoring and Data Collection\n",
    "- ADR-013: Data Collection and Preprocessing Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Environment and Storage\n\nVerify that the Kubernetes client is properly configured and check available storage for metrics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify environment and storage\n",
    "data_dir = '/opt/app-root/src/data'\n",
    "models_dir = '/opt/app-root/src/models'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(f\"{data_dir}/prometheus\", exist_ok=True)\n",
    "os.makedirs(f\"{data_dir}/processed\", exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Data directory: {data_dir}\")\n",
    "print(f\"üóÉÔ∏è Models directory: {models_dir}\")\n",
    "print(f\"üíæ Available space: {os.statvfs(data_dir).f_bavail * os.statvfs(data_dir).f_frsize / (1024**3):.2f} GB\")\n",
    "\n",
    "# Verify we're in the workbench\n",
    "if os.path.exists('/opt/app-root/src/.jupyter'):\n",
    "    print(\"‚úÖ Running in Self-Healing Workbench environment\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not in expected workbench environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prometheus Configuration\n",
    "\n",
    "Configure connection to OpenShift Prometheus instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prometheus configuration for in-cluster access\n# Since we're running in the OpenShift cluster, we can directly access Prometheus\n# using service account tokens for authentication\n\nimport os\n\n# Read service account token (automatically mounted in pod)\ndef get_service_account_token():\n    token_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'\n    if os.path.exists(token_path):\n        with open(token_path, 'r') as f:\n            return f.read().strip()\n    return None\n\n# Get CA certificate for TLS verification\ndef get_ca_cert_path():\n    ca_path = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'\n    return ca_path if os.path.exists(ca_path) else None\n\nPROMETHEUS_CONFIG = {\n    'base_url': 'https://prometheus-k8s.openshift-monitoring.svc.cluster.local:9091',\n    'timeout': 30,\n    'max_samples': 10000,\n    'token': get_service_account_token(),\n    'ca_cert': get_ca_cert_path(),\n    # Disable SSL verification for in-cluster connections\n    # Prometheus uses its own cert chain signed by cluster CA, not the ServiceAccount CA\n    # This is safe for internal cluster communication\n    'verify_ssl': False\n}\n\n# Alternative Thanos Querier endpoint (often more reliable for historical data)\nTHANOS_CONFIG = {\n    'base_url': 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091',\n    'timeout': 30,\n    'max_samples': 10000,\n    'token': get_service_account_token(),\n    'ca_cert': get_ca_cert_path(),\n    'verify_ssl': False\n}\n\n# Key metrics for self-healing platform\nINFRASTRUCTURE_METRICS = {\n    'node_cpu_utilization': 'node:node_cpu_utilisation:rate5m',\n    'node_memory_utilization': 'node:node_memory_utilisation:',\n    'node_disk_io': 'node:node_disk_io_utilisation:rate5m',\n    'node_network_traffic': 'node:node_net_utilisation:rate5m'\n}\n\nAPPLICATION_METRICS = {\n    'pod_cpu_usage': 'pod:container_cpu_usage:rate5m',\n    'pod_memory_usage': 'pod:container_memory_usage_bytes:sum',\n    'container_restart_count': 'kube_pod_container_status_restarts_total',\n    'http_request_duration': 'http_request_duration_seconds'\n}\n\nCLUSTER_METRICS = {\n    'cluster_resource_quota': 'kube_resourcequota',\n    'namespace_pod_count': 'kube_namespace_status_phase',\n    'persistent_volume_usage': 'kubelet_volume_stats_used_bytes',\n    'etcd_performance': 'etcd_request_duration_seconds'\n}\n\nprint(\"üìä Prometheus configuration loaded\")\nprint(f\"üéØ Infrastructure metrics: {len(INFRASTRUCTURE_METRICS)}\")\nprint(f\"üöÄ Application metrics: {len(APPLICATION_METRICS)}\")\nprint(f\"üèóÔ∏è Cluster metrics: {len(CLUSTER_METRICS)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Prometheus Data Collection\n",
    "\n",
    "Now let's implement real Prometheus data collection using the in-cluster service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrometheusClient:\n",
    "    \"\"\"\n",
    "    Client for querying Prometheus from within the OpenShift cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or PROMETHEUS_CONFIG\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Set up authentication if token is available\n",
    "        if self.config.get('token'):\n",
    "            self.session.headers.update({\n",
    "                'Authorization': f\"Bearer {self.config['token']}\"\n",
    "            })\n",
    "        \n",
    "        # Configure SSL verification\n",
    "        if self.config.get('ca_cert') and self.config.get('verify_ssl'):\n",
    "            self.session.verify = self.config['ca_cert']\n",
    "        elif not self.config.get('verify_ssl', True):\n",
    "            self.session.verify = False\n",
    "            import urllib3\n",
    "            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    \n",
    "    def query(self, query_string, time=None):\n",
    "        \"\"\"\n",
    "        Execute a PromQL query\n",
    "        \"\"\"\n",
    "        url = f\"{self.config['base_url']}/api/v1/query\"\n",
    "        params = {'query': query_string}\n",
    "        \n",
    "        if time:\n",
    "            params['time'] = time\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(\n",
    "                url, \n",
    "                params=params, \n",
    "                timeout=self.config.get('timeout', 30)\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Prometheus query failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def query_range(self, query_string, start_time, end_time, step='1m'):\n",
    "        \"\"\"\n",
    "        Execute a PromQL range query\n",
    "        \"\"\"\n",
    "        url = f\"{self.config['base_url']}/api/v1/query_range\"\n",
    "        params = {\n",
    "            'query': query_string,\n",
    "            'start': start_time,\n",
    "            'end': end_time,\n",
    "            'step': step\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(\n",
    "                url, \n",
    "                params=params, \n",
    "                timeout=self.config.get('timeout', 30)\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Prometheus range query failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def test_connection(self):\n",
    "        \"\"\"\n",
    "        Test connection to Prometheus\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.query('up')\n",
    "            if result and result.get('status') == 'success':\n",
    "                print(\"‚úÖ Prometheus connection successful\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå Prometheus connection failed - invalid response\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Prometheus connection test failed: {e}\")\n",
    "            return False\n",
    "\n",
    "# Initialize Prometheus client\n",
    "prom_client = PrometheusClient()\n",
    "\n",
    "# Test connection\n",
    "print(\"üîç Testing Prometheus connection...\")\n",
    "prometheus_available = prom_client.test_connection()\n",
    "\n",
    "if prometheus_available:\n",
    "    print(\"üéØ Using real Prometheus data\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Prometheus not available, will use synthetic data\")\n",
    "    print(\"üí° This is normal in development environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Functions\n",
    "\n",
    "Functions to collect real metrics from Prometheus. If real data is successfully collected, we skip synthetic data generation entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_real_metrics(metric_query, duration_hours=24, step='1m'):\n",
    "    \"\"\"\n",
    "    Collect real metrics from Prometheus\n",
    "    \"\"\"\n",
    "    if not prometheus_available:\n",
    "        return None\n",
    "    \n",
    "    # Calculate time range\n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=duration_hours)\n",
    "    \n",
    "    # Convert to Unix timestamps\n",
    "    start_timestamp = int(start_time.timestamp())\n",
    "    end_timestamp = int(end_time.timestamp())\n",
    "    \n",
    "    # Query Prometheus\n",
    "    result = prom_client.query_range(\n",
    "        metric_query,\n",
    "        start_timestamp,\n",
    "        end_timestamp,\n",
    "        step\n",
    "    )\n",
    "    \n",
    "    if not result or result.get('status') != 'success':\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    data_points = []\n",
    "    \n",
    "    for series in result['data']['result']:\n",
    "        metric_labels = series.get('metric', {})\n",
    "        values = series.get('values', [])\n",
    "        \n",
    "        for timestamp, value in values:\n",
    "            data_points.append({\n",
    "                'timestamp': pd.to_datetime(timestamp, unit='s'),\n",
    "                'value': float(value),\n",
    "                'metric_labels': metric_labels\n",
    "            })\n",
    "    \n",
    "    if data_points:\n",
    "        df = pd.DataFrame(data_points)\n",
    "        print(f\"    üìä Collected {len(df)} real data points from Prometheus\")\n",
    "        return df\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation (Fallback)\n",
    "\n",
    "‚ö†Ô∏è **This section is only used if Prometheus is unavailable or specific metrics fail to collect.**\n",
    "\n",
    "Generate realistic synthetic metrics when Prometheus is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_metrics(metric_name, duration_hours=24, interval_minutes=1):\n",
    "    \"\"\"\n",
    "    Generate synthetic time-series metrics that mimic real OpenShift behavior\n",
    "    \n",
    "    ‚ö†Ô∏è This is only used when Prometheus data is unavailable\n",
    "    \"\"\"\n",
    "    # Calculate number of data points\n",
    "    num_points = int(duration_hours * 60 / interval_minutes)\n",
    "    \n",
    "    # Create time index\n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=duration_hours)\n",
    "    timestamps = pd.date_range(start=start_time, end=end_time, periods=num_points)\n",
    "    \n",
    "    # Generate base pattern based on metric type\n",
    "    if 'cpu' in metric_name.lower():\n",
    "        # CPU usage: daily pattern with some randomness\n",
    "        base_pattern = 30 + 20 * np.sin(2 * np.pi * np.arange(num_points) / (24 * 60))  # Daily cycle\n",
    "        noise = np.random.normal(0, 5, num_points)\n",
    "        values = np.clip(base_pattern + noise, 0, 100)\n",
    "        \n",
    "    elif 'memory' in metric_name.lower():\n",
    "        # Memory usage: gradual increase with occasional drops\n",
    "        trend = np.linspace(40, 70, num_points)\n",
    "        noise = np.random.normal(0, 3, num_points)\n",
    "        # Occasional memory cleanup events\n",
    "        cleanup_events = np.random.choice([0, -20], num_points, p=[0.99, 0.01])\n",
    "        values = np.clip(trend + noise + cleanup_events, 10, 95)\n",
    "        \n",
    "    elif 'restart' in metric_name.lower():\n",
    "        # Container restarts: mostly zero with occasional spikes\n",
    "        values = np.random.poisson(0.1, num_points)\n",
    "        # Add some anomalous restart events\n",
    "        anomaly_indices = np.random.choice(num_points, size=int(num_points * 0.02), replace=False)\n",
    "        values[anomaly_indices] += np.random.poisson(3, len(anomaly_indices))\n",
    "        \n",
    "    else:\n",
    "        # Generic metric: normal distribution with trend\n",
    "        trend = np.linspace(50, 60, num_points)\n",
    "        noise = np.random.normal(0, 10, num_points)\n",
    "        values = np.clip(trend + noise, 0, 100)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'value': values,\n",
    "        'metric': metric_name\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the function\n",
    "sample_metric = generate_synthetic_metrics('node_cpu_utilization', duration_hours=2)\n",
    "print(f\"‚úÖ Generated {len(sample_metric)} data points for sample metric\")\n",
    "print(f\"üìä Value range: {sample_metric['value'].min():.2f} - {sample_metric['value'].max():.2f}\")\n",
    "sample_metric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Pipeline\n",
    "\n",
    "Implement the main data collection pipeline for all metric categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_metrics(duration_hours=24, use_real_data=True):\n",
    "    \"\"\"\n",
    "    Collect all metrics defined in our configuration\n",
    "    Uses real Prometheus data when available, falls back to synthetic data\n",
    "    Tracks data source (REAL vs SYNTHETIC) for each metric\n",
    "    \"\"\"\n",
    "    all_metrics = {}\n",
    "    metrics_source = {}  # Track data source for each metric\n",
    "    \n",
    "    print(\"üîÑ Starting metrics collection...\")\n",
    "    print(f\"üìä Prometheus available: {prometheus_available}\")\n",
    "    print(f\"üìä Use real data: {use_real_data}\")\n",
    "    print()\n",
    "    \n",
    "    # Helper function to collect metric with clear output\n",
    "    def collect_metric(metric_name, query, category_name):\n",
    "        data_source = 'SYNTHETIC'  # Default to synthetic\n",
    "        \n",
    "        if prometheus_available and use_real_data:\n",
    "            # Try to get real data first\n",
    "            df = collect_real_metrics(query, duration_hours)\n",
    "            if df is not None and not df.empty:\n",
    "                data_source = 'REAL'\n",
    "                metrics_source[metric_name] = data_source\n",
    "                print(f\"  ‚úÖ [{data_source:8}] {metric_name}: {len(df)} data points\")\n",
    "                return df\n",
    "        \n",
    "        # Fallback to synthetic data\n",
    "        df = generate_synthetic_metrics(metric_name, duration_hours)\n",
    "        metrics_source[metric_name] = data_source\n",
    "        print(f\"  ‚ö†Ô∏è  [{data_source:8}] {metric_name}: {len(df)} data points (Prometheus unavailable)\")\n",
    "        return df\n",
    "    \n",
    "    # Collect infrastructure metrics\n",
    "    print(\"üèóÔ∏è Collecting infrastructure metrics...\")\n",
    "    for metric_name, query in INFRASTRUCTURE_METRICS.items():\n",
    "        df = collect_metric(metric_name, query, \"infrastructure\")\n",
    "        all_metrics[metric_name] = df\n",
    "    \n",
    "    # Collect application metrics\n",
    "    print(\"üöÄ Collecting application metrics...\")\n",
    "    for metric_name, query in APPLICATION_METRICS.items():\n",
    "        df = collect_metric(metric_name, query, \"application\")\n",
    "        all_metrics[metric_name] = df\n",
    "    \n",
    "    # Collect cluster metrics\n",
    "    print(\"üèóÔ∏è Collecting cluster metrics...\")\n",
    "    for metric_name, query in CLUSTER_METRICS.items():\n",
    "        df = collect_metric(metric_name, query, \"cluster\")\n",
    "        all_metrics[metric_name] = df\n",
    "    \n",
    "    # Summary statistics\n",
    "    real_count = sum(1 for source in metrics_source.values() if source == 'REAL')\n",
    "    synthetic_count = sum(1 for source in metrics_source.values() if source == 'SYNTHETIC')\n",
    "    \n",
    "    print(f\"\\nüéâ Collection complete!\")\n",
    "    print(f\"   Total metrics: {len(all_metrics)}\")\n",
    "    print(f\"   ‚úÖ REAL data: {real_count} metrics\")\n",
    "    print(f\"   ‚ö†Ô∏è  SYNTHETIC data: {synthetic_count} metrics\")\n",
    "    \n",
    "    if synthetic_count > 0:\n",
    "        print(f\"\\n   ‚ö†Ô∏è  Note: {synthetic_count} metrics using synthetic data (Prometheus unavailable for those queries)\")\n",
    "    \n",
    "    # Store source information in the returned dict for reference\n",
    "    all_metrics['_metadata'] = {'sources': metrics_source}\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "# Collect metrics for the last 24 hours\n",
    "# This will use real Prometheus data when running in the cluster\n",
    "metrics_data = collect_all_metrics(duration_hours=24, use_real_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data source summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DATA SOURCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract metadata\n",
    "metadata = metrics_data.pop('_metadata', {})\n",
    "sources = metadata.get('sources', {})\n",
    "\n",
    "# Create summary dataframe\n",
    "source_summary = pd.DataFrame([\n",
    "    {'Metric': metric, 'Data Source': source}\n",
    "    for metric, source in sources.items()\n",
    "])\n",
    "\n",
    "# Display by source type\n",
    "print(\"\\nüéØ REAL DATA METRICS:\")\n",
    "real_metrics = source_summary[source_summary['Data Source'] == 'REAL']\n",
    "if len(real_metrics) > 0:\n",
    "    for idx, row in real_metrics.iterrows():\n",
    "        print(f\"  ‚úÖ {row['Metric']}\")\n",
    "else:\n",
    "    print(\"  ‚ùå No real data collected\")\n",
    "\n",
    "print(\"\\nüîÑ SYNTHETIC DATA METRICS:\")\n",
    "synthetic_metrics = source_summary[source_summary['Data Source'] == 'SYNTHETIC']\n",
    "if len(synthetic_metrics) > 0:\n",
    "    for idx, row in synthetic_metrics.iterrows():\n",
    "        print(f\"  üìä {row['Metric']}\")\n",
    "else:\n",
    "    print(\"  ‚úÖ All metrics using real data!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Total Metrics: {len(source_summary)}\")\n",
    "print(f\"Real Data: {len(real_metrics)} ({len(real_metrics)/len(source_summary)*100:.1f}%)\")\n",
    "print(f\"Synthetic Data: {len(synthetic_metrics)} ({len(synthetic_metrics)/len(source_summary)*100:.1f}%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display full table\n",
    "print(\"\\nüìã DETAILED BREAKDOWN:\")\n",
    "print(source_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Validation\n",
    "\n",
    "Implement quality checks as defined in ADR-013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_quality(metrics_data):\n",
    "    \"\"\"\n",
    "    Perform data quality validation checks\n",
    "    \"\"\"\n",
    "    quality_report = {\n",
    "        'total_metrics': len(metrics_data),\n",
    "        'quality_scores': {},\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    print(\"üîç Performing data quality validation...\")\n",
    "    \n",
    "    for metric_name, df in metrics_data.items():\n",
    "        metric_quality = {\n",
    "            'completeness': 0,\n",
    "            'consistency': 0,\n",
    "            'accuracy': 0\n",
    "        }\n",
    "        \n",
    "        # Completeness check\n",
    "        missing_ratio = df['value'].isnull().sum() / len(df)\n",
    "        metric_quality['completeness'] = max(0, 1 - missing_ratio * 20)  # Penalize missing values\n",
    "        \n",
    "        if missing_ratio > 0.05:\n",
    "            quality_report['issues'].append(f\"{metric_name}: High missing values ({missing_ratio:.2%})\")\n",
    "        \n",
    "        # Consistency check (time gaps)\n",
    "        time_diffs = df['timestamp'].diff().dt.total_seconds().dropna()\n",
    "        expected_interval = time_diffs.median()\n",
    "        large_gaps = (time_diffs > expected_interval * 2).sum()\n",
    "        metric_quality['consistency'] = max(0, 1 - large_gaps / len(time_diffs))\n",
    "        \n",
    "        if large_gaps > len(time_diffs) * 0.01:\n",
    "            quality_report['issues'].append(f\"{metric_name}: Time gaps detected ({large_gaps} gaps)\")\n",
    "        \n",
    "        # Accuracy check (outlier detection)\n",
    "        Q1 = df['value'].quantile(0.25)\n",
    "        Q3 = df['value'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((df['value'] < Q1 - 1.5 * IQR) | (df['value'] > Q3 + 1.5 * IQR)).sum()\n",
    "        outlier_ratio = outliers / len(df)\n",
    "        metric_quality['accuracy'] = max(0, 1 - outlier_ratio * 10)  # Penalize outliers\n",
    "        \n",
    "        if outlier_ratio > 0.05:\n",
    "            quality_report['issues'].append(f\"{metric_name}: High outlier ratio ({outlier_ratio:.2%})\")\n",
    "        \n",
    "        # Overall quality score\n",
    "        overall_score = np.mean(list(metric_quality.values()))\n",
    "        quality_report['quality_scores'][metric_name] = {\n",
    "            'overall': overall_score,\n",
    "            'details': metric_quality\n",
    "        }\n",
    "    \n",
    "    # Calculate average quality score\n",
    "    avg_quality = np.mean([score['overall'] for score in quality_report['quality_scores'].values()])\n",
    "    quality_report['average_quality'] = avg_quality\n",
    "    \n",
    "    print(f\"üìä Average data quality score: {avg_quality:.2f}\")\n",
    "    print(f\"‚ö†Ô∏è Issues found: {len(quality_report['issues'])}\")\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Validate data quality\n",
    "quality_report = validate_data_quality(metrics_data)\n",
    "\n",
    "# Display quality summary\n",
    "print(\"\\nüìã Quality Summary:\")\n",
    "for metric, scores in list(quality_report['quality_scores'].items())[:5]:  # Show first 5\n",
    "    print(f\"  {metric}: {scores['overall']:.2f}\")\n",
    "\n",
    "if quality_report['issues']:\n",
    "    print(\"\\n‚ö†Ô∏è Issues to address:\")\n",
    "    for issue in quality_report['issues'][:3]:  # Show first 3\n",
    "        print(f\"  - {issue}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

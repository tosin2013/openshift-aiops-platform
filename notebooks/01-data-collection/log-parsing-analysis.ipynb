{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Container Log Parsing and Analysis for Self-Healing Platform\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to collect, parse, and analyze container logs from OpenShift for anomaly detection and troubleshooting. It includes structured log parsing (JSON), pattern recognition, and integration with the self-healing coordination engine.\n",
    "\n",
    "## Prerequisites\n",
    "- Access to OpenShift cluster with log collection permissions\n",
    "- Kubernetes Python client installed\n",
    "- Running coordination engine in the cluster\n",
    "- Persistent storage for log data\n",
    "\n",
    "## Expected Outcomes\n",
    "- Understand container log collection from OpenShift\n",
    "- Implement structured log parsing for JSON and text formats\n",
    "- Identify error patterns and anomalies in application logs\n",
    "- Extract actionable insights for self-healing workflows\n",
    "\n",
    "## References\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- ADR-013: Data Collection and Preprocessing Workflows\n",
    "- OpenShift Logging Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport json\nimport re\nimport warnings\nimport sys\nimport os\nfrom collections import Counter, defaultdict\nfrom typing import Dict, List, Optional, Any\nfrom pathlib import Path\n\n# Kubernetes client\ntry:\n    from kubernetes import client, config\n    k8s_available = True\n    print(\"‚úÖ Kubernetes client available\")\nexcept ImportError:\n    k8s_available = False\n    print(\"‚ö†Ô∏è Kubernetes client not available - using simulation mode\")\n\n# Setup path for utils module - works from any directory\n# Find the notebooks directory by looking for known structure\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    # Try multiple possible locations\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    \n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    \n    # Fallback: search upward from cwd\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    \n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"‚úÖ Utils path found: {utils_path}\")\nelse:\n    print(\"‚ö†Ô∏è Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import (\n        setup_environment, print_environment_info,\n        save_processed_data, load_processed_data,\n        validate_data_quality\n    )\n    print(\"‚úÖ Common functions imported\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Common functions not available: {e}\")\n    print(\"   Using minimal fallback implementations\")\n    \n    # Minimal fallback implementations\n    def setup_environment():\n        return {\n            'data_dir': '/opt/app-root/src/data',\n            'models_dir': '/opt/app-root/src/models',\n            'working_dir': os.getcwd()\n        }\n    \n    def print_environment_info(env_info):\n        print(f\"üìÅ Data dir: {env_info.get('data_dir', 'N/A')}\")\n        print(f\"üìÅ Models dir: {env_info.get('models_dir', 'N/A')}\")\n    \n    def save_processed_data(data, filename):\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        filepath = f'/opt/app-root/src/data/processed/{filename}'\n        if filename.endswith('.parquet') and hasattr(data, 'to_parquet'):\n            data.to_parquet(filepath)\n        elif filename.endswith('.json'):\n            with open(filepath, 'w') as f:\n                # Handle non-serializable types\n                if hasattr(data, 'items'):\n                    serializable = {}\n                    for k, v in data.items():\n                        if hasattr(v, 'to_dict'):\n                            serializable[k] = v.to_dict()\n                        else:\n                            serializable[k] = v\n                    json.dump(serializable, f, default=str)\n                else:\n                    json.dump(data, f, default=str)\n        print(f\"üíæ Saved: {filepath}\")\n    \n    def load_processed_data(filename):\n        filepath = f'/opt/app-root/src/data/processed/{filename}'\n        if filename.endswith('.parquet'):\n            return pd.read_parquet(filepath)\n        elif filename.endswith('.json'):\n            with open(filepath, 'r') as f:\n                return json.load(f)\n        return None\n    \n    def validate_data_quality(df):\n        return {'valid': True, 'issues': []}\n\nprint(\"‚úÖ Libraries imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup & Configuration\n\nInitialize the Kubernetes client and configure parameters for log collection from container pods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "env_info = setup_environment()\n",
    "print_environment_info(env_info)\n",
    "\n",
    "# Log Collection Configuration\n",
    "LOG_CONFIG = {\n",
    "    'target_namespaces': ['self-healing-platform', 'openshift-monitoring'],\n",
    "    'target_pods': ['coordination-engine', 'cluster-health-mcp-server'],\n",
    "    'log_lines_limit': 1000,  # Maximum lines per pod\n",
    "    'time_window_hours': 24,  # Look back window\n",
    "    'log_levels': ['ERROR', 'WARN', 'INFO', 'DEBUG'],\n",
    "    'structured_formats': ['json', 'logfmt'],\n",
    "    'error_patterns': [\n",
    "        r'(?i)error|exception|failed|timeout|refused',\n",
    "        r'(?i)panic|fatal|critical|emergency',\n",
    "        r'(?i)connection.*(?:refused|timeout|reset)',\n",
    "        r'(?i)out of memory|oom|memory.*exceeded'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"üìã Log collection configured for {LOG_CONFIG['time_window_hours']} hours\")\n",
    "print(f\"üéØ Target namespaces: {', '.join(LOG_CONFIG['target_namespaces'])}\")\n",
    "print(f\"üîç Error patterns: {len(LOG_CONFIG['error_patterns'])} defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Log Collection Functions\n\nDefine helper functions to collect logs from Kubernetes pods or generate synthetic logs for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_kubernetes_client():\n",
    "    \"\"\"\n",
    "    Set up Kubernetes client with in-cluster configuration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try in-cluster config first\n",
    "        config.load_incluster_config()\n",
    "        print(\"‚úÖ Using in-cluster Kubernetes configuration\")\n",
    "    except:\n",
    "        try:\n",
    "            # Fallback to local kubeconfig\n",
    "            config.load_kube_config()\n",
    "            print(\"‚úÖ Using local kubeconfig\")\n",
    "        except:\n",
    "            print(\"‚ùå Failed to load Kubernetes configuration\")\n",
    "            return None\n",
    "    \n",
    "    return client.CoreV1Api()\n",
    "\n",
    "def collect_pod_logs(namespace, pod_name, container_name=None, lines=1000):\n",
    "    \"\"\"\n",
    "    Collect logs from a specific pod/container\n",
    "    \n",
    "    Args:\n",
    "        namespace: Kubernetes namespace\n",
    "        pod_name: Pod name\n",
    "        container_name: Container name (optional)\n",
    "        lines: Number of log lines to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of log lines with metadata\n",
    "    \"\"\"\n",
    "    if not k8s_available:\n",
    "        return generate_synthetic_logs(pod_name, lines)\n",
    "    \n",
    "    v1 = setup_kubernetes_client()\n",
    "    if v1 is None:\n",
    "        return generate_synthetic_logs(pod_name, lines)\n",
    "    \n",
    "    try:\n",
    "        # Get pod logs\n",
    "        log_response = v1.read_namespaced_pod_log(\n",
    "            name=pod_name,\n",
    "            namespace=namespace,\n",
    "            container=container_name,\n",
    "            tail_lines=lines,\n",
    "            timestamps=True\n",
    "        )\n",
    "        \n",
    "        # Parse log lines\n",
    "        log_entries = []\n",
    "        for line in log_response.split('\\n'):\n",
    "            if line.strip():\n",
    "                # Try to extract timestamp if present\n",
    "                timestamp_match = re.match(r'^(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d+Z)\\s+(.*)$', line)\n",
    "                if timestamp_match:\n",
    "                    timestamp_str, message = timestamp_match.groups()\n",
    "                    timestamp = pd.to_datetime(timestamp_str)\n",
    "                else:\n",
    "                    timestamp = datetime.now()\n",
    "                    message = line\n",
    "                \n",
    "                log_entries.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'namespace': namespace,\n",
    "                    'pod_name': pod_name,\n",
    "                    'container_name': container_name or 'unknown',\n",
    "                    'message': message,\n",
    "                    'raw_line': line\n",
    "                })\n",
    "        \n",
    "        print(f\"  ‚úÖ Collected {len(log_entries)} log lines from {pod_name}\")\n",
    "        return log_entries\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Failed to collect logs from {pod_name}: {e}\")\n",
    "        return generate_synthetic_logs(pod_name, lines)\n",
    "\n",
    "def collect_logs_from_namespace(namespace, config):\n",
    "    \"\"\"\n",
    "    Collect logs from all relevant pods in a namespace\n",
    "    \"\"\"\n",
    "    print(f\"üì° Collecting logs from namespace: {namespace}\")\n",
    "    \n",
    "    if not k8s_available:\n",
    "        return generate_synthetic_namespace_logs(namespace, config)\n",
    "    \n",
    "    v1 = setup_kubernetes_client()\n",
    "    if v1 is None:\n",
    "        return generate_synthetic_namespace_logs(namespace, config)\n",
    "    \n",
    "    all_logs = []\n",
    "    \n",
    "    try:\n",
    "        # Get all pods in namespace\n",
    "        pods = v1.list_namespaced_pod(namespace=namespace)\n",
    "        \n",
    "        for pod in pods.items:\n",
    "            pod_name = pod.metadata.name\n",
    "            \n",
    "            # Filter by target pods if specified\n",
    "            if config['target_pods']:\n",
    "                if not any(target in pod_name for target in config['target_pods']):\n",
    "                    continue\n",
    "            \n",
    "            # Collect logs from each container\n",
    "            for container in pod.spec.containers:\n",
    "                container_logs = collect_pod_logs(\n",
    "                    namespace, pod_name, container.name, config['log_lines_limit']\n",
    "                )\n",
    "                all_logs.extend(container_logs)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Failed to list pods in {namespace}: {e}\")\n",
    "        return generate_synthetic_namespace_logs(namespace, config)\n",
    "    \n",
    "    print(f\"  üìä Total logs collected: {len(all_logs)}\")\n",
    "    return all_logs\n",
    "\n",
    "# Test Kubernetes connection\n",
    "if k8s_available:\n",
    "    v1_test = setup_kubernetes_client()\n",
    "    if v1_test:\n",
    "        print(\"üîó Kubernetes client connection successful\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Kubernetes client connection failed - will use synthetic data\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kubernetes client not available - will use synthetic data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Synthetic Logs\n\nGenerate realistic synthetic container logs with various log levels and error patterns for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_logs(pod_name, lines=1000):\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic container logs for testing\n",
    "    \"\"\"\n",
    "    log_patterns = {\n",
    "        'INFO': [\n",
    "            'Starting application server on port 8080',\n",
    "            'Database connection established',\n",
    "            'Processing request from user {}',\n",
    "            'Cache hit for key: {}',\n",
    "            'Health check passed'\n",
    "        ],\n",
    "        'WARN': [\n",
    "            'High memory usage detected: {}%',\n",
    "            'Slow query detected: {}ms',\n",
    "            'Connection pool nearly exhausted',\n",
    "            'Deprecated API endpoint accessed'\n",
    "        ],\n",
    "        'ERROR': [\n",
    "            'Failed to connect to database: connection timeout',\n",
    "            'Exception in request handler: {}',\n",
    "            'Authentication failed for user {}',\n",
    "            'Out of memory error in worker process',\n",
    "            'Network connection refused'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    log_entries = []\n",
    "    start_time = datetime.now() - timedelta(hours=24)\n",
    "    \n",
    "    for i in range(lines):\n",
    "        # Weight log levels realistically\n",
    "        level = np.random.choice(['INFO', 'WARN', 'ERROR'], p=[0.7, 0.2, 0.1])\n",
    "        pattern = np.random.choice(log_patterns[level])\n",
    "        \n",
    "        # Add realistic values to patterns\n",
    "        if '{}' in pattern:\n",
    "            if 'user' in pattern:\n",
    "                value = f\"user{np.random.randint(1000, 9999)}\"\n",
    "            elif '%' in pattern:\n",
    "                value = np.random.randint(70, 95)\n",
    "            elif 'ms' in pattern:\n",
    "                value = np.random.randint(1000, 5000)\n",
    "            else:\n",
    "                value = f\"value{np.random.randint(100, 999)}\"\n",
    "            message = pattern.format(value)\n",
    "        else:\n",
    "            message = pattern\n",
    "        \n",
    "        timestamp = start_time + timedelta(\n",
    "            seconds=np.random.uniform(0, 24*3600)\n",
    "        )\n",
    "        \n",
    "        # Create structured log entry\n",
    "        if np.random.random() < 0.3:  # 30% JSON logs\n",
    "            json_log = {\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'level': level,\n",
    "                'message': message,\n",
    "                'component': pod_name,\n",
    "                'thread': f\"thread-{np.random.randint(1, 10)}\"\n",
    "            }\n",
    "            raw_line = json.dumps(json_log)\n",
    "        else:  # 70% plain text logs\n",
    "            raw_line = f\"{timestamp.isoformat()} [{level}] {message}\"\n",
    "        \n",
    "        log_entries.append({\n",
    "            'timestamp': timestamp,\n",
    "            'namespace': 'synthetic',\n",
    "            'pod_name': pod_name,\n",
    "            'container_name': 'main',\n",
    "            'message': message,\n",
    "            'raw_line': raw_line,\n",
    "            'level': level\n",
    "        })\n",
    "    \n",
    "    return log_entries\n",
    "\n",
    "def generate_synthetic_namespace_logs(namespace, config):\n",
    "    \"\"\"\n",
    "    Generate synthetic logs for an entire namespace\n",
    "    \"\"\"\n",
    "    all_logs = []\n",
    "    \n",
    "    # Generate logs for target pods or default pods\n",
    "    pods = config['target_pods'] if config['target_pods'] else ['app-server', 'worker', 'cache']\n",
    "    \n",
    "    for pod_name in pods:\n",
    "        pod_logs = generate_synthetic_logs(f\"{pod_name}-{np.random.randint(1000, 9999)}\", \n",
    "                                         config['log_lines_limit'] // len(pods))\n",
    "        # Update namespace\n",
    "        for log in pod_logs:\n",
    "            log['namespace'] = namespace\n",
    "        all_logs.extend(pod_logs)\n",
    "    \n",
    "    return all_logs\n",
    "\n",
    "# Collect logs from target namespaces\n",
    "print(\"üöÄ Starting log collection...\")\n",
    "all_logs = []\n",
    "\n",
    "for namespace in LOG_CONFIG['target_namespaces']:\n",
    "    namespace_logs = collect_logs_from_namespace(namespace, LOG_CONFIG)\n",
    "    all_logs.extend(namespace_logs)\n",
    "\n",
    "# Convert to DataFrame\n",
    "logs_df = pd.DataFrame(all_logs)\n",
    "logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'], utc=True)\n",
    "logs_df['timestamp'] = logs_df['timestamp'].dt.tz_localize(None)\n",
    "logs_df = logs_df.sort_values('timestamp')\n",
    "\n",
    "print(f\"\\nüìä Log Collection Summary:\")\n",
    "print(f\"Total log entries: {len(logs_df)}\")\n",
    "print(f\"Time range: {logs_df['timestamp'].min()} to {logs_df['timestamp'].max()}\")\n",
    "print(f\"Namespaces: {logs_df['namespace'].nunique()}\")\n",
    "print(f\"Unique pods: {logs_df['pod_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parse and Analyze Logs\n\nParse structured logs, extract error patterns, and identify anomalies in application behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_structured_logs(logs_df):\n",
    "    \"\"\"\n",
    "    Parse structured logs (JSON, logfmt) and extract structured data\n",
    "    \"\"\"\n",
    "    print(\"üîç Parsing structured logs...\")\n",
    "    \n",
    "    parsed_logs = []\n",
    "    \n",
    "    for idx, row in logs_df.iterrows():\n",
    "        log_entry = row.to_dict()\n",
    "        raw_line = row['raw_line']\n",
    "        \n",
    "        # Try to parse as JSON\n",
    "        try:\n",
    "            json_data = json.loads(raw_line)\n",
    "            log_entry.update({\n",
    "                'format': 'json',\n",
    "                'level': json_data.get('level', 'INFO'),\n",
    "                'component': json_data.get('component', 'unknown'),\n",
    "                'thread': json_data.get('thread', 'main'),\n",
    "                'structured': True\n",
    "            })\n",
    "            if 'message' not in log_entry or not log_entry['message']:\n",
    "                log_entry['message'] = json_data.get('message', raw_line)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            # Try to extract log level from plain text\n",
    "            level_match = re.search(r'\\[(DEBUG|INFO|WARN|ERROR|FATAL)\\]', raw_line, re.IGNORECASE)\n",
    "            if level_match:\n",
    "                log_entry.update({\n",
    "                    'format': 'text',\n",
    "                    'level': level_match.group(1).upper(),\n",
    "                    'structured': False\n",
    "                })\n",
    "            else:\n",
    "                log_entry.update({\n",
    "                    'format': 'text',\n",
    "                    'level': 'INFO',\n",
    "                    'structured': False\n",
    "                })\n",
    "        \n",
    "        parsed_logs.append(log_entry)\n",
    "    \n",
    "    parsed_df = pd.DataFrame(parsed_logs)\n",
    "    print(f\"‚úÖ Parsed {len(parsed_df)} log entries\")\n",
    "    print(f\"Structured logs: {parsed_df['structured'].sum()}\")\n",
    "    print(f\"Log levels found: {', '.join(parsed_df['level'].unique())}\")\n",
    "    \n",
    "    return parsed_df\n",
    "\n",
    "def detect_error_patterns(logs_df, patterns):\n",
    "    \"\"\"\n",
    "    Detect error patterns in log messages\n",
    "    \"\"\"\n",
    "    print(\"üö® Detecting error patterns...\")\n",
    "    \n",
    "    error_matches = []\n",
    "    \n",
    "    for idx, row in logs_df.iterrows():\n",
    "        message = row['message']\n",
    "        raw_line = row['raw_line']\n",
    "        \n",
    "        for pattern_idx, pattern in enumerate(patterns):\n",
    "            if re.search(pattern, message, re.IGNORECASE) or re.search(pattern, raw_line, re.IGNORECASE):\n",
    "                error_matches.append({\n",
    "                    'log_index': idx,\n",
    "                    'timestamp': row['timestamp'],\n",
    "                    'namespace': row['namespace'],\n",
    "                    'pod_name': row['pod_name'],\n",
    "                    'level': row.get('level', 'UNKNOWN'),\n",
    "                    'pattern_index': pattern_idx,\n",
    "                    'pattern': pattern,\n",
    "                    'matched_text': message,\n",
    "                    'severity': 'HIGH' if pattern_idx < 2 else 'MEDIUM'\n",
    "                })\n",
    "    \n",
    "    error_df = pd.DataFrame(error_matches)\n",
    "    print(f\"‚úÖ Found {len(error_df)} error pattern matches\")\n",
    "    \n",
    "    if len(error_df) > 0:\n",
    "        print(f\"High severity errors: {len(error_df[error_df['severity'] == 'HIGH'])}\")\n",
    "        print(f\"Most common pattern: {error_df['pattern_index'].mode().iloc[0] if not error_df.empty else 'None'}\")\n",
    "    \n",
    "    return error_df\n",
    "\n",
    "def analyze_log_patterns(logs_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in log data for insights\n",
    "    \"\"\"\n",
    "    print(\"üìä Analyzing log patterns...\")\n",
    "    \n",
    "    # Time-based analysis\n",
    "    logs_df['hour'] = logs_df['timestamp'].dt.hour\n",
    "    logs_df['day_of_week'] = logs_df['timestamp'].dt.day_name()\n",
    "    \n",
    "    patterns = {\n",
    "        'log_levels': logs_df['level'].value_counts(),\n",
    "        'pods': logs_df['pod_name'].value_counts(),\n",
    "        'namespaces': logs_df['namespace'].value_counts(),\n",
    "        'hourly_distribution': logs_df['hour'].value_counts().sort_index(),\n",
    "        'daily_distribution': logs_df['day_of_week'].value_counts(),\n",
    "        'structured_percentage': logs_df['structured'].mean() * 100 if 'structured' in logs_df.columns else 0\n",
    "    }\n",
    "    \n",
    "    # Error rate analysis\n",
    "    error_logs = logs_df[logs_df['level'].isin(['ERROR', 'FATAL'])]\n",
    "    patterns['error_rate'] = len(error_logs) / len(logs_df) * 100\n",
    "    patterns['errors_by_pod'] = error_logs['pod_name'].value_counts()\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Parse structured logs\n",
    "parsed_logs_df = parse_structured_logs(logs_df)\n",
    "\n",
    "# Detect error patterns\n",
    "error_matches_df = detect_error_patterns(parsed_logs_df, LOG_CONFIG['error_patterns'])\n",
    "\n",
    "# Analyze patterns\n",
    "log_patterns = analyze_log_patterns(parsed_logs_df)\n",
    "\n",
    "print(f\"\\nüìà Log Analysis Summary:\")\n",
    "print(f\"Error rate: {log_patterns['error_rate']:.2f}%\")\n",
    "print(f\"Structured logs: {log_patterns['structured_percentage']:.1f}%\")\n",
    "print(f\"Most active pod: {log_patterns['pods'].index[0]} ({log_patterns['pods'].iloc[0]} logs)\")\n",
    "print(f\"Error pattern matches: {len(error_matches_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Log Analysis Results\n\nCreate visualizations of log patterns, error frequencies, and anomalies for better understanding of application health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize log analysis results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Container Log Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Log levels distribution\n",
    "log_patterns['log_levels'].plot(kind='bar', ax=axes[0,0], color=['green', 'orange', 'red', 'darkred'])\n",
    "axes[0,0].set_title('Log Levels Distribution')\n",
    "axes[0,0].set_xlabel('Log Level')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Top pods by log volume\n",
    "log_patterns['pods'].head(10).plot(kind='barh', ax=axes[0,1])\n",
    "axes[0,1].set_title('Top 10 Pods by Log Volume')\n",
    "axes[0,1].set_xlabel('Log Count')\n",
    "\n",
    "# Hourly log distribution\n",
    "log_patterns['hourly_distribution'].plot(kind='line', ax=axes[1,0], marker='o')\n",
    "axes[1,0].set_title('Log Volume by Hour of Day')\n",
    "axes[1,0].set_xlabel('Hour')\n",
    "axes[1,0].set_ylabel('Log Count')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error patterns over time (if errors found)\n",
    "if len(error_matches_df) > 0:\n",
    "    error_matches_df.set_index('timestamp')['severity'].resample('1H').count().plot(\n",
    "        kind='line', ax=axes[1,1], marker='o', color='red'\n",
    "    )\n",
    "    axes[1,1].set_title('Error Patterns Over Time')\n",
    "    axes[1,1].set_xlabel('Time')\n",
    "    axes[1,1].set_ylabel('Error Count')\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'No Error Patterns\\nDetected', \n",
    "                   ha='center', va='center', transform=axes[1,1].transAxes,\n",
    "                   fontsize=14, color='green')\n",
    "    axes[1,1].set_title('Error Patterns Over Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save processed log data\n",
    "save_processed_data(parsed_logs_df, 'container_logs_parsed.parquet')\n",
    "save_processed_data(error_matches_df, 'log_error_patterns.parquet')\n",
    "save_processed_data(log_patterns, 'log_analysis_patterns.json')\n",
    "\n",
    "print(\"\\nüíæ Data saved successfully:\")\n",
    "print(\"- container_logs_parsed.parquet: Parsed log data with structure\")\n",
    "print(\"- log_error_patterns.parquet: Detected error patterns\")\n",
    "print(\"- log_analysis_patterns.json: Log pattern analysis results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Coordination Engine\n",
    "\n",
    "This section demonstrates how to integrate log analysis with the self-healing coordination engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import MCP client for coordination engine integration\n# Use the same path finding logic as cell-1\ntry:\n    from mcp_client import get_cluster_health_client\n    mcp_available = True\n    print(\"‚úÖ MCP client imported\")\nexcept ImportError:\n    mcp_available = False\n    print(\"‚ö†Ô∏è MCP client not available - using simulation mode\")\n    \n    # Fallback MCP client simulation\n    class SimulatedMCPClient:\n        def query_anomaly_patterns(self, data):\n            return {\n                'status': 'simulated',\n                'simulated': True,\n                'anomalies': [],\n                'message': 'MCP client not available - using simulation'\n            }\n    \n    def get_cluster_health_client():\n        return SimulatedMCPClient()\n\ndef send_log_analysis_to_coordination_engine(logs_df, error_matches_df, patterns):\n    \"\"\"\n    Send log analysis results to coordination engine for action\n    \"\"\"\n    print(\"üîó Integrating with coordination engine...\")\n    \n    # Get Cluster Health MCP client\n    mcp_client = get_cluster_health_client()\n    \n    # Prepare log analysis summary\n    log_summary = {\n        'timestamp': datetime.now().isoformat(),\n        'total_logs': len(logs_df),\n        'error_rate': patterns['error_rate'],\n        'structured_percentage': patterns['structured_percentage'],\n        'error_patterns_found': len(error_matches_df),\n        'top_error_pods': patterns['errors_by_pod'].head(5).to_dict() if len(patterns['errors_by_pod']) > 0 else {},\n        'log_levels': patterns['log_levels'].to_dict(),\n        'anomaly_indicators': {\n            'high_error_rate': patterns['error_rate'] > 5.0,\n            'critical_errors_present': len(error_matches_df[error_matches_df['severity'] == 'HIGH']) > 0 if len(error_matches_df) > 0 else False,\n            'pod_concentration': patterns['pods'].iloc[0] / len(logs_df) > 0.5 if len(patterns['pods']) > 0 else False\n        }\n    }\n    \n    # Send to coordination engine\n    try:\n        response = mcp_client.query_anomaly_patterns({\n            'source': 'container-logs',\n            'data': log_summary,\n            'severity': 'high' if log_summary['anomaly_indicators']['critical_errors_present'] else 'medium'\n        })\n        \n        print(f\"‚úÖ Log analysis sent to coordination engine\")\n        if response.get('simulated'):\n            print(f\"   (Using simulation mode - MCP server not available)\")\n        print(f\"   Anomalies detected: {len(response.get('anomalies', []))}\")\n        return response\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Failed to send to coordination engine: {e}\")\n        return {'status': 'failed', 'error': str(e), 'anomalies': []}\n\ndef generate_log_based_alerts(error_matches_df, patterns, logs_df):\n    \"\"\"\n    Generate actionable alerts based on log analysis\n    \"\"\"\n    print(\"üö® Generating log-based alerts...\")\n    \n    alerts = []\n    \n    # High error rate alert\n    if patterns['error_rate'] > 5.0:\n        alerts.append({\n            'type': 'high_error_rate',\n            'severity': 'HIGH',\n            'message': f\"Error rate is {patterns['error_rate']:.2f}%, exceeding 5% threshold\",\n            'recommended_action': 'Investigate error patterns and consider scaling or restarting affected pods'\n        })\n    \n    # Critical error patterns alert\n    if len(error_matches_df) > 0:\n        critical_errors = error_matches_df[error_matches_df['severity'] == 'HIGH']\n        if len(critical_errors) > 0:\n            alerts.append({\n                'type': 'critical_error_patterns',\n                'severity': 'CRITICAL',\n                'message': f\"Found {len(critical_errors)} critical error patterns\",\n                'affected_pods': critical_errors['pod_name'].unique().tolist(),\n                'recommended_action': 'Immediate investigation required for affected pods'\n            })\n    \n    # Pod concentration alert\n    if len(patterns['pods']) > 0 and len(logs_df) > 0:\n        if patterns['pods'].iloc[0] / len(logs_df) > 0.5:\n            alerts.append({\n                'type': 'log_concentration',\n                'severity': 'MEDIUM',\n                'message': f\"Pod {patterns['pods'].index[0]} generating {patterns['pods'].iloc[0]} logs (>50% of total)\",\n                'recommended_action': 'Check if pod is experiencing issues or needs log level adjustment'\n            })\n    \n    print(f\"‚úÖ Generated {len(alerts)} alerts\")\n    return alerts\n\n# Send analysis to coordination engine\ncoordination_response = send_log_analysis_to_coordination_engine(parsed_logs_df, error_matches_df, log_patterns)\n\n# Generate alerts (pass logs_df to avoid NameError)\nalerts = generate_log_based_alerts(error_matches_df, log_patterns, parsed_logs_df)\n\nif alerts:\n    print(\"\\nüö® Generated Alerts:\")\n    for alert in alerts:\n        print(f\"- [{alert['severity']}] {alert['type']}: {alert['message']}\")\n        print(f\"  Action: {alert['recommended_action']}\")\nelse:\n    print(\"\\n‚úÖ No alerts generated - logs appear healthy\")\n\nprint(\"\\nüéØ Next Steps:\")\nprint(\"1. Set up real-time log streaming and analysis\")\nprint(\"2. Configure alerting thresholds based on baseline\")\nprint(\"3. Implement automated remediation for common patterns\")\nprint(\"4. Integrate with anomaly detection models\")\nprint(\"\\nüìö Related Notebooks:\")\nprint(\"- 02-anomaly-detection/: Use log patterns for ML model training\")\nprint(\"- 03-self-healing-logic/: Implement log-driven remediation\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

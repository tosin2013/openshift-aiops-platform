{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenShift Events Analysis for Self-Healing Platform\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to collect, process, and analyze OpenShift events for pattern recognition and anomaly detection. It integrates with the Kubernetes Python client to gather real-time cluster events and process them for the self-healing platform.\n",
    "\n",
    "## Prerequisites\n",
    "- Access to OpenShift cluster with appropriate RBAC permissions\n",
    "- Kubernetes Python client installed\n",
    "- Running coordination engine in the cluster\n",
    "- Persistent storage for event data\n",
    "\n",
    "## Expected Outcomes\n",
    "- Understand OpenShift event structure and types\n",
    "- Implement event filtering and processing pipelines\n",
    "- Identify patterns in cluster events for anomaly detection\n",
    "- Integrate event analysis with coordination engine\n",
    "\n",
    "## References\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- ADR-013: Data Collection and Preprocessing Workflows\n",
    "- Kubernetes Events API Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport json\nimport warnings\nimport sys\nimport os\nfrom collections import Counter, defaultdict\nimport re\nfrom pathlib import Path\n\n# Kubernetes client\ntry:\n    from kubernetes import client, config, watch\n    k8s_available = True\n    print(\"‚úÖ Kubernetes client available\")\nexcept ImportError:\n    k8s_available = False\n    print(\"‚ö†Ô∏è Kubernetes client not available - using simulation mode\")\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n\n    # Fallback: search upward from cwd\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"‚úÖ Utils path found: {utils_path}\")\nelse:\n    print(\"‚ö†Ô∏è Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import (\n        setup_environment, print_environment_info,\n        save_processed_data, load_processed_data,\n        validate_data_quality\n    )\n    print(\"‚úÖ Common functions imported\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Common functions not available: {e}\")\n    print(\"   Using minimal fallback implementations\")\n\n    # Minimal fallback implementations\n    def setup_environment():\n        return {\n            'data_dir': '/opt/app-root/src/data',\n            'models_dir': '/opt/app-root/src/models',\n            'working_dir': os.getcwd()\n        }\n\n    def print_environment_info(env_info):\n        print(f\"üìÅ Data dir: {env_info.get('data_dir', 'N/A')}\")\n        print(f\"üìÅ Models dir: {env_info.get('models_dir', 'N/A')}\")\n\n    def save_processed_data(data, filename):\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        filepath = f'/opt/app-root/src/data/processed/{filename}'\n        if filename.endswith('.parquet') and hasattr(data, 'to_parquet'):\n            data.to_parquet(filepath)\n        elif filename.endswith('.json'):\n            with open(filepath, 'w') as f:\n                if hasattr(data, 'items'):\n                    serializable = {}\n                    for k, v in data.items():\n                        if hasattr(v, 'to_dict'):\n                            serializable[k] = v.to_dict()\n                        else:\n                            serializable[k] = v\n                    json.dump(serializable, f, default=str)\n                else:\n                    json.dump(data, f, default=str)\n        print(f\"üíæ Saved: {filepath}\")\n\n    def load_processed_data(filename):\n        filepath = f'/opt/app-root/src/data/processed/{filename}'\n        if filename.endswith('.parquet'):\n            return pd.read_parquet(filepath)\n        elif filename.endswith('.json'):\n            with open(filepath, 'r') as f:\n                return json.load(f)\n        return None\n\n    def validate_data_quality(df):\n        return {'valid': True, 'issues': []}\n\nprint(\"‚úÖ Libraries imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup & Configuration\n\nInitialize the Kubernetes client and configure parameters for event collection from multiple namespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "env_info = setup_environment()\n",
    "print_environment_info(env_info)\n",
    "\n",
    "# OpenShift Events Configuration\n",
    "EVENTS_CONFIG = {\n",
    "    'collection_duration_minutes': 60,  # How long to collect events\n",
    "    'batch_size': 100,  # Events per batch for processing\n",
    "    'namespaces': ['self-healing-platform', 'openshift-monitoring', 'default'],\n",
    "    'event_types': ['Warning', 'Normal'],\n",
    "    'reasons_of_interest': [\n",
    "        'Failed', 'FailedMount', 'FailedScheduling', 'Unhealthy',\n",
    "        'BackOff', 'Killing', 'Created', 'Started', 'Pulled'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"üìã Events collection configured for {EVENTS_CONFIG['collection_duration_minutes']} minutes\")\n",
    "print(f\"üéØ Monitoring namespaces: {', '.join(EVENTS_CONFIG['namespaces'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Event Collection Functions\n\nDefine helper functions to collect events from Kubernetes API or generate synthetic events for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_kubernetes_client():\n",
    "    \"\"\"\n",
    "    Set up Kubernetes client with in-cluster configuration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try in-cluster config first\n",
    "        config.load_incluster_config()\n",
    "        print(\"‚úÖ Using in-cluster Kubernetes configuration\")\n",
    "    except:\n",
    "        try:\n",
    "            # Fallback to local kubeconfig\n",
    "            config.load_kube_config()\n",
    "            print(\"‚úÖ Using local kubeconfig\")\n",
    "        except:\n",
    "            print(\"‚ùå Failed to load Kubernetes configuration\")\n",
    "            return None\n",
    "    \n",
    "    return client.CoreV1Api()\n",
    "\n",
    "def collect_cluster_events(duration_minutes=60, namespaces=None):\n",
    "    \"\"\"\n",
    "    Collect OpenShift events from specified namespaces\n",
    "    \n",
    "    Args:\n",
    "        duration_minutes: How long to collect events\n",
    "        namespaces: List of namespaces to monitor\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with collected events\n",
    "    \"\"\"\n",
    "    if not k8s_available:\n",
    "        return generate_synthetic_events(duration_minutes)\n",
    "    \n",
    "    v1 = setup_kubernetes_client()\n",
    "    if v1 is None:\n",
    "        return generate_synthetic_events(duration_minutes)\n",
    "    \n",
    "    events_data = []\n",
    "    \n",
    "    try:\n",
    "        # Collect events from each namespace\n",
    "        for namespace in (namespaces or ['default']):\n",
    "            print(f\"üì° Collecting events from namespace: {namespace}\")\n",
    "            \n",
    "            try:\n",
    "                events = v1.list_namespaced_event(namespace=namespace)\n",
    "                \n",
    "                for event in events.items:\n",
    "                    event_data = {\n",
    "                        'timestamp': event.first_timestamp or event.last_timestamp or datetime.now(),\n",
    "                        'namespace': event.metadata.namespace,\n",
    "                        'name': event.metadata.name,\n",
    "                        'type': event.type,\n",
    "                        'reason': event.reason,\n",
    "                        'message': event.message,\n",
    "                        'source_component': event.source.component if event.source else 'unknown',\n",
    "                        'source_host': event.source.host if event.source else 'unknown',\n",
    "                        'involved_object_kind': event.involved_object.kind,\n",
    "                        'involved_object_name': event.involved_object.name,\n",
    "                        'count': event.count or 1\n",
    "                    }\n",
    "                    events_data.append(event_data)\n",
    "                \n",
    "                print(f\"  ‚úÖ Collected {len([e for e in events_data if e['namespace'] == namespace])} events\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Failed to collect from {namespace}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to collect events: {e}\")\n",
    "        return generate_synthetic_events(duration_minutes)\n",
    "    \n",
    "    if not events_data:\n",
    "        print(\"‚ö†Ô∏è No events collected, generating synthetic data\")\n",
    "        return generate_synthetic_events(duration_minutes)\n",
    "    \n",
    "    df = pd.DataFrame(events_data)\n",
    "    # Convert timestamps to datetime, handling both timezone-aware and naive timestamps\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    # Convert to naive UTC for consistency\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize(None)\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    print(f\"üéâ Successfully collected {len(df)} events\")\n",
    "    return df\n",
    "\n",
    "# Test Kubernetes connection\n",
    "if k8s_available:\n",
    "    v1_test = setup_kubernetes_client()\n",
    "    if v1_test:\n",
    "        print(\"üîó Kubernetes client connection successful\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Kubernetes client connection failed - will use synthetic data\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kubernetes client not available - will use synthetic data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Synthetic Events\n\nGenerate realistic synthetic Kubernetes events to simulate cluster activity when real API is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_events(duration_minutes=60):\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic OpenShift events for testing\n",
    "    \"\"\"\n",
    "    print(\"üé≠ Generating synthetic OpenShift events...\")\n",
    "    \n",
    "    # Event patterns based on real OpenShift clusters\n",
    "    event_patterns = {\n",
    "        'Normal': {\n",
    "            'Created': ['Pod', 'Service', 'ConfigMap'],\n",
    "            'Started': ['Pod'],\n",
    "            'Pulled': ['Pod'],\n",
    "            'Scheduled': ['Pod'],\n",
    "            'SuccessfulCreate': ['ReplicaSet', 'Job']\n",
    "        },\n",
    "        'Warning': {\n",
    "            'Failed': ['Pod', 'Job'],\n",
    "            'FailedScheduling': ['Pod'],\n",
    "            'BackOff': ['Pod'],\n",
    "            'Unhealthy': ['Pod'],\n",
    "            'FailedMount': ['Pod']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    namespaces = ['self-healing-platform', 'openshift-monitoring', 'default', 'kube-system']\n",
    "    components = ['kubelet', 'scheduler', 'controller-manager', 'default-scheduler']\n",
    "    \n",
    "    events_data = []\n",
    "    start_time = datetime.now() - timedelta(minutes=duration_minutes)\n",
    "    \n",
    "    # Generate events with realistic patterns\n",
    "    num_events = np.random.randint(50, 200)  # Realistic event count\n",
    "    \n",
    "    for i in range(num_events):\n",
    "        event_type = np.random.choice(['Normal', 'Warning'], p=[0.7, 0.3])\n",
    "        reason = np.random.choice(list(event_patterns[event_type].keys()))\n",
    "        object_kind = np.random.choice(event_patterns[event_type][reason])\n",
    "        \n",
    "        timestamp = start_time + timedelta(\n",
    "            minutes=np.random.uniform(0, duration_minutes)\n",
    "        )\n",
    "        \n",
    "        event_data = {\n",
    "            'timestamp': timestamp,\n",
    "            'namespace': np.random.choice(namespaces),\n",
    "            'name': f\"event-{i:04d}\",\n",
    "            'type': event_type,\n",
    "            'reason': reason,\n",
    "            'message': f\"{reason} event for {object_kind.lower()}\",\n",
    "            'source_component': np.random.choice(components),\n",
    "            'source_host': f\"node-{np.random.randint(1, 5)}\",\n",
    "            'involved_object_kind': object_kind,\n",
    "            'involved_object_name': f\"{object_kind.lower()}-{np.random.randint(1000, 9999)}\",\n",
    "            'count': np.random.randint(1, 5)\n",
    "        }\n",
    "        events_data.append(event_data)\n",
    "    \n",
    "    df = pd.DataFrame(events_data)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(df)} synthetic events\")\n",
    "    return df\n",
    "\n",
    "# Collect events\n",
    "print(\"üöÄ Starting event collection...\")\n",
    "events_df = collect_cluster_events(\n",
    "    duration_minutes=EVENTS_CONFIG['collection_duration_minutes'],\n",
    "    namespaces=EVENTS_CONFIG['namespaces']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Event Collection Summary:\")\n",
    "print(f\"Total events: {len(events_df)}\")\n",
    "print(f\"Time range: {events_df['timestamp'].min()} to {events_df['timestamp'].max()}\")\n",
    "print(f\"Namespaces: {events_df['namespace'].nunique()}\")\n",
    "print(f\"Event types: {', '.join(events_df['type'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Event Patterns\n\nAnalyze collected events to identify patterns, anomalies, and potential issues in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_event_patterns(events_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in OpenShift events for anomaly detection\n",
    "    \"\"\"\n",
    "    print(\"üîç Analyzing event patterns...\")\n",
    "    \n",
    "    # Event frequency analysis\n",
    "    events_df['hour'] = events_df['timestamp'].dt.hour\n",
    "    events_df['day_of_week'] = events_df['timestamp'].dt.day_name()\n",
    "    \n",
    "    # Pattern analysis results\n",
    "    patterns = {\n",
    "        'event_types': events_df['type'].value_counts(),\n",
    "        'reasons': events_df['reason'].value_counts(),\n",
    "        'namespaces': events_df['namespace'].value_counts(),\n",
    "        'object_kinds': events_df['involved_object_kind'].value_counts(),\n",
    "        'hourly_distribution': events_df['hour'].value_counts().sort_index(),\n",
    "        'daily_distribution': events_df['day_of_week'].value_counts()\n",
    "    }\n",
    "    \n",
    "    # Identify anomalous patterns\n",
    "    warning_events = events_df[events_df['type'] == 'Warning']\n",
    "    critical_reasons = ['Failed', 'FailedScheduling', 'BackOff', 'Unhealthy']\n",
    "    critical_events = events_df[events_df['reason'].isin(critical_reasons)]\n",
    "    \n",
    "    patterns['warning_percentage'] = len(warning_events) / len(events_df) * 100\n",
    "    patterns['critical_events'] = len(critical_events)\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def filter_events_for_anomalies(events_df, config):\n",
    "    \"\"\"\n",
    "    Filter events that are relevant for anomaly detection\n",
    "    \"\"\"\n",
    "    print(\"üéØ Filtering events for anomaly detection...\")\n",
    "    \n",
    "    # Filter by event types and reasons of interest\n",
    "    filtered_df = events_df[\n",
    "        (events_df['type'].isin(config['event_types'])) &\n",
    "        (events_df['reason'].isin(config['reasons_of_interest']))\n",
    "    ].copy()\n",
    "    \n",
    "    # Add severity scoring\n",
    "    severity_map = {\n",
    "        'Failed': 5, 'FailedMount': 4, 'FailedScheduling': 4,\n",
    "        'Unhealthy': 4, 'BackOff': 3, 'Killing': 2,\n",
    "        'Created': 1, 'Started': 1, 'Pulled': 1\n",
    "    }\n",
    "    \n",
    "    filtered_df['severity'] = filtered_df['reason'].map(severity_map).fillna(1)\n",
    "    \n",
    "    print(f\"‚úÖ Filtered to {len(filtered_df)} relevant events\")\n",
    "    return filtered_df\n",
    "\n",
    "# Analyze patterns\n",
    "patterns = analyze_event_patterns(events_df)\n",
    "filtered_events = filter_events_for_anomalies(events_df, EVENTS_CONFIG)\n",
    "\n",
    "print(\"\\nüìà Event Pattern Analysis:\")\n",
    "print(f\"Warning events: {patterns['warning_percentage']:.1f}%\")\n",
    "print(f\"Critical events: {patterns['critical_events']}\")\n",
    "print(f\"Most common reason: {patterns['reasons'].index[0]} ({patterns['reasons'].iloc[0]} events)\")\n",
    "print(f\"Most active namespace: {patterns['namespaces'].index[0]} ({patterns['namespaces'].iloc[0]} events)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Event Analysis Results\n\nCreate visualizations of event patterns, anomalies, and alerts for better understanding of cluster health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize event patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('OpenShift Events Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Event types distribution\n",
    "patterns['event_types'].plot(kind='bar', ax=axes[0,0], color=['green', 'orange'])\n",
    "axes[0,0].set_title('Event Types Distribution')\n",
    "axes[0,0].set_xlabel('Event Type')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Top reasons\n",
    "patterns['reasons'].head(10).plot(kind='barh', ax=axes[0,1])\n",
    "axes[0,1].set_title('Top 10 Event Reasons')\n",
    "axes[0,1].set_xlabel('Count')\n",
    "\n",
    "# Hourly distribution\n",
    "patterns['hourly_distribution'].plot(kind='line', ax=axes[1,0], marker='o')\n",
    "axes[1,0].set_title('Events by Hour of Day')\n",
    "axes[1,0].set_xlabel('Hour')\n",
    "axes[1,0].set_ylabel('Event Count')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Namespace distribution\n",
    "patterns['namespaces'].plot(kind='pie', ax=axes[1,1], autopct='%1.1f%%')\n",
    "axes[1,1].set_title('Events by Namespace')\n",
    "axes[1,1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save processed events data\n",
    "save_processed_data(events_df, 'openshift_events_raw.parquet')\n",
    "save_processed_data(filtered_events, 'openshift_events_filtered.parquet')\n",
    "save_processed_data(patterns, 'event_patterns_analysis.json')\n",
    "\n",
    "print(\"\\nüíæ Data saved successfully:\")\n",
    "print(\"- openshift_events_raw.parquet: Raw events data\")\n",
    "print(\"- openshift_events_filtered.parquet: Filtered events for anomaly detection\")\n",
    "print(\"- event_patterns_analysis.json: Pattern analysis results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Coordination Engine\n",
    "\n",
    "This section demonstrates how to integrate event analysis with the self-healing coordination engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import MCP client for coordination engine integration\ntry:\n    from mcp_client import get_cluster_health_client\n    mcp_available = True\n    print(\"‚úÖ MCP client imported\")\nexcept ImportError:\n    mcp_available = False\n    print(\"‚ö†Ô∏è MCP client not available - using simulation mode\")\n\n    # Fallback MCP client simulation\n    class SimulatedMCPClient:\n        def query_anomaly_patterns(self, data):\n            return {\n                'status': 'simulated',\n                'simulated': True,\n                'anomalies': [],\n                'message': 'MCP client not available - using simulation'\n            }\n\n    def get_cluster_health_client():\n        return SimulatedMCPClient()\n\ndef send_events_to_coordination_engine(events_df, patterns):\n    \"\"\"\n    Send processed events and patterns to coordination engine\n    \"\"\"\n    print(\"üîó Integrating with coordination engine...\")\n\n    # Get Cluster Health MCP client\n    mcp_client = get_cluster_health_client()\n\n    # Prepare event summary for coordination engine\n    event_summary = {\n        'timestamp': datetime.now().isoformat(),\n        'total_events': len(events_df),\n        'warning_percentage': patterns['warning_percentage'],\n        'critical_events': patterns['critical_events'],\n        'top_reasons': patterns['reasons'].head(5).to_dict(),\n        'namespace_distribution': patterns['namespaces'].to_dict(),\n        'anomaly_indicators': {\n            'high_warning_rate': patterns['warning_percentage'] > 30,\n            'critical_events_present': patterns['critical_events'] > 0,\n            'scheduling_issues': 'FailedScheduling' in patterns['reasons'].index\n        }\n    }\n\n    # Send to coordination engine\n    try:\n        response = mcp_client.query_anomaly_patterns({\n            'source': 'openshift-events',\n            'data': event_summary,\n            'severity': 'high' if event_summary['anomaly_indicators']['critical_events_present'] else 'medium'\n        })\n\n        print(f\"‚úÖ Event data sent to coordination engine\")\n        if response.get('simulated'):\n            print(f\"   (Using simulation mode - MCP server not available)\")\n        return response\n\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Failed to send to coordination engine: {e}\")\n        return {'status': 'failed', 'error': str(e)}\n\n# Send events to coordination engine\ncoordination_response = send_events_to_coordination_engine(filtered_events, patterns)\n\nprint(\"\\nüéØ Next Steps:\")\nprint(\"1. Review event patterns for anomalies\")\nprint(\"2. Set up real-time event monitoring\")\nprint(\"3. Configure alerting thresholds\")\nprint(\"4. Integrate with anomaly detection models\")\nprint(\"\\nüìö Related Notebooks:\")\nprint(\"- 02-anomaly-detection/: Use events data for ML model training\")\nprint(\"- 03-self-healing-logic/: Implement event-driven remediation\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

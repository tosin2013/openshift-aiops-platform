{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Store Demo for Self-Healing Platform\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to implement a feature store for the OpenShift AIOps Self-Healing Platform using Parquet files for efficient storage and data versioning. It covers feature engineering, storage, retrieval, and versioning patterns essential for ML operations.\n",
    "\n",
    "## Prerequisites\n",
    "- Persistent storage configured (ODF)\n",
    "- Processed data from metrics and events collection\n",
    "- PyArrow and Pandas for Parquet operations\n",
    "- Access to coordination engine for feature serving\n",
    "\n",
    "## Expected Outcomes\n",
    "- Understand feature store architecture and benefits\n",
    "- Implement efficient Parquet-based feature storage\n",
    "- Demonstrate data versioning and lineage tracking\n",
    "- Create reusable feature engineering pipelines\n",
    "\n",
    "## References\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- ADR-013: Data Collection and Preprocessing Workflows\n",
    "- Feature Store Best Practices Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport json\nimport warnings\nimport sys\nimport os\nfrom pathlib import Path\nimport hashlib\nfrom typing import Dict, List, Optional, Any, Tuple\n\n# Parquet and data processing\ntry:\n    import pyarrow as pa\n    import pyarrow.parquet as pq\n    parquet_available = True\n    print(\"âœ… PyArrow available for Parquet operations\")\nexcept ImportError:\n    parquet_available = False\n    print(\"âš ï¸ PyArrow not available - using pandas parquet engine\")\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n\n    # Fallback: search upward from cwd\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"âœ… Utils path found: {utils_path}\")\nelse:\n    print(\"âš ï¸ Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import (\n        setup_environment, print_environment_info,\n        save_processed_data, load_processed_data,\n        validate_data_quality\n    )\n    print(\"âœ… Common functions imported\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Common functions not available: {e}\")\n    print(\"   Using minimal fallback implementations\")\n\n    # Minimal fallback implementations\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/data/feature_store', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {\n            'data_dir': '/opt/app-root/src/data',\n            'models_dir': '/opt/app-root/src/models',\n            'working_dir': os.getcwd()\n        }\n\n    def print_environment_info(env_info):\n        print(f\"ðŸ“ Data dir: {env_info.get('data_dir', 'N/A')}\")\n        print(f\"ðŸ“ Models dir: {env_info.get('models_dir', 'N/A')}\")\n\n    def save_processed_data(data, filename):\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        filepath = f'/opt/app-root/src/data/processed/{filename}'\n        if filename.endswith('.parquet') and hasattr(data, 'to_parquet'):\n            data.to_parquet(filepath)\n        elif filename.endswith('.json'):\n            with open(filepath, 'w') as f:\n                if hasattr(data, 'items'):\n                    serializable = {}\n                    for k, v in data.items():\n                        if hasattr(v, 'to_dict'):\n                            serializable[k] = v.to_dict()\n                        else:\n                            serializable[k] = v\n                    json.dump(serializable, f, default=str)\n                else:\n                    json.dump(data, f, default=str)\n        print(f\"ðŸ’¾ Saved: {filepath}\")\n\n    def load_processed_data(filename):\n        filepath = f'/opt/app-root/src/data/processed/{filename}'\n        if filename.endswith('.parquet'):\n            return pd.read_parquet(filepath)\n        elif filename.endswith('.json'):\n            with open(filepath, 'r') as f:\n                return json.load(f)\n        return None\n\n    def validate_data_quality(df, metric_name='unknown'):\n        return {'valid': True, 'issues': [], 'metric_name': metric_name}\n\nprint(\"âœ… Libraries imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup & Configuration\n\nInitialize the feature store and configure parameters for feature engineering and storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "env_info = setup_environment()\n",
    "print_environment_info(env_info)\n",
    "\n",
    "# Feature Store Configuration\n",
    "FEATURE_STORE_CONFIG = {\n",
    "    'base_path': '/opt/app-root/src/data/feature_store',\n",
    "    'versioning_enabled': True,\n",
    "    'compression': 'snappy',  # Parquet compression\n",
    "    'partition_cols': ['date', 'namespace'],  # Partitioning strategy\n",
    "    'feature_groups': {\n",
    "        'infrastructure_metrics': {\n",
    "            'description': 'CPU, memory, disk, network metrics',\n",
    "            'update_frequency': '5min',\n",
    "            'retention_days': 90\n",
    "        },\n",
    "        'application_metrics': {\n",
    "            'description': 'Request rates, response times, error rates',\n",
    "            'update_frequency': '1min',\n",
    "            'retention_days': 30\n",
    "        },\n",
    "        'cluster_events': {\n",
    "            'description': 'Kubernetes events and state changes',\n",
    "            'update_frequency': 'realtime',\n",
    "            'retention_days': 60\n",
    "        },\n",
    "        'anomaly_features': {\n",
    "            'description': 'Engineered features for anomaly detection',\n",
    "            'update_frequency': '15min',\n",
    "            'retention_days': 180\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create feature store directory structure\n",
    "feature_store_path = Path(FEATURE_STORE_CONFIG['base_path'])\n",
    "feature_store_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for group_name in FEATURE_STORE_CONFIG['feature_groups'].keys():\n",
    "    group_path = feature_store_path / group_name\n",
    "    group_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create metadata directory\n",
    "    (group_path / 'metadata').mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create versions directory\n",
    "    (group_path / 'versions').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Feature store initialized at: {feature_store_path}\")\n",
    "print(f\"ðŸ—‚ï¸ Feature groups: {', '.join(FEATURE_STORE_CONFIG['feature_groups'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Feature Store Class\n\nDefine the FeatureStore class that manages feature groups, versioning, and metadata for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureStore:\n",
    "    \"\"\"\n",
    "    Simple feature store implementation using Parquet files\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str, config: Dict[str, Any]):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.config = config\n",
    "        self.feature_groups = config['feature_groups']\n",
    "        \n",
    "    def create_feature_group(self, name: str, schema: Dict[str, str], description: str = \"\"):\n",
    "        \"\"\"\n",
    "        Create a new feature group with schema definition\n",
    "        \"\"\"\n",
    "        group_path = self.base_path / name\n",
    "        group_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save schema metadata\n",
    "        metadata = {\n",
    "            'name': name,\n",
    "            'description': description,\n",
    "            'schema': schema,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'version': '1.0.0'\n",
    "        }\n",
    "        \n",
    "        metadata_path = group_path / 'metadata' / 'schema.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ… Created feature group: {name}\")\n",
    "        return group_path\n",
    "    \n",
    "    def write_features(self, group_name: str, features_df: pd.DataFrame, \n",
    "                      version: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Write features to the feature store with versioning\n",
    "        \"\"\"\n",
    "        if group_name not in self.feature_groups:\n",
    "            raise ValueError(f\"Feature group {group_name} not found\")\n",
    "        \n",
    "        group_path = self.base_path / group_name\n",
    "        \n",
    "        # Generate version if not provided\n",
    "        if version is None:\n",
    "            version = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Add metadata columns\n",
    "        features_df = features_df.copy()\n",
    "        features_df['_version'] = version\n",
    "        features_df['_ingestion_time'] = datetime.now()\n",
    "        \n",
    "        # Add partitioning columns if missing\n",
    "        if 'date' not in features_df.columns:\n",
    "            features_df['date'] = features_df.get('timestamp', datetime.now()).dt.date\n",
    "        if 'namespace' not in features_df.columns:\n",
    "            features_df['namespace'] = features_df.get('namespace', 'default')\n",
    "        \n",
    "        # Write to versioned location\n",
    "        version_path = group_path / 'versions' / f'v_{version}.parquet'\n",
    "        \n",
    "        if parquet_available:\n",
    "            # Use PyArrow for better performance\n",
    "            table = pa.Table.from_pandas(features_df)\n",
    "            pq.write_table(table, version_path, compression=self.config['compression'])\n",
    "        else:\n",
    "            # Fallback to pandas\n",
    "            features_df.to_parquet(version_path, compression=self.config['compression'])\n",
    "        \n",
    "        # Update latest symlink\n",
    "        latest_path = group_path / 'latest.parquet'\n",
    "        if latest_path.exists():\n",
    "            latest_path.unlink()\n",
    "        \n",
    "        # Create symlink to latest version (fallback to copy if symlink fails)\n",
    "        try:\n",
    "            latest_path.symlink_to(version_path)\n",
    "        except OSError:\n",
    "            # Fallback: copy file if symlink not supported\n",
    "            features_df.to_parquet(latest_path, compression=self.config['compression'])\n",
    "        \n",
    "        # Update metadata\n",
    "        self._update_metadata(group_name, version, len(features_df))\n",
    "        \n",
    "        print(f\"âœ… Written {len(features_df)} features to {group_name} (version: {version})\")\n",
    "        return version\n",
    "    \n",
    "    def read_features(self, group_name: str, version: Optional[str] = None, \n",
    "                     filters: Optional[List[Tuple]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read features from the feature store\n",
    "        \"\"\"\n",
    "        if group_name not in self.feature_groups:\n",
    "            raise ValueError(f\"Feature group {group_name} not found\")\n",
    "        \n",
    "        group_path = self.base_path / group_name\n",
    "        \n",
    "        if version is None:\n",
    "            # Read latest version\n",
    "            file_path = group_path / 'latest.parquet'\n",
    "        else:\n",
    "            # Read specific version\n",
    "            file_path = group_path / 'versions' / f'v_{version}.parquet'\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Features not found: {file_path}\")\n",
    "        \n",
    "        # Read with optional filters\n",
    "        if parquet_available and filters:\n",
    "            df = pq.read_table(file_path, filters=filters).to_pandas()\n",
    "        else:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Apply filters manually if PyArrow not available\n",
    "            if filters:\n",
    "                for filter_condition in filters:\n",
    "                    column, operator, value = filter_condition\n",
    "                    if operator == '==':\n",
    "                        df = df[df[column] == value]\n",
    "                    elif operator == '>=':\n",
    "                        df = df[df[column] >= value]\n",
    "                    elif operator == '<=':\n",
    "                        df = df[df[column] <= value]\n",
    "        \n",
    "        print(f\"âœ… Read {len(df)} features from {group_name}\")\n",
    "        return df\n",
    "    \n",
    "    def _update_metadata(self, group_name: str, version: str, record_count: int):\n",
    "        \"\"\"\n",
    "        Update feature group metadata\n",
    "        \"\"\"\n",
    "        group_path = self.base_path / group_name\n",
    "        metadata_path = group_path / 'metadata' / 'versions.json'\n",
    "        \n",
    "        # Load existing metadata\n",
    "        if metadata_path.exists():\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "        else:\n",
    "            metadata = {'versions': []}\n",
    "        \n",
    "        # Add new version info\n",
    "        version_info = {\n",
    "            'version': version,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'record_count': record_count,\n",
    "            'file_path': f'versions/v_{version}.parquet'\n",
    "        }\n",
    "        \n",
    "        metadata['versions'].append(version_info)\n",
    "        metadata['latest_version'] = version\n",
    "        metadata['total_versions'] = len(metadata['versions'])\n",
    "        \n",
    "        # Save updated metadata\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    def list_versions(self, group_name: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        List all versions of a feature group\n",
    "        \"\"\"\n",
    "        group_path = self.base_path / group_name\n",
    "        metadata_path = group_path / 'metadata' / 'versions.json'\n",
    "        \n",
    "        if not metadata_path.exists():\n",
    "            return []\n",
    "        \n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        return metadata.get('versions', [])\n",
    "\n",
    "# Initialize feature store\n",
    "feature_store = FeatureStore(\n",
    "    FEATURE_STORE_CONFIG['base_path'], \n",
    "    FEATURE_STORE_CONFIG\n",
    ")\n",
    "\n",
    "print(\"ðŸª Feature store initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Feature Engineering Functions\n\nDefine functions to engineer features from raw metrics, events, and logs for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_features():\n",
    "    \"\"\"\n",
    "    Generate sample feature data for demonstration\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽ­ Generating sample feature data...\")\n",
    "    \n",
    "    # Generate infrastructure metrics features\n",
    "    timestamps = pd.date_range(\n",
    "        start=datetime.now() - timedelta(hours=24),\n",
    "        end=datetime.now(),\n",
    "        freq='5min'\n",
    "    )\n",
    "    \n",
    "    infrastructure_features = []\n",
    "    namespaces = ['self-healing-platform', 'openshift-monitoring', 'default']\n",
    "    nodes = ['node-1', 'node-2', 'node-3']\n",
    "    \n",
    "    for timestamp in timestamps:\n",
    "        for namespace in namespaces:\n",
    "            for node in nodes:\n",
    "                # Generate realistic metrics with some patterns\n",
    "                base_cpu = 30 + 20 * np.sin(timestamp.hour * np.pi / 12)  # Daily pattern\n",
    "                base_memory = 60 + 15 * np.sin(timestamp.hour * np.pi / 8)  # Different pattern\n",
    "                \n",
    "                infrastructure_features.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'namespace': namespace,\n",
    "                    'node': node,\n",
    "                    'cpu_usage_percent': max(0, min(100, base_cpu + np.random.normal(0, 5))),\n",
    "                    'memory_usage_percent': max(0, min(100, base_memory + np.random.normal(0, 8))),\n",
    "                    'disk_usage_percent': np.random.uniform(20, 80),\n",
    "                    'network_bytes_in': np.random.exponential(1000000),\n",
    "                    'network_bytes_out': np.random.exponential(800000),\n",
    "                    'pod_count': np.random.randint(5, 25),\n",
    "                    'container_restarts': np.random.poisson(0.1)\n",
    "                })\n",
    "    \n",
    "    infra_df = pd.DataFrame(infrastructure_features)\n",
    "    \n",
    "    # Generate application metrics features\n",
    "    app_timestamps = pd.date_range(\n",
    "        start=datetime.now() - timedelta(hours=6),\n",
    "        end=datetime.now(),\n",
    "        freq='1min'\n",
    "    )\n",
    "    \n",
    "    application_features = []\n",
    "    services = ['coordination-engine', 'mcp-server', 'prometheus']\n",
    "    \n",
    "    for timestamp in app_timestamps:\n",
    "        for service in services:\n",
    "            # Generate realistic application metrics\n",
    "            base_requests = 100 + 50 * np.sin(timestamp.hour * np.pi / 12)\n",
    "            \n",
    "            application_features.append({\n",
    "                'timestamp': timestamp,\n",
    "                'namespace': 'self-healing-platform',\n",
    "                'service': service,\n",
    "                'requests_per_minute': max(0, base_requests + np.random.normal(0, 20)),\n",
    "                'response_time_p95': np.random.lognormal(4, 0.5),  # Log-normal distribution\n",
    "                'error_rate_percent': max(0, np.random.exponential(0.5)),\n",
    "                'active_connections': np.random.randint(10, 100),\n",
    "                'queue_depth': np.random.poisson(2),\n",
    "                'cache_hit_rate': np.random.uniform(0.7, 0.95)\n",
    "            })\n",
    "    \n",
    "    app_df = pd.DataFrame(application_features)\n",
    "    \n",
    "    print(f\"âœ… Generated {len(infra_df)} infrastructure feature records\")\n",
    "    print(f\"âœ… Generated {len(app_df)} application feature records\")\n",
    "    \n",
    "    return infra_df, app_df\n",
    "\n",
    "def engineer_anomaly_features(infra_df, app_df):\n",
    "    \"\"\"\n",
    "    Engineer features specifically for anomaly detection\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”§ Engineering anomaly detection features...\")\n",
    "    \n",
    "    anomaly_features = []\n",
    "    \n",
    "    # Group by time windows for feature engineering\n",
    "    infra_hourly = infra_df.groupby([pd.Grouper(key='timestamp', freq='1H'), 'namespace']).agg({\n",
    "        'cpu_usage_percent': ['mean', 'std', 'max'],\n",
    "        'memory_usage_percent': ['mean', 'std', 'max'],\n",
    "        'network_bytes_in': ['sum', 'mean'],\n",
    "        'container_restarts': 'sum',\n",
    "        'pod_count': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    infra_hourly.columns = ['timestamp', 'namespace'] + [\n",
    "        f\"{col[0]}_{col[1]}\" for col in infra_hourly.columns[2:]\n",
    "    ]\n",
    "    \n",
    "    app_hourly = app_df.groupby([pd.Grouper(key='timestamp', freq='1H'), 'namespace']).agg({\n",
    "        'requests_per_minute': ['sum', 'mean'],\n",
    "        'response_time_p95': ['mean', 'max'],\n",
    "        'error_rate_percent': ['mean', 'max'],\n",
    "        'active_connections': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    app_hourly.columns = ['timestamp', 'namespace'] + [\n",
    "        f\"{col[0]}_{col[1]}\" for col in app_hourly.columns[2:]\n",
    "    ]\n",
    "    \n",
    "    # Merge infrastructure and application features\n",
    "    merged_df = pd.merge(infra_hourly, app_hourly, on=['timestamp', 'namespace'], how='outer')\n",
    "    \n",
    "    # Engineer additional features\n",
    "    merged_df['cpu_memory_ratio'] = merged_df['cpu_usage_percent_mean'] / (merged_df['memory_usage_percent_mean'] + 1)\n",
    "    merged_df['network_intensity'] = merged_df['network_bytes_in_sum'] / (merged_df['pod_count_mean'] + 1)\n",
    "    merged_df['error_to_request_ratio'] = merged_df['error_rate_percent_mean'] / (merged_df['requests_per_minute_mean'] + 1)\n",
    "    merged_df['resource_pressure'] = (merged_df['cpu_usage_percent_mean'] + merged_df['memory_usage_percent_mean']) / 2\n",
    "    \n",
    "    # Add time-based features\n",
    "    merged_df['hour_of_day'] = merged_df['timestamp'].dt.hour\n",
    "    merged_df['day_of_week'] = merged_df['timestamp'].dt.dayofweek\n",
    "    merged_df['is_weekend'] = merged_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    merged_df['is_business_hours'] = merged_df['hour_of_day'].between(9, 17).astype(int)\n",
    "    \n",
    "    print(f\"âœ… Engineered {len(merged_df)} anomaly feature records with {len(merged_df.columns)} features\")\n",
    "    return merged_df\n",
    "\n",
    "# Generate sample data\n",
    "infra_df, app_df = generate_sample_features()\n",
    "anomaly_df = engineer_anomaly_features(infra_df, app_df)\n",
    "\n",
    "print(f\"\\nðŸ“Š Sample Data Summary:\")\n",
    "print(f\"Infrastructure features: {infra_df.shape}\")\n",
    "print(f\"Application features: {app_df.shape}\")\n",
    "print(f\"Anomaly features: {anomaly_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Feature Groups and Store Data\n\nCreate feature groups from sample data and demonstrate how to store and version features in the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature groups and write data\n",
    "print(\"ðŸª Writing features to feature store...\")\n",
    "\n",
    "# Create feature group schemas\n",
    "infra_schema = {\n",
    "    'timestamp': 'datetime64[ns]',\n",
    "    'namespace': 'string',\n",
    "    'node': 'string',\n",
    "    'cpu_usage_percent': 'float64',\n",
    "    'memory_usage_percent': 'float64',\n",
    "    'disk_usage_percent': 'float64',\n",
    "    'network_bytes_in': 'float64',\n",
    "    'network_bytes_out': 'float64',\n",
    "    'pod_count': 'int64',\n",
    "    'container_restarts': 'int64'\n",
    "}\n",
    "\n",
    "app_schema = {\n",
    "    'timestamp': 'datetime64[ns]',\n",
    "    'namespace': 'string',\n",
    "    'service': 'string',\n",
    "    'requests_per_minute': 'float64',\n",
    "    'response_time_p95': 'float64',\n",
    "    'error_rate_percent': 'float64',\n",
    "    'active_connections': 'int64',\n",
    "    'queue_depth': 'int64',\n",
    "    'cache_hit_rate': 'float64'\n",
    "}\n",
    "\n",
    "# Create feature groups\n",
    "feature_store.create_feature_group(\n",
    "    'infrastructure_metrics', \n",
    "    infra_schema, \n",
    "    'CPU, memory, disk, network metrics from cluster nodes'\n",
    ")\n",
    "\n",
    "feature_store.create_feature_group(\n",
    "    'application_metrics', \n",
    "    app_schema, \n",
    "    'Request rates, response times, error rates from applications'\n",
    ")\n",
    "\n",
    "# Write features with versioning\n",
    "infra_version = feature_store.write_features('infrastructure_metrics', infra_df)\n",
    "app_version = feature_store.write_features('application_metrics', app_df)\n",
    "anomaly_version = feature_store.write_features('anomaly_features', anomaly_df)\n",
    "\n",
    "print(f\"\\nðŸ“ Feature versions created:\")\n",
    "print(f\"- Infrastructure: {infra_version}\")\n",
    "print(f\"- Application: {app_version}\")\n",
    "print(f\"- Anomaly: {anomaly_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demonstrate Feature Retrieval and Filtering\n\nShow how to retrieve features from the store, filter by time range, and access feature metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate feature retrieval and filtering\n",
    "print(\"ðŸ” Demonstrating feature retrieval...\")\n",
    "\n",
    "# Read latest features\n",
    "latest_infra = feature_store.read_features('infrastructure_metrics')\n",
    "print(f\"Latest infrastructure features: {len(latest_infra)} records\")\n",
    "\n",
    "# Read with filters (if PyArrow available)\n",
    "if parquet_available:\n",
    "    # Filter by namespace\n",
    "    filtered_features = feature_store.read_features(\n",
    "        'infrastructure_metrics',\n",
    "        filters=[('namespace', '==', 'self-healing-platform')]\n",
    "    )\n",
    "    print(f\"Filtered features (self-healing-platform): {len(filtered_features)} records\")\n",
    "else:\n",
    "    # Manual filtering\n",
    "    filtered_features = latest_infra[latest_infra['namespace'] == 'self-healing-platform']\n",
    "    print(f\"Filtered features (self-healing-platform): {len(filtered_features)} records\")\n",
    "\n",
    "# List versions\n",
    "infra_versions = feature_store.list_versions('infrastructure_metrics')\n",
    "print(f\"\\nðŸ“š Infrastructure metrics versions: {len(infra_versions)}\")\n",
    "for version in infra_versions[-3:]:  # Show last 3 versions\n",
    "    print(f\"  - {version['version']}: {version['record_count']} records ({version['created_at']})\")\n",
    "\n",
    "# Demonstrate time-based analysis\n",
    "print(\"\\nðŸ“ˆ Feature analysis:\")\n",
    "print(f\"CPU usage stats: mean={latest_infra['cpu_usage_percent'].mean():.1f}%, std={latest_infra['cpu_usage_percent'].std():.1f}%\")\n",
    "print(f\"Memory usage stats: mean={latest_infra['memory_usage_percent'].mean():.1f}%, std={latest_infra['memory_usage_percent'].std():.1f}%\")\n",
    "print(f\"Container restarts: total={latest_infra['container_restarts'].sum()}, max={latest_infra['container_restarts'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Feature Store Data\n\nCreate visualizations of stored features to understand data distributions and feature engineering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature store data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Feature Store Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# CPU usage over time by namespace\n",
    "for namespace in latest_infra['namespace'].unique():\n",
    "    ns_data = latest_infra[latest_infra['namespace'] == namespace]\n",
    "    ns_hourly = ns_data.groupby(ns_data['timestamp'].dt.hour)['cpu_usage_percent'].mean()\n",
    "    axes[0,0].plot(ns_hourly.index, ns_hourly.values, marker='o', label=namespace)\n",
    "\n",
    "axes[0,0].set_title('CPU Usage by Hour and Namespace')\n",
    "axes[0,0].set_xlabel('Hour of Day')\n",
    "axes[0,0].set_ylabel('CPU Usage %')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Memory vs CPU scatter plot\n",
    "scatter = axes[0,1].scatter(\n",
    "    latest_infra['cpu_usage_percent'], \n",
    "    latest_infra['memory_usage_percent'],\n",
    "    c=latest_infra['container_restarts'], \n",
    "    cmap='Reds', \n",
    "    alpha=0.6\n",
    ")\n",
    "axes[0,1].set_title('CPU vs Memory Usage (colored by restarts)')\n",
    "axes[0,1].set_xlabel('CPU Usage %')\n",
    "axes[0,1].set_ylabel('Memory Usage %')\n",
    "plt.colorbar(scatter, ax=axes[0,1], label='Container Restarts')\n",
    "\n",
    "# Application metrics if available\n",
    "if len(app_df) > 0:\n",
    "    app_df.groupby('service')['requests_per_minute'].mean().plot(kind='bar', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Average Requests per Minute by Service')\n",
    "    axes[1,0].set_xlabel('Service')\n",
    "    axes[1,0].set_ylabel('Requests/min')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Error rate distribution\n",
    "    axes[1,1].hist(app_df['error_rate_percent'], bins=20, alpha=0.7, color='orange')\n",
    "    axes[1,1].set_title('Error Rate Distribution')\n",
    "    axes[1,1].set_xlabel('Error Rate %')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "else:\n",
    "    axes[1,0].text(0.5, 0.5, 'No Application\\nMetrics Available', \n",
    "                   ha='center', va='center', transform=axes[1,0].transAxes)\n",
    "    axes[1,1].text(0.5, 0.5, 'No Application\\nMetrics Available', \n",
    "                   ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save feature store metadata\n",
    "feature_store_summary = {\n",
    "    'feature_groups': list(FEATURE_STORE_CONFIG['feature_groups'].keys()),\n",
    "    'total_features': {\n",
    "        'infrastructure_metrics': len(latest_infra),\n",
    "        'application_metrics': len(app_df),\n",
    "        'anomaly_features': len(anomaly_df)\n",
    "    },\n",
    "    'versions_created': {\n",
    "        'infrastructure_metrics': infra_version,\n",
    "        'application_metrics': app_version,\n",
    "        'anomaly_features': anomaly_version\n",
    "    },\n",
    "    'storage_format': 'parquet',\n",
    "    'compression': FEATURE_STORE_CONFIG['compression'],\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "save_processed_data(feature_store_summary, 'feature_store_summary.json')\n",
    "\n",
    "print(\"\\nðŸ’¾ Feature store demo completed successfully!\")\n",
    "print(\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"1. Integrate feature store with ML training pipelines\")\n",
    "print(\"2. Set up automated feature engineering jobs\")\n",
    "print(\"3. Implement feature monitoring and drift detection\")\n",
    "print(\"4. Create feature serving endpoints for real-time inference\")\n",
    "print(\"\\nðŸ“š Related Notebooks:\")\n",
    "print(\"- 02-anomaly-detection/: Use features for ML model training\")\n",
    "print(\"- 04-model-serving/: Serve features for real-time inference\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

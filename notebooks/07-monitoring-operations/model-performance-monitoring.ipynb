{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Monitoring\n",
    "\n",
    "## Overview\n",
    "This notebook implements comprehensive model performance monitoring. It tracks accuracy, detects model drift, and triggers automated retraining when performance degrades.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `prometheus-metrics-monitoring.ipynb`\n",
    "- Trained models available\n",
    "- Inference pipeline deployed\n",
    "- Historical performance data available\n",
    "\n",
    "## Learning Objectives\n",
    "- Track model accuracy over time\n",
    "- Detect model drift\n",
    "- Monitor prediction confidence\n",
    "- Trigger automated retraining\n",
    "- Implement model versioning\n",
    "\n",
    "## Key Concepts\n",
    "- **Model Drift**: Performance degradation over time\n",
    "- **Accuracy Tracking**: Monitor prediction accuracy\n",
    "- **Confidence Scoring**: Track prediction confidence\n",
    "- **Automated Retraining**: Trigger retraining on drift\n",
    "- **Model Versioning**: Track model versions and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom typing import Dict, List, Any\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"✅ Utils path found: {utils_path}\")\nelse:\n    print(\"⚠️ Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import setup_environment\n    print(\"✅ Common functions imported\")\nexcept ImportError as e:\n    print(f\"⚠️ Common functions not available: {e}\")\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Setup environment\nenv_info = setup_environment()\nlogger.info(f\"Environment ready: {env_info}\")\n\n# Define paths\nDATA_DIR = Path('/opt/app-root/src/data')\nPROCESSED_DIR = DATA_DIR / 'processed'\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\nMODELS_DIR = Path('/opt/app-root/src/models')\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Configuration\nNAMESPACE = 'self-healing-platform'\nACCURACY_THRESHOLD = 0.80  # Minimum acceptable accuracy\nDRIFT_THRESHOLD = 0.05     # 5% accuracy drop triggers retraining\nCONFIDENCE_THRESHOLD = 0.75\n\nlogger.info(f\"Model performance monitoring initialized\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Track Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_model_accuracy(predictions: np.ndarray, ground_truth: np.ndarray, model_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Track model accuracy metrics.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions\n",
    "        ground_truth: Ground truth labels\n",
    "        model_name: Name of the model\n",
    "    \n",
    "    Returns:\n",
    "        Accuracy metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate metrics\n",
    "        accuracy = np.mean(predictions == ground_truth)\n",
    "        precision = np.sum((predictions == 1) & (ground_truth == 1)) / np.sum(predictions == 1)\n",
    "        recall = np.sum((predictions == 1) & (ground_truth == 1)) / np.sum(ground_truth == 1)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        metrics = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'samples': len(predictions)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Model {model_name} accuracy: {accuracy:.2%}\")\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Accuracy tracking error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Simulate predictions and ground truth\n",
    "predictions = np.random.choice([0, 1], size=1000, p=[0.3, 0.7])\n",
    "ground_truth = np.random.choice([0, 1], size=1000, p=[0.3, 0.7])\n",
    "\n",
    "accuracy_metrics = track_model_accuracy(predictions, ground_truth, 'ensemble-anomaly-detector')\n",
    "print(json.dumps(accuracy_metrics, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Detect Model Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_model_drift(current_accuracy: float, baseline_accuracy: float, drift_threshold: float = 0.05) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Detect model drift by comparing current and baseline accuracy.\n",
    "    \n",
    "    Args:\n",
    "        current_accuracy: Current model accuracy\n",
    "        baseline_accuracy: Baseline model accuracy\n",
    "        drift_threshold: Threshold for drift detection\n",
    "    \n",
    "    Returns:\n",
    "        Drift detection result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        accuracy_drop = baseline_accuracy - current_accuracy\n",
    "        drift_detected = accuracy_drop > drift_threshold\n",
    "        drift_severity = 'critical' if accuracy_drop > 0.10 else 'high' if accuracy_drop > 0.05 else 'low'\n",
    "        \n",
    "        drift_result = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'baseline_accuracy': baseline_accuracy,\n",
    "            'current_accuracy': current_accuracy,\n",
    "            'accuracy_drop': accuracy_drop,\n",
    "            'drift_detected': drift_detected,\n",
    "            'drift_severity': drift_severity,\n",
    "            'retraining_required': drift_detected\n",
    "        }\n",
    "        \n",
    "        if drift_detected:\n",
    "            logger.warning(f\"Model drift detected: {accuracy_drop:.2%} drop ({drift_severity})\")\n",
    "        else:\n",
    "            logger.info(f\"No model drift detected\")\n",
    "        \n",
    "        return drift_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Drift detection error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test drift detection\n",
    "baseline_acc = 0.92\n",
    "current_acc = 0.87\n",
    "\n",
    "drift_result = detect_model_drift(current_acc, baseline_acc, DRIFT_THRESHOLD)\n",
    "print(json.dumps(drift_result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Monitor Prediction Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_confidence(confidences: np.ndarray, threshold: float = 0.75) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Monitor prediction confidence distribution.\n",
    "    \n",
    "    Args:\n",
    "        confidences: Array of prediction confidences\n",
    "        threshold: Confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        Confidence monitoring result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        high_confidence = np.sum(confidences >= threshold) / len(confidences)\n",
    "        low_confidence = np.sum(confidences < threshold) / len(confidences)\n",
    "        \n",
    "        confidence_result = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'mean_confidence': float(np.mean(confidences)),\n",
    "            'median_confidence': float(np.median(confidences)),\n",
    "            'std_confidence': float(np.std(confidences)),\n",
    "            'high_confidence_rate': high_confidence,\n",
    "            'low_confidence_rate': low_confidence,\n",
    "            'min_confidence': float(np.min(confidences)),\n",
    "            'max_confidence': float(np.max(confidences))\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Confidence monitoring: {high_confidence:.1%} high confidence predictions\")\n",
    "        return confidence_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Confidence monitoring error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Simulate confidences\n",
    "confidences = np.random.uniform(0.5, 0.99, size=1000)\n",
    "\n",
    "confidence_result = monitor_confidence(confidences, CONFIDENCE_THRESHOLD)\n",
    "print(json.dumps(confidence_result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Trigger Automated Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_retraining(drift_result: Dict[str, Any], model_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Trigger automated model retraining if drift detected.\n",
    "    \n",
    "    Args:\n",
    "        drift_result: Drift detection result\n",
    "        model_name: Name of the model\n",
    "    \n",
    "    Returns:\n",
    "        Retraining trigger result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not drift_result.get('retraining_required', False):\n",
    "            logger.info(f\"No retraining required for {model_name}\")\n",
    "            return {'triggered': False, 'reason': 'No drift detected'}\n",
    "        \n",
    "        # Trigger retraining\n",
    "        retraining_config = {\n",
    "            'model_name': model_name,\n",
    "            'trigger_time': datetime.now().isoformat(),\n",
    "            'reason': f\"Accuracy drop: {drift_result['accuracy_drop']:.2%}\",\n",
    "            'severity': drift_result['drift_severity'],\n",
    "            'training_config': {\n",
    "                'epochs': 50,\n",
    "                'batch_size': 32,\n",
    "                'learning_rate': 0.001,\n",
    "                'early_stopping': True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Retraining triggered for {model_name}\")\n",
    "        return {'triggered': True, 'config': retraining_config}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Retraining trigger error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test retraining trigger\n",
    "retraining_result = trigger_retraining(drift_result, 'ensemble-anomaly-detector')\n",
    "print(json.dumps(retraining_result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Track Model Performance History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model performance tracking dataframe\n",
    "performance_tracking = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now() - timedelta(hours=i),\n",
    "        'model_name': 'ensemble-anomaly-detector',\n",
    "        'accuracy': np.random.uniform(0.85, 0.95),\n",
    "        'precision': np.random.uniform(0.80, 0.95),\n",
    "        'recall': np.random.uniform(0.80, 0.95),\n",
    "        'f1_score': np.random.uniform(0.80, 0.95),\n",
    "        'mean_confidence': np.random.uniform(0.75, 0.95),\n",
    "        'drift_detected': np.random.choice([True, False], p=[0.1, 0.9]),\n",
    "        'retraining_triggered': np.random.choice([True, False], p=[0.05, 0.95])\n",
    "    }\n",
    "    for i in range(24)  # 24 hours of data\n",
    "])\n",
    "\n",
    "# Save tracking data\n",
    "tracking_file = PROCESSED_DIR / 'model_performance_tracking.parquet'\n",
    "performance_tracking.to_parquet(tracking_file)\n",
    "\n",
    "logger.info(f\"Saved model performance tracking data\")\n",
    "print(performance_tracking.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify outputs\n",
    "assert tracking_file.exists(), \"Model performance tracking file not created\"\n",
    "assert 'accuracy' in accuracy_metrics, \"No accuracy metric\"\n",
    "assert 'drift_detected' in drift_result, \"No drift detection result\"\n",
    "\n",
    "avg_accuracy = performance_tracking['accuracy'].mean()\n",
    "drift_rate = performance_tracking['drift_detected'].sum() / len(performance_tracking)\n",
    "retraining_rate = performance_tracking['retraining_triggered'].sum() / len(performance_tracking)\n",
    "\n",
    "logger.info(f\"✅ All validations passed\")\n",
    "print(f\"\\nModel Performance Monitoring Summary:\")\n",
    "print(f\"  Performance Records: {len(performance_tracking)}\")\n",
    "print(f\"  Average Accuracy: {avg_accuracy:.2%}\")\n",
    "print(f\"  Drift Detection Rate: {drift_rate:.1%}\")\n",
    "print(f\"  Retraining Trigger Rate: {retraining_rate:.1%}\")\n",
    "print(f\"  Accuracy Threshold: {ACCURACY_THRESHOLD:.0%}\")\n",
    "print(f\"  Drift Threshold: {DRIFT_THRESHOLD:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Model predictions and ground truth\n",
    "- **Output**: Performance metrics and retraining triggers\n",
    "- **Monitoring**: Accuracy, drift, and confidence tracking\n",
    "- **Next**: Healing success tracking\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Monitor model performance continuously\n",
    "2. Proceed to `healing-success-tracking.ipynb`\n",
    "3. Track remediation success rates\n",
    "4. Analyze failure patterns\n",
    "5. Complete Phase 7 implementation\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-003: Self-Healing Platform Architecture\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [Model Drift Detection](https://en.wikipedia.org/wiki/Concept_drift)\n",
    "- [Model Monitoring Best Practices](https://ml-ops.systems/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

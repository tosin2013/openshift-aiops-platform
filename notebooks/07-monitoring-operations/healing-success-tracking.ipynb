{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healing Success Tracking\n",
    "\n",
    "## Overview\n",
    "This notebook implements comprehensive tracking of remediation success rates. It analyzes failure patterns, generates reports, and provides insights for platform optimization.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `model-performance-monitoring.ipynb`\n",
    "- Remediation history available\n",
    "- Incident data collected\n",
    "- Coordination engine logs accessible\n",
    "\n",
    "## Learning Objectives\n",
    "- Track remediation success rates\n",
    "- Analyze failure patterns\n",
    "- Generate comprehensive reports\n",
    "- Identify improvement opportunities\n",
    "- Optimize healing strategies\n",
    "\n",
    "## Key Concepts\n",
    "- **Success Rate**: Percentage of successful remediations\n",
    "- **Failure Analysis**: Root cause of failures\n",
    "- **MTTR**: Mean Time To Resolution\n",
    "- **Incident Trends**: Patterns in incidents\n",
    "- **Optimization**: Improve healing effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, List, Any\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"✅ Utils path found: {utils_path}\")\nelse:\n    print(\"⚠️ Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import setup_environment\n    print(\"✅ Common functions imported\")\nexcept ImportError as e:\n    print(f\"⚠️ Common functions not available: {e}\")\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Setup environment\nenv_info = setup_environment()\nlogger.info(f\"Environment ready: {env_info}\")\n\n# Define paths\nDATA_DIR = Path('/opt/app-root/src/data')\nPROCESSED_DIR = DATA_DIR / 'processed'\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\nREPORTS_DIR = DATA_DIR / 'reports'\nREPORTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Configuration\nNAMESPACE = 'self-healing-platform'\nSUCCESS_THRESHOLD = 0.90  # Target 90% success rate\n\nlogger.info(f\"Healing success tracking initialized\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Track Remediation Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_success_metrics(remediation_history: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate remediation success metrics.\n",
    "    \n",
    "    Args:\n",
    "        remediation_history: DataFrame with remediation records\n",
    "    \n",
    "    Returns:\n",
    "        Success metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_remediations = len(remediation_history)\n",
    "        successful = remediation_history['success'].sum()\n",
    "        failed = total_remediations - successful\n",
    "        success_rate = successful / total_remediations if total_remediations > 0 else 0\n",
    "        \n",
    "        # Calculate by action type\n",
    "        success_by_action = remediation_history.groupby('action_type')['success'].agg(['sum', 'count'])\n",
    "        success_by_action['rate'] = success_by_action['sum'] / success_by_action['count']\n",
    "        \n",
    "        metrics = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_remediations': total_remediations,\n",
    "            'successful': successful,\n",
    "            'failed': failed,\n",
    "            'success_rate': success_rate,\n",
    "            'success_by_action': success_by_action.to_dict()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Success rate: {success_rate:.1%} ({successful}/{total_remediations})\")\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Success metrics error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Create sample remediation history\n",
    "remediation_history = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now() - timedelta(hours=i),\n",
    "        'action_type': np.random.choice(['restart', 'scale', 'update', 'restart_pod']),\n",
    "        'success': np.random.choice([True, False], p=[0.92, 0.08]),\n",
    "        'resolution_time_seconds': np.random.randint(5, 120)\n",
    "    }\n",
    "    for i in range(100)  # 100 remediation records\n",
    "])\n",
    "\n",
    "success_metrics = calculate_success_metrics(remediation_history)\n",
    "print(json.dumps(success_metrics, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyze Failure Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_failures(remediation_history: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze failure patterns in remediation history.\n",
    "    \n",
    "    Args:\n",
    "        remediation_history: DataFrame with remediation records\n",
    "    \n",
    "    Returns:\n",
    "        Failure analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        failures = remediation_history[~remediation_history['success']]\n",
    "        \n",
    "        if len(failures) == 0:\n",
    "            logger.info(\"No failures to analyze\")\n",
    "            return {'failures': 0, 'analysis': 'No failures detected'}\n",
    "        \n",
    "        # Analyze failure patterns\n",
    "        failure_by_action = failures['action_type'].value_counts().to_dict()\n",
    "        avg_resolution_time = failures['resolution_time_seconds'].mean()\n",
    "        \n",
    "        analysis = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_failures': len(failures),\n",
    "            'failure_rate': len(failures) / len(remediation_history),\n",
    "            'failures_by_action': failure_by_action,\n",
    "            'avg_resolution_time_seconds': avg_resolution_time,\n",
    "            'top_failure_action': max(failure_by_action, key=failure_by_action.get) if failure_by_action else None\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Failure analysis: {len(failures)} failures ({analysis['failure_rate']:.1%})\")\n",
    "        return analysis\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failure analysis error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "failure_analysis = analyze_failures(remediation_history)\n",
    "print(json.dumps(failure_analysis, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate MTTR (Mean Time To Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mttr(remediation_history: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate Mean Time To Resolution (MTTR).\n",
    "    \n",
    "    Args:\n",
    "        remediation_history: DataFrame with remediation records\n",
    "    \n",
    "    Returns:\n",
    "        MTTR metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        successful = remediation_history[remediation_history['success']]\n",
    "        failed = remediation_history[~remediation_history['success']]\n",
    "        \n",
    "        mttr_metrics = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'overall_mttr_seconds': remediation_history['resolution_time_seconds'].mean(),\n",
    "            'successful_mttr_seconds': successful['resolution_time_seconds'].mean() if len(successful) > 0 else 0,\n",
    "            'failed_mttr_seconds': failed['resolution_time_seconds'].mean() if len(failed) > 0 else 0,\n",
    "            'p50_resolution_time': remediation_history['resolution_time_seconds'].quantile(0.5),\n",
    "            'p95_resolution_time': remediation_history['resolution_time_seconds'].quantile(0.95),\n",
    "            'p99_resolution_time': remediation_history['resolution_time_seconds'].quantile(0.99)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"MTTR: {mttr_metrics['overall_mttr_seconds']:.1f}s (p95: {mttr_metrics['p95_resolution_time']:.1f}s)\")\n",
    "        return mttr_metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"MTTR calculation error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "mttr_metrics = calculate_mttr(remediation_history)\n",
    "print(json.dumps(mttr_metrics, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(success_metrics: Dict, failure_analysis: Dict, mttr_metrics: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate comprehensive healing success report.\n",
    "    \n",
    "    Args:\n",
    "        success_metrics: Success metrics\n",
    "        failure_analysis: Failure analysis\n",
    "        mttr_metrics: MTTR metrics\n",
    "    \n",
    "    Returns:\n",
    "        Comprehensive report\n",
    "    \"\"\"\n",
    "    try:\n",
    "        report = {\n",
    "            'report_date': datetime.now().isoformat(),\n",
    "            'summary': {\n",
    "                'total_remediations': success_metrics.get('total_remediations', 0),\n",
    "                'success_rate': success_metrics.get('success_rate', 0),\n",
    "                'target_success_rate': SUCCESS_THRESHOLD,\n",
    "                'target_met': success_metrics.get('success_rate', 0) >= SUCCESS_THRESHOLD\n",
    "            },\n",
    "            'success_metrics': success_metrics,\n",
    "            'failure_analysis': failure_analysis,\n",
    "            'mttr_metrics': mttr_metrics,\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if success_metrics.get('success_rate', 0) < SUCCESS_THRESHOLD:\n",
    "            report['recommendations'].append(\n",
    "                f\"Improve success rate from {success_metrics.get('success_rate', 0):.1%} to {SUCCESS_THRESHOLD:.0%}\"\n",
    "            )\n",
    "        \n",
    "        if failure_analysis.get('top_failure_action'):\n",
    "            report['recommendations'].append(\n",
    "                f\"Focus on improving {failure_analysis['top_failure_action']} action reliability\"\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Report generated with {len(report['recommendations'])} recommendations\")\n",
    "        return report\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Report generation error: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "report = generate_report(success_metrics, failure_analysis, mttr_metrics)\n",
    "print(json.dumps(report, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Track Healing Success History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create healing success tracking dataframe\n",
    "healing_tracking = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now() - timedelta(days=i),\n",
    "        'total_incidents': np.random.randint(10, 50),\n",
    "        'resolved_incidents': np.random.randint(8, 50),\n",
    "        'success_rate': np.random.uniform(0.85, 0.98),\n",
    "        'avg_mttr_seconds': np.random.randint(10, 120),\n",
    "        'failure_rate': np.random.uniform(0.02, 0.15)\n",
    "    }\n",
    "    for i in range(30)  # 30 days of data\n",
    "])\n",
    "\n",
    "# Save tracking data\n",
    "tracking_file = PROCESSED_DIR / 'healing_success_tracking.parquet'\n",
    "healing_tracking.to_parquet(tracking_file)\n",
    "\n",
    "# Save report\n",
    "report_file = REPORTS_DIR / f\"healing_success_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "logger.info(f\"Saved healing success tracking data and report\")\n",
    "print(healing_tracking.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify outputs\n",
    "assert tracking_file.exists(), \"Healing tracking file not created\"\n",
    "assert report_file.exists(), \"Report file not created\"\n",
    "\n",
    "avg_success_rate = healing_tracking['success_rate'].mean()\n",
    "avg_mttr = healing_tracking['avg_mttr_seconds'].mean()\n",
    "total_incidents = healing_tracking['total_incidents'].sum()\n",
    "\n",
    "logger.info(f\"✅ All validations passed\")\n",
    "print(f\"\\nHealing Success Tracking Summary:\")\n",
    "print(f\"  Tracking Records: {len(healing_tracking)}\")\n",
    "print(f\"  Average Success Rate: {avg_success_rate:.1%}\")\n",
    "print(f\"  Average MTTR: {avg_mttr:.1f}s\")\n",
    "print(f\"  Total Incidents: {total_incidents}\")\n",
    "print(f\"  Success Target: {SUCCESS_THRESHOLD:.0%}\")\n",
    "print(f\"  Target Met: {'✅ Yes' if avg_success_rate >= SUCCESS_THRESHOLD else '❌ No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Remediation history and incident data\n",
    "- **Output**: Success metrics, reports, and recommendations\n",
    "- **Monitoring**: Success rates, MTTR, and failure patterns\n",
    "- **Next**: Phase 7 complete\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Monitor healing success continuously\n",
    "2. Review reports and recommendations\n",
    "3. Implement improvements based on analysis\n",
    "4. Optimize remediation strategies\n",
    "5. Complete notebook roadmap implementation\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-003: Self-Healing Platform Architecture\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [MTTR Metrics](https://en.wikipedia.org/wiki/Mean_time_to_recovery)\n",
    "- [Incident Management Best Practices](https://en.wikipedia.org/wiki/Incident_management)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Anomaly Response\n",
    "\n",
    "## Overview\n",
    "This notebook detects network anomalies (latency, packet loss, connectivity issues) and executes network healing actions. It monitors network health and responds to connectivity problems automatically.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `resource-exhaustion-detection.ipynb`\n",
    "- Network monitoring enabled\n",
    "- Prometheus metrics available\n",
    "- Coordination engine accessible\n",
    "\n",
    "## Learning Objectives\n",
    "- Detect network anomalies\n",
    "- Analyze connectivity issues\n",
    "- Execute network healing actions\n",
    "- Monitor network recovery\n",
    "- Track network health metrics\n",
    "\n",
    "## Key Concepts\n",
    "- **Network Metrics**: Latency, packet loss, throughput\n",
    "- **Connectivity Issues**: DNS failures, connection timeouts\n",
    "- **Network Healing**: Restart services, update routes\n",
    "- **Health Checks**: Verify connectivity restoration\n",
    "- **Performance Monitoring**: Track network metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"✅ Utils path found: {utils_path}\")\nelse:\n    print(\"⚠️ Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import setup_environment\n    print(\"✅ Common functions imported\")\nexcept ImportError as e:\n    print(f\"⚠️ Common functions not available: {e}\")\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Setup environment\nenv_info = setup_environment()\nlogger.info(f\"Environment ready: {env_info}\")\n\n# Define paths\nDATA_DIR = Path('/opt/app-root/src/data')\nPROCESSED_DIR = DATA_DIR / 'processed'\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n\n# Configuration\nNAMESPACE = 'self-healing-platform'\nLATENCY_THRESHOLD = 500  # milliseconds\nPACKET_LOSS_THRESHOLD = 5  # percentage\nCONNECTION_TIMEOUT = 10  # seconds\n\nlogger.info(f\"Network anomaly response initialized\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Detect Network Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_network_anomalies(namespace, latency_threshold=500, packet_loss_threshold=5):\n",
    "    \"\"\"\n",
    "    Detect network anomalies in cluster.\n",
    "    \n",
    "    Args:\n",
    "        namespace: Kubernetes namespace\n",
    "        latency_threshold: Latency threshold in ms\n",
    "        packet_loss_threshold: Packet loss threshold in %\n",
    "    \n",
    "    Returns:\n",
    "        List of detected network anomalies\n",
    "    \"\"\"\n",
    "    anomalies = []\n",
    "    \n",
    "    try:\n",
    "        # Sample network metrics\n",
    "        network_metrics = [\n",
    "            {'service': 'api-service', 'latency_ms': 650, 'packet_loss': 2.5, 'status': 'degraded'},\n",
    "            {'service': 'database-service', 'latency_ms': 200, 'packet_loss': 0.1, 'status': 'healthy'},\n",
    "            {'service': 'cache-service', 'latency_ms': 1200, 'packet_loss': 8.5, 'status': 'critical'},\n",
    "        ]\n",
    "        \n",
    "        for metric in network_metrics:\n",
    "            if metric['latency_ms'] > latency_threshold or metric['packet_loss'] > packet_loss_threshold:\n",
    "                anomalies.append({\n",
    "                    'service': metric['service'],\n",
    "                    'latency_ms': metric['latency_ms'],\n",
    "                    'packet_loss': metric['packet_loss'],\n",
    "                    'severity': 'critical' if metric['latency_ms'] > 1000 else 'high',\n",
    "                    'detected_at': datetime.now().isoformat()\n",
    "                })\n",
    "                logger.warning(f\"Network anomaly detected: {metric['service']}\")\n",
    "        \n",
    "        return anomalies\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error detecting network anomalies: {e}\")\n",
    "        return []\n",
    "\n",
    "# Detect anomalies\n",
    "network_anomalies = detect_network_anomalies(NAMESPACE, LATENCY_THRESHOLD, PACKET_LOSS_THRESHOLD)\n",
    "logger.info(f\"Detected {len(network_anomalies)} network anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyze Connectivity Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_connectivity(service_name, namespace):\n",
    "    \"\"\"\n",
    "    Analyze connectivity issues for a service.\n",
    "    \n",
    "    Args:\n",
    "        service_name: Service name\n",
    "        namespace: Kubernetes namespace\n",
    "    \n",
    "    Returns:\n",
    "        Connectivity analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Simulate connectivity checks\n",
    "        connectivity_checks = {\n",
    "            'dns_resolution': True,\n",
    "            'tcp_connection': False,\n",
    "            'http_health_check': False,\n",
    "            'service_endpoints': 2,\n",
    "            'network_policies': 'blocking_traffic'\n",
    "        }\n",
    "        \n",
    "        analysis = {\n",
    "            'service': service_name,\n",
    "            'checks': connectivity_checks,\n",
    "            'root_cause': 'network_policy' if connectivity_checks['network_policies'] == 'blocking_traffic' else 'service_unavailable',\n",
    "            'analysis_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Analyzed connectivity for {service_name}\")\n",
    "        return analysis\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing connectivity: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Analyze connectivity\n",
    "connectivity_analyses = []\n",
    "for anomaly in network_anomalies:\n",
    "    analysis = analyze_connectivity(anomaly['service'], NAMESPACE)\n",
    "    connectivity_analyses.append(analysis)\n",
    "\n",
    "logger.info(f\"Analyzed connectivity for {len(connectivity_analyses)} services\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Execute Network Healing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_network_healing(analysis, namespace):\n",
    "    \"\"\"\n",
    "    Execute network healing actions.\n",
    "    \n",
    "    Args:\n",
    "        analysis: Connectivity analysis\n",
    "        namespace: Kubernetes namespace\n",
    "    \n",
    "    Returns:\n",
    "        Healing result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        root_cause = analysis.get('root_cause', 'unknown')\n",
    "        service = analysis['service']\n",
    "        \n",
    "        healing_actions = {\n",
    "            'network_policy': {\n",
    "                'action': 'update_network_policy',\n",
    "                'command': f'oc patch networkpolicy -n {namespace}'\n",
    "            },\n",
    "            'service_unavailable': {\n",
    "                'action': 'restart_service',\n",
    "                'command': f'oc rollout restart deployment {service} -n {namespace}'\n",
    "            },\n",
    "            'dns_failure': {\n",
    "                'action': 'restart_dns',\n",
    "                'command': f'oc rollout restart deployment coredns -n kube-system'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        action_config = healing_actions.get(root_cause, healing_actions['service_unavailable'])\n",
    "        \n",
    "        logger.info(f\"Executing {action_config['action']} for {service}\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'service': service,\n",
    "            'action': action_config['action'],\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Healing error: {e}\")\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "# Execute healing\n",
    "healing_results = []\n",
    "for analysis in connectivity_analyses:\n",
    "    result = execute_network_healing(analysis, NAMESPACE)\n",
    "    healing_results.append(result)\n",
    "\n",
    "logger.info(f\"Executed network healing for {len(healing_results)} services\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Verify Network Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_network_recovery(service_name, namespace):\n",
    "    \"\"\"\n",
    "    Verify network recovery after healing.\n",
    "    \n",
    "    Args:\n",
    "        service_name: Service name\n",
    "        namespace: Kubernetes namespace\n",
    "    \n",
    "    Returns:\n",
    "        Recovery verification result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Simulate recovery checks\n",
    "        recovery_metrics = {\n",
    "            'latency_ms': np.random.randint(100, 300),\n",
    "            'packet_loss': np.random.uniform(0, 1),\n",
    "            'connection_success_rate': np.random.uniform(0.95, 1.0),\n",
    "            'service_healthy': True\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'service': service_name,\n",
    "            'recovered': recovery_metrics['service_healthy'],\n",
    "            'metrics': recovery_metrics,\n",
    "            'verification_time': datetime.now().isoformat()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error verifying recovery: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Verify recovery\n",
    "recovery_verifications = []\n",
    "for anomaly in network_anomalies:\n",
    "    verification = verify_network_recovery(anomaly['service'], NAMESPACE)\n",
    "    recovery_verifications.append(verification)\n",
    "\n",
    "logger.info(f\"Verified recovery for {len(recovery_verifications)} services\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Track Network Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network health tracking dataframe\n",
    "network_tracking = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'service': anomaly['service'],\n",
    "        'initial_latency_ms': anomaly['latency_ms'],\n",
    "        'initial_packet_loss': anomaly['packet_loss'],\n",
    "        'healing_action': healing_results[i]['action'] if i < len(healing_results) else 'none',\n",
    "        'recovery_success': recovery_verifications[i]['recovered'] if i < len(recovery_verifications) else False,\n",
    "        'final_latency_ms': recovery_verifications[i]['metrics']['latency_ms'] if i < len(recovery_verifications) else None,\n",
    "        'final_packet_loss': recovery_verifications[i]['metrics']['packet_loss'] if i < len(recovery_verifications) else None\n",
    "    }\n",
    "    for i, anomaly in enumerate(network_anomalies)\n",
    "])\n",
    "\n",
    "# Save tracking data\n",
    "tracking_file = PROCESSED_DIR / 'network_anomaly_response.parquet'\n",
    "network_tracking.to_parquet(tracking_file)\n",
    "\n",
    "logger.info(f\"Saved network health tracking data\")\n",
    "print(network_tracking.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify outputs\n",
    "assert len(network_anomalies) > 0, \"No network anomalies detected\"\n",
    "assert len(connectivity_analyses) > 0, \"No connectivity analyses performed\"\n",
    "assert len(healing_results) > 0, \"No healing executed\"\n",
    "assert tracking_file.exists(), \"Network tracking file not created\"\n",
    "\n",
    "recovery_rate = network_tracking['recovery_success'].sum() / len(network_tracking) if len(network_tracking) > 0 else 0\n",
    "logger.info(f\"✅ All validations passed\")\n",
    "print(f\"\\nNetwork Anomaly Response Summary:\")\n",
    "print(f\"  Anomalies Detected: {len(network_anomalies)}\")\n",
    "print(f\"  Healing Actions Executed: {len(healing_results)}\")\n",
    "print(f\"  Recovery Success Rate: {recovery_rate:.1%}\")\n",
    "if len(network_tracking) > 0:\n",
    "    print(f\"  Avg Latency Improvement: {(network_tracking['initial_latency_ms'].mean() - network_tracking['final_latency_ms'].mean()):.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Network metrics and connectivity checks\n",
    "- **Output**: Network healing actions and recovery tracking\n",
    "- **Monitoring**: Prometheus for network health metrics\n",
    "- **Next**: Complete platform demonstration\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Monitor network health metrics\n",
    "2. Proceed to `complete-platform-demo.ipynb`\n",
    "3. Run full end-to-end workflow\n",
    "4. Demonstrate all healing capabilities\n",
    "5. Validate complete platform functionality\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-003: Self-Healing Platform Architecture\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [Kubernetes Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n",
    "- [OpenShift Network Troubleshooting](https://docs.openshift.com/container-platform/latest/networking/troubleshooting-network.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

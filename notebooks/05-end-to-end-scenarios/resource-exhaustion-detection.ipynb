{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Exhaustion Detection\n",
    "\n",
    "## Overview\n",
    "This notebook detects resource exhaustion (CPU, memory) before it causes service degradation. It predicts when resources will be exhausted and triggers proactive scaling or optimization actions.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `pod-crash-loop-healing.ipynb`\n",
    "- Prometheus metrics available\n",
    "- Inference pipeline deployed\n",
    "- Coordination engine accessible\n",
    "\n",
    "## Learning Objectives\n",
    "- Monitor resource usage trends\n",
    "- Predict resource exhaustion\n",
    "- Trigger proactive scaling\n",
    "- Optimize resource allocation\n",
    "- Track scaling effectiveness\n",
    "\n",
    "## Key Concepts\n",
    "- **Resource Metrics**: CPU, memory, disk usage\n",
    "- **Trend Analysis**: Detect increasing resource usage\n",
    "- **Prediction**: Forecast when resources will be exhausted\n",
    "- **Proactive Scaling**: Scale before exhaustion occurs\n",
    "- **Optimization**: Adjust resource requests/limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"✅ Utils path found: {utils_path}\")\nelse:\n    print(\"⚠️ Utils path not found - will use fallback implementations\")\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import setup_environment\n    print(\"✅ Common functions imported\")\nexcept ImportError as e:\n    print(f\"⚠️ Common functions not available: {e}\")\n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Setup environment\nenv_info = setup_environment()\nlogger.info(f\"Environment ready: {env_info}\")\n\n# Define paths\nDATA_DIR = Path('/opt/app-root/src/data')\nPROCESSED_DIR = DATA_DIR / 'processed'\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n\n# Configuration\nNAMESPACE = 'self-healing-platform'\nCPU_THRESHOLD = 80  # CPU usage threshold (%)\nMEMORY_THRESHOLD = 85  # Memory usage threshold (%)\nPREDICTION_WINDOW = 24  # Hours to predict ahead\n\nlogger.info(f\"Resource exhaustion detection initialized\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Section\n",
    "\n",
    "### 1. Collect Resource Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_resource_metrics(namespace, hours=24):\n",
    "    \"\"\"\n",
    "    Collect resource metrics from Prometheus.\n",
    "    \n",
    "    Args:\n",
    "        namespace: Kubernetes namespace\n",
    "        hours: Historical data window\n",
    "    \n",
    "    Returns:\n",
    "        Resource metrics dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate sample metrics\n",
    "        timestamps = pd.date_range(end=datetime.now(), periods=hours, freq='1H')\n",
    "        \n",
    "        # Simulate increasing resource usage\n",
    "        cpu_usage = np.linspace(30, 85, hours) + np.random.normal(0, 5, hours)\n",
    "        memory_usage = np.linspace(40, 90, hours) + np.random.normal(0, 5, hours)\n",
    "        disk_usage = np.linspace(50, 75, hours) + np.random.normal(0, 3, hours)\n",
    "        \n",
    "        metrics = pd.DataFrame({\n",
    "            'timestamp': timestamps,\n",
    "            'cpu_usage': np.clip(cpu_usage, 0, 100),\n",
    "            'memory_usage': np.clip(memory_usage, 0, 100),\n",
    "            'disk_usage': np.clip(disk_usage, 0, 100),\n",
    "            'pod_count': np.random.randint(5, 15, hours),\n",
    "            'request_rate': np.random.randint(100, 500, hours)\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Collected {len(metrics)} resource metrics\")\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error collecting metrics: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Collect metrics\n",
    "resource_metrics = collect_resource_metrics(NAMESPACE, hours=24)\n",
    "logger.info(f\"Resource metrics shape: {resource_metrics.shape}\")\n",
    "print(resource_metrics.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyze Resource Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_resource_trends(metrics):\n",
    "    \"\"\"\n",
    "    Analyze resource usage trends.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Resource metrics dataframe\n",
    "    \n",
    "    Returns:\n",
    "        Trend analysis result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        trends = {}\n",
    "        \n",
    "        for resource in ['cpu_usage', 'memory_usage', 'disk_usage']:\n",
    "            # Calculate trend\n",
    "            x = np.arange(len(metrics))\n",
    "            y = metrics[resource].values\n",
    "            \n",
    "            # Linear regression\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            \n",
    "            # Current value\n",
    "            current = y[-1]\n",
    "            \n",
    "            trends[resource] = {\n",
    "                'current': float(current),\n",
    "                'slope': float(slope),\n",
    "                'trend': 'increasing' if slope > 0.5 else 'stable' if slope > -0.5 else 'decreasing',\n",
    "                'r_squared': float(r_value ** 2),\n",
    "                'hours_to_threshold': None\n",
    "            }\n",
    "            \n",
    "            # Calculate hours to threshold\n",
    "            threshold = 90 if resource != 'disk_usage' else 85\n",
    "            if slope > 0:\n",
    "                hours_to_threshold = (threshold - current) / slope\n",
    "                if hours_to_threshold > 0:\n",
    "                    trends[resource]['hours_to_threshold'] = float(hours_to_threshold)\n",
    "        \n",
    "        logger.info(f\"Analyzed resource trends\")\n",
    "        return trends\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing trends: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Analyze trends\n",
    "resource_trends = analyze_resource_trends(resource_metrics)\n",
    "print(json.dumps(resource_trends, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Predict Resource Exhaustion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_exhaustion(trends, prediction_window=24):\n",
    "    \"\"\"\n",
    "    Predict resource exhaustion within prediction window.\n",
    "    \n",
    "    Args:\n",
    "        trends: Resource trend analysis\n",
    "        prediction_window: Hours to predict ahead\n",
    "    \n",
    "    Returns:\n",
    "        Exhaustion predictions\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for resource, trend_data in trends.items():\n",
    "        hours_to_threshold = trend_data.get('hours_to_threshold')\n",
    "        \n",
    "        if hours_to_threshold is not None and hours_to_threshold <= prediction_window:\n",
    "            predictions.append({\n",
    "                'resource': resource,\n",
    "                'current_usage': trend_data['current'],\n",
    "                'trend': trend_data['trend'],\n",
    "                'hours_to_exhaustion': hours_to_threshold,\n",
    "                'action_required': True,\n",
    "                'urgency': 'critical' if hours_to_threshold < 6 else 'high' if hours_to_threshold < 12 else 'medium'\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"Predicted {len(predictions)} resource exhaustion events\")\n",
    "    return predictions\n",
    "\n",
    "# Predict exhaustion\n",
    "exhaustion_predictions = predict_exhaustion(resource_trends, PREDICTION_WINDOW)\n",
    "print(json.dumps(exhaustion_predictions, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Trigger Scaling Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_scaling(predictions, namespace):\n",
    "    \"\"\"\n",
    "    Trigger scaling actions based on predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Exhaustion predictions\n",
    "        namespace: Kubernetes namespace\n",
    "    \n",
    "    Returns:\n",
    "        Scaling actions executed\n",
    "    \"\"\"\n",
    "    scaling_actions = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        resource = prediction['resource']\n",
    "        urgency = prediction['urgency']\n",
    "        \n",
    "        if resource == 'cpu_usage':\n",
    "            action = {\n",
    "                'type': 'scale_replicas',\n",
    "                'target': 'deployment',\n",
    "                'replicas': 5 if urgency == 'critical' else 3,\n",
    "                'reason': f\"CPU exhaustion predicted in {prediction['hours_to_exhaustion']:.1f} hours\"\n",
    "            }\n",
    "        elif resource == 'memory_usage':\n",
    "            action = {\n",
    "                'type': 'increase_memory_limit',\n",
    "                'target': 'pod',\n",
    "                'memory_limit': '2Gi' if urgency == 'critical' else '1.5Gi',\n",
    "                'reason': f\"Memory exhaustion predicted in {prediction['hours_to_exhaustion']:.1f} hours\"\n",
    "            }\n",
    "        else:\n",
    "            action = {\n",
    "                'type': 'cleanup_disk',\n",
    "                'target': 'node',\n",
    "                'reason': f\"Disk exhaustion predicted in {prediction['hours_to_exhaustion']:.1f} hours\"\n",
    "            }\n",
    "        \n",
    "        logger.info(f\"Executing scaling action: {action['type']}\")\n",
    "        scaling_actions.append(action)\n",
    "    \n",
    "    return scaling_actions\n",
    "\n",
    "# Trigger scaling\n",
    "scaling_actions = trigger_scaling(exhaustion_predictions, NAMESPACE)\n",
    "print(json.dumps(scaling_actions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Track Scaling Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaling tracking dataframe\n",
    "scaling_tracking = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'resource': pred['resource'],\n",
    "        'action_type': scaling_actions[i]['type'] if i < len(scaling_actions) else 'none',\n",
    "        'urgency': pred['urgency'],\n",
    "        'hours_to_exhaustion': pred['hours_to_exhaustion'],\n",
    "        'scaling_success': True,\n",
    "        'resource_after_scaling': pred['current_usage'] - np.random.uniform(5, 15)\n",
    "    }\n",
    "    for i, pred in enumerate(exhaustion_predictions)\n",
    "])\n",
    "\n",
    "# Save tracking data\n",
    "tracking_file = PROCESSED_DIR / 'resource_exhaustion_scaling.parquet'\n",
    "scaling_tracking.to_parquet(tracking_file)\n",
    "\n",
    "logger.info(f\"Saved scaling tracking data\")\n",
    "print(scaling_tracking.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify outputs\n",
    "assert len(resource_metrics) > 0, \"No resource metrics collected\"\n",
    "assert len(resource_trends) > 0, \"No trends analyzed\"\n",
    "assert tracking_file.exists(), \"Scaling tracking file not created\"\n",
    "\n",
    "logger.info(f\"✅ All validations passed\")\n",
    "print(f\"\\nResource Exhaustion Detection Summary:\")\n",
    "print(f\"  Metrics Collected: {len(resource_metrics)}\")\n",
    "print(f\"  Exhaustion Predictions: {len(exhaustion_predictions)}\")\n",
    "print(f\"  Scaling Actions Triggered: {len(scaling_actions)}\")\n",
    "if len(scaling_tracking) > 0:\n",
    "    print(f\"  Scaling Success Rate: {scaling_tracking['scaling_success'].sum() / len(scaling_tracking):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Section\n",
    "\n",
    "This notebook integrates with:\n",
    "- **Input**: Prometheus metrics and trend analysis\n",
    "- **Output**: Scaling actions and effectiveness tracking\n",
    "- **Monitoring**: Real-time resource usage monitoring\n",
    "- **Next**: Network anomaly response\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Monitor scaling effectiveness\n",
    "2. Proceed to `network-anomaly-response.ipynb`\n",
    "3. Implement network healing workflows\n",
    "4. Test complete end-to-end scenarios\n",
    "5. Validate resource optimization\n",
    "\n",
    "## References\n",
    "\n",
    "- ADR-003: Self-Healing Platform Architecture\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [Kubernetes Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n",
    "- [Resource Requests and Limits](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Self-Healing Platform - Environment Setup\n",
    "\n",
    "**Phase**: Setup (00)  \n",
    "**Objective**: Verify and configure the workbench environment  \n",
    "**Time**: 5-10 minutes  \n",
    "**Status**: ‚úÖ Ready\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook verifies that your RHODS workbench is properly configured for executing all 30 notebooks in the Self-Healing Platform.\n",
    "\n",
    "### What This Notebook Does\n",
    "\n",
    "1. ‚úÖ Verifies Python and PyTorch installation\n",
    "2. ‚úÖ Checks GPU availability\n",
    "3. ‚úÖ Verifies persistent storage volumes\n",
    "4. ‚úÖ Tests required dependencies\n",
    "5. ‚úÖ Creates necessary directories\n",
    "6. ‚úÖ Generates setup summary report\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- You're in the RHODS workbench\n",
    "- Repository is cloned to `/opt/app-root/src/openshift-aiops-platform/`\n",
    "- You have terminal access\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Verify Python and PyTorch Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PYTHON & PYTORCH VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Python version\n",
    "print(f\"\\n‚úì Python Version: {sys.version}\")\n",
    "print(f\"‚úì Python Executable: {sys.executable}\")\n",
    "\n",
    "# PyTorch version\n",
    "import torch\n",
    "print(f\"\\n‚úì PyTorch Version: {torch.__version__}\")\n",
    "print(f\"‚úì PyTorch Location: {torch.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GPU AVAILABILITY CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\n‚úì CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"‚úì GPU Device Count: {device_count}\")\n",
    "    \n",
    "    for i in range(device_count):\n",
    "        print(f\"  - GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Test GPU\n",
    "    x = torch.randn(1000, 1000).cuda()\n",
    "    y = torch.randn(1000, 1000).cuda()\n",
    "    z = torch.matmul(x, y)\n",
    "    print(f\"\\n‚úì GPU Test: PASSED (matrix multiplication successful)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  GPU Not Available - Will use CPU\")\n",
    "    print(\"   Note: Phase 2 LSTM notebook requires GPU\")\n",
    "    print(\"   You can still run other notebooks on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Persistent Storage Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERSISTENT STORAGE VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check data volume\n",
    "data_path = '/opt/app-root/src/data'\n",
    "models_path = '/opt/app-root/src/models'\n",
    "\n",
    "print(f\"\\n‚úì Data Volume: {data_path}\")\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"  Status: EXISTS\")\n",
    "    stat = shutil.disk_usage(data_path)\n",
    "    print(f\"  Total: {stat.total / 1e9:.2f} GB\")\n",
    "    print(f\"  Used: {stat.used / 1e9:.2f} GB\")\n",
    "    print(f\"  Free: {stat.free / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(f\"  Status: NOT FOUND\")\n",
    "\n",
    "print(f\"\\n‚úì Models Volume: {models_path}\")\n",
    "if os.path.exists(models_path):\n",
    "    print(f\"  Status: EXISTS\")\n",
    "    stat = shutil.disk_usage(models_path)\n",
    "    print(f\"  Total: {stat.total / 1e9:.2f} GB\")\n",
    "    print(f\"  Used: {stat.used / 1e9:.2f} GB\")\n",
    "    print(f\"  Free: {stat.free / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(f\"  Status: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Required Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Install Missing Dependencies (Optional)\n",
    "\n",
    "Run the cell below to install any missing packages. This is optional but recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport sys\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CHECKING/INSTALLING DEPENDENCIES\")\nprint(\"=\" * 80)\n\n# Packages that may need to be installed\npackages_to_check = {\n    'statsmodels': 'statsmodels',\n    'prophet': 'prophet',\n    'pyod': 'pyod',\n    'xgboost': 'xgboost',\n    'lightgbm': 'lightgbm',\n    'kserve': 'kserve',\n    'seaborn': 'seaborn',\n    'yaml': 'pyyaml',  # import name differs from pip name\n}\n\n# Check which packages are missing\nmissing_packages = []\nfor import_name, pip_name in packages_to_check.items():\n    try:\n        __import__(import_name)\n        print(f\"‚úì {pip_name} already installed\")\n    except ImportError:\n        missing_packages.append(pip_name)\n        print(f\"‚úó {pip_name} not found\")\n\nif missing_packages:\n    print(f\"\\nInstalling missing packages: {', '.join(missing_packages)}\")\n    print(\"This may take a few minutes...\\n\")\n    try:\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet'] + missing_packages)\n        print(\"\\n‚úÖ Installation complete!\")\n    except subprocess.CalledProcessError as e:\n        print(f\"\\n‚ö†Ô∏è  pip install failed (exit code {e.returncode})\")\n        print(\"   This is expected in read-only container environments.\")\n        print(\"   If running in validation mode, packages should be pre-installed in the image.\")\nelse:\n    print(\"\\n‚úÖ All dependencies already installed - skipping pip install\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEPENDENCY VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Core dependencies (required for all notebooks)\n",
    "core_dependencies = {\n",
    "    'numpy': 'NumPy',\n",
    "    'pandas': 'Pandas',\n",
    "    'sklearn': 'Scikit-learn',\n",
    "    'statsmodels': 'Statsmodels',\n",
    "    'prophet': 'Prophet',\n",
    "    'pyod': 'PyOD',\n",
    "    'xgboost': 'XGBoost',\n",
    "    'lightgbm': 'LightGBM',\n",
    "    'prometheus_client': 'Prometheus Client',\n",
    "    'matplotlib': 'Matplotlib',\n",
    "    'seaborn': 'Seaborn',\n",
    "    'plotly': 'Plotly',\n",
    "    'requests': 'Requests',\n",
    "    'yaml': 'PyYAML',  # Note: import as 'yaml', not 'pyyaml'\n",
    "}\n",
    "\n",
    "# Optional dependencies (nice to have, but not critical)\n",
    "optional_dependencies = {\n",
    "    'kubernetes': 'Kubernetes',  # Has dependency conflicts, optional\n",
    "    'kserve': 'KServe',  # Has dependency conflicts, optional\n",
    "}\n",
    "\n",
    "missing = []\n",
    "installed = []\n",
    "optional_missing = []\n",
    "\n",
    "print(\"\\nCore Dependencies:\")\n",
    "for module, name in core_dependencies.items():\n",
    "    try:\n",
    "        __import__(module)\n",
    "        installed.append(name)\n",
    "        print(f\"‚úì {name}\")\n",
    "    except ImportError:\n",
    "        missing.append(name)\n",
    "        print(f\"‚úó {name} - NOT INSTALLED\")\n",
    "\n",
    "print(\"\\nOptional Dependencies:\")\n",
    "for module, name in optional_dependencies.items():\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"‚úì {name}\")\n",
    "    except ImportError:\n",
    "        optional_missing.append(name)\n",
    "        print(f\"‚ö†Ô∏è  {name} - NOT INSTALLED (optional)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"Summary: {len(installed)}/{len(core_dependencies)} core dependencies installed\")\n",
    "print(f\"=\" * 80)\n",
    "\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n‚ùå Missing core dependencies: {', '.join(missing)}\")\n",
    "    print(f\"\\nTo install missing packages, run in terminal:\")\n",
    "    print(f\"pip install --user {' '.join(missing).lower()}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All core dependencies installed!\")\n",
    "\n",
    "if optional_missing:\n",
    "    print(f\"\\n‚ö†Ô∏è  Optional dependencies not installed: {', '.join(optional_missing)}\")\n",
    "    print(f\"   These are optional and may have dependency conflicts.\")\n",
    "    print(f\"   You can still run all notebooks without them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Necessary Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING NECESSARY DIRECTORIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "directories = [\n",
    "    '/opt/app-root/src/data/processed',\n",
    "    '/opt/app-root/src/data/training',\n",
    "    '/opt/app-root/src/data/reports',\n",
    "    '/opt/app-root/src/models/anomaly-detection',\n",
    "    '/opt/app-root/src/models/serving',\n",
    "    '/opt/app-root/src/models/checkpoints',\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"‚úì {directory}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All directories created/verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Setup Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SETUP SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'python_version': sys.version.split()[0],\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'cuda_available': torch.cuda.is_available(),\n",
    "    'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    'dependencies_installed': len(installed),\n",
    "    'dependencies_missing': len(missing),\n",
    "    'data_volume_exists': os.path.exists('/opt/app-root/src/data'),\n",
    "    'models_volume_exists': os.path.exists('/opt/app-root/src/models'),\n",
    "}\n",
    "\n",
    "print(f\"\\nSetup Timestamp: {summary['timestamp']}\")\n",
    "print(f\"Python Version: {summary['python_version']}\")\n",
    "print(f\"PyTorch Version: {summary['pytorch_version']}\")\n",
    "print(f\"CUDA Available: {summary['cuda_available']}\")\n",
    "print(f\"GPU Count: {summary['gpu_count']}\")\n",
    "print(f\"Dependencies: {summary['dependencies_installed']}/{len(core_dependencies)} installed\")\n",
    "print(f\"Data Volume: {'‚úì' if summary['data_volume_exists'] else '‚úó'}\")\n",
    "print(f\"Models Volume: {'‚úì' if summary['models_volume_exists'] else '‚úó'}\")\n",
    "\n",
    "# Save summary\n",
    "summary_path = '/opt/app-root/src/data/setup_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Setup summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Setup Complete!\n",
    "\n",
    "Your environment is ready for the Self-Healing Platform notebooks!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Review the summary above** - Check that all components are verified\n",
    "2. **If GPU is available** - Great! You can run all notebooks including Phase 2 LSTM\n",
    "3. **If GPU is not available** - You can still run all notebooks except Phase 2 LSTM on CPU\n",
    "4. **If dependencies are missing** - Run the pip install command shown above\n",
    "\n",
    "### Start Executing Notebooks\n",
    "\n",
    "Now you're ready to start with Phase 1:\n",
    "\n",
    "1. Navigate to: `notebooks/01-data-collection/`\n",
    "2. Open: `01-prometheus-metrics-collection.ipynb`\n",
    "3. Run all cells with \"Run All\" button\n",
    "4. Follow the execution checklist: `docs/NOTEBOOK-EXECUTION-CHECKLIST.md`\n",
    "\n",
    "### Execution Timeline\n",
    "\n",
    "- **Phase 1**: Data Collection (2-3 hours)\n",
    "- **Phase 2**: Anomaly Detection (3-4 hours)\n",
    "- **Phase 3**: Self-Healing Logic (2-3 hours)\n",
    "- **Phase 4**: Model Serving (2-3 hours)\n",
    "- **Phase 5**: End-to-End Scenarios (2-3 hours)\n",
    "- **Phase 6**: MCP & Lightspeed (2 hours)\n",
    "- **Phase 7**: Monitoring & Operations (2 hours)\n",
    "- **Phase 8**: Advanced Scenarios (2-3 hours)\n",
    "\n",
    "**Total: 18-24 hours**\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

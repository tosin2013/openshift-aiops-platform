# ADR-032: Infrastructure Validation Notebook for User Readiness

**Status**: Implemented
**Date**: 2025-11-04
**Renumbered From**: Originally ADR-029 (renumbered 2025-11-19 to resolve duplicate)
**Implementation**: Tier 1 validation notebook (`00-platform-readiness-validation.ipynb`)
**Deciders**: Platform Team
**Technical Story**: Comprehensive validation of platform infrastructure before users execute notebooks

## Context and Problem Statement

Users need to verify that all platform components (coordination engine, model serving, S3 storage, Prometheus, MCP server, etc.) are operational and accessible before executing the 30 notebooks in the Self-Healing Platform. Currently, `00-setup/environment-setup.ipynb` only validates basic workbench setup (Python, PyTorch, GPU) but doesn't verify platform infrastructure readiness.

### Current Gap

- ‚ùå No validation of coordination engine URL and health
- ‚ùå No verification of model serving endpoints (KServe)
- ‚ùå No check for S3/object storage accessibility
- ‚ùå No validation of Prometheus metrics API
- ‚ùå No MCP server connectivity test
- ‚ùå No ArgoCD application health check
- ‚ùå No Tekton pipeline availability verification
- ‚ùå No external secrets verification

**Problem**: Users may start notebook execution without realizing critical platform components are missing or misconfigured, leading to failures mid-workflow.

## Decision Drivers

1. **User Experience**: Users should know immediately if infrastructure is ready
2. **Fail Fast**: Detect misconfigurations before time-consuming notebook execution
3. **Dual Execution**: Must run both in Jupyter (manual) and Tekton (automated CI/CD)
4. **Comprehensive Coverage**: Validate all dependencies required by the 30 notebooks
5. **Clear Reporting**: Provide actionable error messages for failed checks
6. **Automation**: Enable CI/CD validation via Tekton pipelines

## Considered Options

### Option 1: Create Comprehensive Validation Notebook (SELECTED)
**Approach**: Create `notebooks/00-setup/00-platform-readiness-validation.ipynb` that checks all infrastructure components

**Checks to Implement**:
1. **Basic Environment** (from existing notebook)
   - Python version (3.11+)
   - PyTorch installation (2025.1)
   - GPU availability
   - Persistent storage (data, models volumes)

2. **Platform Infrastructure**
   - Coordination Engine: Health endpoint, API accessibility
   - Model Serving: KServe InferenceServices, serving runtimes
   - Object Storage: S3 bucket accessibility, credentials validation
   - Monitoring: Prometheus query API, Grafana dashboards
   - MCP Server: Cluster Health MCP connectivity, tool availability

3. **OpenShift Components**
   - ArgoCD Applications: Status and sync state
   - Tekton Pipelines: Pipeline availability, trigger configuration
   - External Secrets: SecretStore, ExternalSecrets status
   - Routes: Public endpoints accessibility

4. **Network Connectivity**
   - In-cluster service resolution
   - External API endpoints
   - Route accessibility (if needed)

**Outputs**:
- ‚úÖ Comprehensive validation report (JSON + Markdown)
- ‚úÖ Pass/fail status for each component
- ‚úÖ Actionable error messages with remediation steps
- ‚úÖ Summary dashboard (visual indicators)

### Option 2: Shell Script Validation
**Approach**: Create bash script with `curl` and `oc` commands

**Rejected Because**:
- ‚ùå Not executable within Jupyter notebooks
- ‚ùå Requires users to switch to terminal
- ‚ùå Harder to integrate with Tekton (needs custom task)
- ‚ùå No visual reporting in notebooks

### Option 3: Ansible Playbook Validation
**Approach**: Use Ansible playbook with validation tasks

**Rejected Because**:
- ‚ùå Not runnable within Jupyter notebooks
- ‚ùå Requires Ansible installation in workbench
- ‚ùå Overly complex for user-facing validation
- ‚úÖ (Keep for infrastructure deployment validation - already exists)

## Decision Outcome

**Chosen Option**: Option 1 - Comprehensive Validation Notebook

### Implementation Plan

#### Phase 1: Create Validation Notebook (Week 1)
1. Create `notebooks/00-setup/00-platform-readiness-validation.ipynb`
2. Implement validation functions in `notebooks/utils/validation_helpers.py`
3. Add visual reporting with status indicators
4. Generate JSON validation report for automation

#### Phase 2: Tekton Integration (Week 1)
1. Create Tekton task: `platform-readiness-validation-task.yaml`
2. Install `papermill` for programmatic notebook execution
3. Create pipeline: `platform-readiness-validation-pipeline.yaml`
4. Add webhook trigger for automated validation

#### Phase 3: Documentation & Testing (Week 1)
1. Update `notebooks/README.md` with validation workflow
2. Create troubleshooting guide for common failures
3. Test in RHODS workbench environment
4. Test via Tekton pipeline execution

### Validation Checklist

The notebook will validate the following components:

#### ‚úÖ Basic Environment
- [ ] Python 3.11+
- [ ] PyTorch 2025.1
- [ ] GPU available (warning if not, not blocking)
- [ ] Data volume mounted: `/opt/app-root/src/data` (20Gi)
- [ ] Models volume mounted: `/opt/app-root/src/models` (50Gi)
- [ ] Config directory writable: `/opt/app-root/src/.config`

#### ‚úÖ Platform Infrastructure
- [ ] Coordination Engine
  - URL: `http://coordination-engine.self-healing-platform.svc.cluster.local:8080`
  - Health endpoint: `/health` returns 200
  - Metrics endpoint: `/metrics` accessible
- [ ] Model Serving (KServe)
  - InferenceService `anomaly-detector` ready
  - InferenceService `predictive-analytics` ready
  - Serving runtimes deployed (sklearn, tensorflow)
- [ ] Object Storage (S3/ODF)
  - S3 endpoint accessible
  - Credentials available (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
  - Bucket `model-storage` exists
  - Upload/download test successful
- [ ] Monitoring Stack
  - Prometheus query API: `https://prometheus-k8s.openshift-monitoring.svc.cluster.local:9091/api/v1/query`
  - Grafana dashboards deployed
  - ServiceMonitor for coordination engine exists
- [ ] MCP Server
  - URL: `http://cluster-health-mcp-server.self-healing-platform.svc.cluster.local:3000`
  - Health endpoint accessible
  - Tool availability: `cluster_diagnostics`, `anomaly_detection`, `deployment_readiness`

#### ‚úÖ OpenShift Components
- [ ] ArgoCD Applications
  - Application `self-healing-platform` synced and healthy
  - No sync errors
- [ ] Tekton Pipelines
  - Pipeline `deployment-validation-pipeline` exists
  - Pipeline `model-serving-validation-pipeline` exists
  - EventListener `github-webhook-listener` running
- [ ] External Secrets
  - SecretStore `kubernetes-secret-store` ready
  - ExternalSecret `gitea-credentials` synced
  - ExternalSecret `registry-credentials` synced
  - ExternalSecret `database-credentials` synced
- [ ] Routes (if needed for external access)
  - Coordination engine route (if exists)
  - MCP server route (if exists)

#### ‚úÖ Network Connectivity
- [ ] In-cluster DNS resolution
- [ ] Service mesh connectivity (if enabled)
- [ ] Pod-to-pod communication

### Validation Report Format

**JSON Output** (`/opt/app-root/src/.config/validation-report.json`):
```json
{
  "validation_date": "2025-11-04T10:30:00Z",
  "validation_status": "PASSED",
  "summary": {
    "total_checks": 35,
    "passed": 33,
    "failed": 0,
    "warnings": 2
  },
  "checks": [
    {
      "category": "Platform Infrastructure",
      "component": "Coordination Engine",
      "check": "Health Endpoint",
      "status": "PASSED",
      "url": "http://coordination-engine.self-healing-platform.svc.cluster.local:8080/health",
      "response_time_ms": 45,
      "details": "HTTP 200 OK"
    },
    {
      "category": "Platform Infrastructure",
      "component": "GPU",
      "check": "GPU Availability",
      "status": "WARNING",
      "details": "GPU not available, will use CPU (acceptable for most notebooks)"
    }
  ]
}
```

**Markdown Output** (displayed in notebook):
```markdown
# üöÄ Platform Readiness Validation Report

**Status**: ‚úÖ PASSED
**Date**: 2025-11-04 10:30:00 UTC
**Checks**: 33/35 PASSED, 0 FAILED, 2 WARNINGS

## Summary

| Category | Passed | Failed | Warnings |
|----------|--------|--------|----------|
| Basic Environment | 5/6 | 0 | 1 (GPU) |
| Platform Infrastructure | 12/12 | 0 | 0 |
| OpenShift Components | 10/10 | 0 | 0 |
| Network Connectivity | 6/7 | 0 | 1 (External Route) |

## Component Status

### ‚úÖ Basic Environment
- ‚úÖ Python 3.11.5
- ‚úÖ PyTorch 2025.1
- ‚ö†Ô∏è  GPU: Not available (acceptable)
- ‚úÖ Data Volume: 20Gi mounted
- ‚úÖ Models Volume: 50Gi mounted
- ‚úÖ Config Directory: writable

### ‚úÖ Platform Infrastructure
- ‚úÖ Coordination Engine: http://coordination-engine... (45ms)
- ‚úÖ Model Serving: anomaly-detector (READY), predictive-analytics (READY)
- ‚úÖ Object Storage: s3://model-storage (accessible, 10MB test OK)
- ‚úÖ Monitoring: Prometheus query API (responsive)
- ‚úÖ MCP Server: http://cluster-health-mcp-server... (3 tools available)

... (detailed component checks)

## Next Steps

‚úÖ Platform is ready for notebook execution!

1. Start with Phase 01: Data Collection
2. Follow notebook execution order (01 ‚Üí 08)
3. Refer to notebooks/README.md for complete guide
```

## Implementation Details

### Tekton Integration

**Tekton Task**: `tekton/tasks/platform-readiness-validation-task.yaml`

```yaml
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: platform-readiness-validation
spec:
  description: Validate platform infrastructure readiness by executing validation notebook
  params:
    - name: notebook-path
      description: Path to validation notebook
      default: "notebooks/00-setup/00-platform-readiness-validation.ipynb"
    - name: output-path
      description: Path to save validation report
      default: "/workspace/validation-report.json"
  steps:
    - name: install-papermill
      image: quay.io/modh/runtime-images:jupyter-datascience-ubi9-python-3.11-2025.1
      script: |
        #!/bin/bash
        pip install papermill nbconvert

    - name: execute-notebook
      image: quay.io/modh/runtime-images:jupyter-datascience-ubi9-python-3.11-2025.1
      script: |
        #!/bin/bash
        set -e

        papermill \
          $(params.notebook-path) \
          /tmp/validation-output.ipynb \
          --log-output \
          --log-level INFO

        # Convert to HTML for artifact storage
        jupyter nbconvert \
          --to html \
          /tmp/validation-output.ipynb \
          --output /workspace/validation-report.html

        # Extract JSON report
        cat /opt/app-root/src/.config/validation-report.json > $(params.output-path)

    - name: check-validation-status
      image: registry.access.redhat.com/ubi9/ubi-minimal
      script: |
        #!/bin/bash
        STATUS=$(jq -r '.validation_status' $(params.output-path))

        if [ "$STATUS" != "PASSED" ]; then
          echo "‚ùå Platform validation FAILED"
          jq '.checks[] | select(.status == "FAILED")' $(params.output-path)
          exit 1
        fi

        echo "‚úÖ Platform validation PASSED"
  results:
    - name: validation-status
      description: Overall validation status (PASSED/FAILED)
    - name: checks-passed
      description: Number of checks passed
    - name: checks-failed
      description: Number of checks failed
```

**Tekton Pipeline**: `tekton/pipelines/platform-readiness-validation-pipeline.yaml`

```yaml
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: platform-readiness-validation-pipeline
spec:
  description: Validate platform readiness before user notebook execution
  tasks:
    - name: platform-readiness-validation
      taskRef:
        name: platform-readiness-validation
      params:
        - name: notebook-path
          value: "notebooks/00-setup/00-platform-readiness-validation.ipynb"
```

### Validation Helpers Module

**File**: `notebooks/utils/validation_helpers.py`

```python
"""
Platform validation helpers for Self-Healing Platform
Provides comprehensive infrastructure validation functions
"""

import requests
import os
import json
from typing import Dict, Any, List, Tuple
from datetime import datetime

def validate_coordination_engine() -> Dict[str, Any]:
    """Validate coordination engine health and accessibility"""
    url = "http://coordination-engine.self-healing-platform.svc.cluster.local:8080/health"
    try:
        response = requests.get(url, timeout=5)
        return {
            "status": "PASSED" if response.status_code == 200 else "FAILED",
            "url": url,
            "response_time_ms": int(response.elapsed.total_seconds() * 1000),
            "details": f"HTTP {response.status_code}"
        }
    except Exception as e:
        return {
            "status": "FAILED",
            "url": url,
            "details": str(e)
        }

def validate_model_serving() -> Dict[str, Any]:
    """Validate KServe InferenceServices"""
    # Implementation using OpenShift API
    pass

def validate_object_storage() -> Dict[str, Any]:
    """Validate S3/ODF accessibility"""
    # Implementation with boto3
    pass

def validate_mcp_server() -> Dict[str, Any]:
    """Validate MCP server connectivity"""
    # Implementation using MCP client
    pass

# ... (additional validation functions)

def generate_validation_report(checks: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Generate comprehensive validation report"""
    passed = sum(1 for c in checks if c['status'] == 'PASSED')
    failed = sum(1 for c in checks if c['status'] == 'FAILED')
    warnings = sum(1 for c in checks if c['status'] == 'WARNING')

    return {
        "validation_date": datetime.utcnow().isoformat() + "Z",
        "validation_status": "PASSED" if failed == 0 else "FAILED",
        "summary": {
            "total_checks": len(checks),
            "passed": passed,
            "failed": failed,
            "warnings": warnings
        },
        "checks": checks
    }
```

## Positive Consequences

- ‚úÖ **Early Detection**: Catch infrastructure issues before notebook execution
- ‚úÖ **Clear Feedback**: Users know exactly what's missing or misconfigured
- ‚úÖ **Automation-Ready**: Tekton integration enables CI/CD validation
- ‚úÖ **Self-Service**: Users can validate independently without admin help
- ‚úÖ **Comprehensive**: Covers all 35+ infrastructure dependencies
- ‚úÖ **Actionable**: Provides remediation steps for failures
- ‚úÖ **Reusable**: Can be run multiple times without side effects
- ‚úÖ **Visual**: Clear status indicators and summary dashboard
- ‚úÖ **Persistent**: JSON report stored for debugging and auditing

## Negative Consequences

- ‚ö†Ô∏è  **Maintenance Overhead**: Must update validation checks when new components added
- ‚ö†Ô∏è  **False Positives**: Network blips may cause transient failures
- ‚ö†Ô∏è  **Execution Time**: 35+ checks may take 2-3 minutes to complete
- ‚ö†Ô∏è  **Credentials Required**: Validation needs access to cluster APIs (RBAC)
- ‚ö†Ô∏è  **Version Drift**: Validation logic may lag behind infrastructure changes

### Mitigation Strategies

1. **Maintenance**: Include validation updates in ADRs for new components
2. **Reliability**: Implement retries (3 attempts) for network-dependent checks
3. **Performance**: Run checks in parallel where possible
4. **Security**: Use ServiceAccount with minimal RBAC permissions
5. **Documentation**: Keep validation checklist in sync with IMPLEMENTATION-PLAN.md

## Links

- **Related ADRs**:
  - ADR-012: Notebook Architecture for End-to-End Workflows
  - ADR-013: Data Collection and Preprocessing Workflows
  - ADR-021: Tekton Pipeline for Post-Deployment Validation
  - ADR-027: CI/CD Pipeline Automation with Tekton and ArgoCD

- **Implementation**:
  - Notebook: `notebooks/00-setup/00-platform-readiness-validation.ipynb`
  - Validation Helpers: `notebooks/utils/validation_helpers.py`
  - Tekton Task: `tekton/tasks/platform-readiness-validation-task.yaml`
  - Tekton Pipeline: `tekton/pipelines/platform-readiness-validation-pipeline.yaml`

- **Documentation**:
  - Notebooks README: `notebooks/README.md`
  - Implementation Plan: `docs/IMPLEMENTATION-PLAN.md`
  - Troubleshooting Guide: `docs/TROUBLESHOOTING.md` (to be created)

## Acceptance Criteria

- [ ] Validation notebook created with 35+ infrastructure checks
- [ ] All validation functions implemented in `validation_helpers.py`
- [ ] JSON report generation working
- [ ] Markdown report rendered in notebook
- [ ] Tekton task executes notebook via papermill
- [ ] Tekton pipeline integrates validation task
- [ ] Webhook trigger configured (optional)
- [ ] Documentation updated (notebooks/README.md)
- [ ] Tested in RHODS workbench
- [ ] Tested via Tekton pipeline
- [ ] All validation checks passing on target cluster

## Success Metrics

- **Coverage**: 35+ infrastructure components validated
- **Accuracy**: <5% false positive rate
- **Performance**: Validation completes in <3 minutes
- **Reliability**: 99% success rate for valid infrastructure
- **Adoption**: 100% of users run validation before notebooks
- **Automation**: Validation runs automatically on every deployment

---

**Status**: Proposed
**Next Steps**: Review and approval, then implementation Phase 1

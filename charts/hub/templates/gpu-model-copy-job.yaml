{{- if and .Values.modelServing.enabled .Values.models }}
{{- range .Values.models }}
{{- if .gpuTrained }}
{{- /*
  GPU Model Copy Job: {{ .name }}

  Purpose: Copies trained model from GPU PVC (gp3-csi, RWO) to shared PVC
           (CephFS, RWX) immediately after GPU training completes.

  Why: GPU nodes cannot mount CephFS, so the training notebook writes to the
       GPU PVC. This job runs on a non-GPU node that can mount both PVCs and
       copies the model before downstream consumers (InferenceServices,
       validation pipeline) need it.

  Sync Wave Timeline:
    wave 3:  GPU training writes model.pkl to model-storage-gpu-pvc
    wave 4:  This job copies model.pkl to model-storage-pvc (CephFS)  <-- HERE
    wave 5:  restart-predictors-job finds model on CephFS
    wave 11: model-restart-job runs as safety net

  The RWO GPU PVC can only be mounted by one pod at a time. This job stays
  Pending until the training pod releases the PVC, then the init container
  polls for the model file before the main container copies it.
*/ -}}
---
apiVersion: batch/v1
kind: Job
metadata:
  name: gpu-model-copy-{{ .name }}
  namespace: {{ $.Values.main.namespace }}
  labels:
    app.kubernetes.io/component: model-serving
    app.kubernetes.io/name: gpu-model-copy
    app.kubernetes.io/part-of: self-healing-platform
    model-name: {{ .name }}
  annotations:
    argocd.argoproj.io/sync-wave: "4"
    argocd.argoproj.io/hook: PostSync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
    description: "Copies GPU-trained model {{ .name }} from RWO PVC to CephFS RWX PVC"
spec:
  backoffLimit: 5
  activeDeadlineSeconds: 900
  template:
    metadata:
      labels:
        app.kubernetes.io/component: model-serving
        app.kubernetes.io/name: gpu-model-copy
        model-name: {{ .name }}
    spec:
      restartPolicy: OnFailure
      # No GPU tolerations or nodeSelector — must run on a non-GPU node
      # that can mount CephFS (RWX)
      initContainers:
      - name: wait-for-gpu-model
        image: registry.access.redhat.com/ubi9/ubi-minimal:latest
        command:
        - /bin/bash
        - -c
        - |
          set -e
          MODEL_NAME="{{ .name }}"
          GPU_MODEL_PATH="/mnt/models-gpu/${MODEL_NAME}/model.pkl"
          MAX_WAIT=600  # 10 minutes
          ELAPSED=0
          INTERVAL=10

          echo "=========================================="
          echo "Waiting for GPU-trained model: $MODEL_NAME"
          echo "Expected path: $GPU_MODEL_PATH"
          echo "Max wait: ${MAX_WAIT}s"
          echo "=========================================="

          while [ $ELAPSED -lt $MAX_WAIT ]; do
            if [ -f "$GPU_MODEL_PATH" ]; then
              FILE_SIZE=$(stat -c%s "$GPU_MODEL_PATH" 2>/dev/null || echo "0")
              if [ "$FILE_SIZE" -gt 0 ]; then
                echo "✓ Model file found: $GPU_MODEL_PATH (size: $FILE_SIZE bytes)"
                exit 0
              fi
              echo "⏳ Model file exists but is empty, waiting for write to complete..."
            else
              echo "⏳ Model not yet available, waiting ${INTERVAL}s... (${ELAPSED}s/${MAX_WAIT}s)"
            fi
            sleep $INTERVAL
            ELAPSED=$((ELAPSED + INTERVAL))
          done

          echo "✗ ERROR: GPU model not found after ${MAX_WAIT}s: $GPU_MODEL_PATH"
          exit 1
        volumeMounts:
        - name: model-storage-gpu
          mountPath: /mnt/models-gpu
      containers:
      - name: copy-model
        image: registry.access.redhat.com/ubi9/ubi-minimal:latest
        command:
        - /bin/bash
        - -c
        - |
          set -e
          MODEL_NAME="{{ .name }}"
          GPU_PATH="/mnt/models-gpu/${MODEL_NAME}/model.pkl"
          FINAL_PATH="/mnt/models/${MODEL_NAME}/model.pkl"
          FINAL_DIR="/mnt/models/${MODEL_NAME}"

          echo "=========================================="
          echo "Copying GPU model to shared storage"
          echo "=========================================="
          echo "Source: $GPU_PATH"
          echo "Target: $FINAL_PATH"

          # Verify source exists (init container already checked, but be safe)
          if [ ! -f "$GPU_PATH" ]; then
            echo "✗ ERROR: Source model not found: $GPU_PATH"
            exit 1
          fi

          GPU_SIZE=$(stat -c%s "$GPU_PATH" 2>/dev/null || echo "0")
          echo "Source size: $GPU_SIZE bytes"

          # Create target directory
          mkdir -p "$FINAL_DIR"

          # Copy model file
          echo "Copying model..."
          cp -f "$GPU_PATH" "$FINAL_PATH"

          # Verify copy
          if [ ! -f "$FINAL_PATH" ]; then
            echo "✗ ERROR: Copy failed — target file not found after cp"
            exit 1
          fi

          TARGET_SIZE=$(stat -c%s "$FINAL_PATH" 2>/dev/null || echo "0")
          echo "Target size: $TARGET_SIZE bytes"

          if [ "$GPU_SIZE" != "$TARGET_SIZE" ]; then
            echo "✗ ERROR: Size mismatch — source: $GPU_SIZE, target: $TARGET_SIZE"
            exit 1
          fi

          if [ "$TARGET_SIZE" -eq 0 ]; then
            echo "✗ ERROR: Copied file is empty"
            exit 1
          fi

          echo "=========================================="
          echo "✓ Model copied successfully!"
          echo "  Model:  $MODEL_NAME"
          echo "  Size:   $TARGET_SIZE bytes"
          echo "=========================================="
        volumeMounts:
        - name: model-storage-gpu
          mountPath: /mnt/models-gpu
        - name: model-storage
          mountPath: /mnt/models
      volumes:
      - name: model-storage-gpu
        persistentVolumeClaim:
          claimName: model-storage-gpu-pvc
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-storage-pvc
{{- end }}
{{- end }}
{{- end }}

{{- if .Values.modelServing.enabled }}
---
# KServe sklearn ServingRuntime (in application namespace)
# Enables serving of scikit-learn models through KServe
# This runtime is required for InferenceServices that use sklearn predictor
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: sklearn-runtime
  namespace: {{ .Values.main.namespace }}
  labels:
    app.kubernetes.io/name: kserve-sklearn-runtime
    app.kubernetes.io/component: model-serving
    app.kubernetes.io/part-of: self-healing-platform
spec:
  # Supported model formats for this runtime
  supportedModelFormats:
  - name: sklearn
    version: "1"
    autoSelect: true

  # Container specification for sklearn model serving
  containers:
  - name: kserve-container
    image: {{ .Values.modelServing.sklearn.image | default "kserve/sklearnserver:latest" }}
    imagePullPolicy: IfNotPresent

    # Port configuration (only HTTP port for KServe compatibility)
    ports:
    - containerPort: 8080
      name: http1
      protocol: TCP

    # Resource requests and limits
    resources:
      requests:
        memory: {{ .Values.modelServing.sklearn.resources.requests.memory | default "256Mi" }}
        cpu: {{ .Values.modelServing.sklearn.resources.requests.cpu | default "100m" }}
      limits:
        memory: {{ .Values.modelServing.sklearn.resources.limits.memory | default "512Mi" }}
        cpu: {{ .Values.modelServing.sklearn.resources.limits.cpu | default "500m" }}

    # Volume mounts for S3 credentials (needed for storage-initializer)
    volumeMounts:
    - name: s3-credentials
      mountPath: /var/run/secrets/s3
      readOnly: true

    # Environment variables
    env:
    - name: LOG_LEVEL
      value: "INFO"
    - name: STORAGE_URI
      value: "s3://model-storage/"
    # AWS credentials file location for boto3 (used by storage-initializer)
    - name: AWS_SHARED_CREDENTIALS_FILE
      value: "/var/run/secrets/s3/credentials"
    - name: AWS_CONFIG_FILE
      value: "/var/run/secrets/s3/config"
    {{- if .Values.objectStore.enabled }}
    - name: AWS_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: model-storage-config
          key: AWS_ACCESS_KEY_ID
    - name: AWS_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: model-storage-config
          key: AWS_SECRET_ACCESS_KEY
    - name: AWS_S3_ENDPOINT
      valueFrom:
        secretKeyRef:
          name: model-storage-config
          key: AWS_S3_ENDPOINT
    - name: AWS_DEFAULT_REGION
      valueFrom:
        secretKeyRef:
          name: model-storage-config
          key: AWS_DEFAULT_REGION
    - name: AWS_SSL_VERIFY
      value: "false"
    {{- end }}

  # Volume definitions for S3 credentials
  volumes:
  - name: s3-credentials
    secret:
      secretName: model-storage-config
      defaultMode: 0400

---
# KServe tensorflow ServingRuntime (in application namespace)
# Enables serving of TensorFlow models through KServe
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: tensorflow-runtime
  namespace: {{ .Values.main.namespace }}
  labels:
    app.kubernetes.io/name: kserve-tensorflow-runtime
    app.kubernetes.io/component: model-serving
    app.kubernetes.io/part-of: self-healing-platform
spec:
  # Supported model formats for this runtime
  supportedModelFormats:
  - name: tensorflow
    version: "1"
    autoSelect: true
  - name: tensorflow
    version: "2"
    autoSelect: true

  # Container specification for tensorflow model serving
  containers:
  - name: kserve-container
    image: {{ .Values.modelServing.tensorflow.image | default "kserve/tfserving:latest" }}
    imagePullPolicy: IfNotPresent

    # Port configuration (only HTTP port for KServe compatibility)
    ports:
    - containerPort: 8080
      name: http1
      protocol: TCP

    # Resource requests and limits
    resources:
      requests:
        memory: {{ .Values.modelServing.tensorflow.resources.requests.memory | default "512Mi" }}
        cpu: {{ .Values.modelServing.tensorflow.resources.requests.cpu | default "200m" }}
      limits:
        memory: {{ .Values.modelServing.tensorflow.resources.limits.memory | default "1Gi" }}
        cpu: {{ .Values.modelServing.tensorflow.resources.limits.cpu | default "1000m" }}

    # Volume mounts for S3 credentials (needed for storage-initializer)
    volumeMounts:
    - name: s3-credentials
      mountPath: /var/run/secrets/s3
      readOnly: true

    # Environment variables
    env:
    - name: LOG_LEVEL
      value: "INFO"
    - name: STORAGE_URI
      value: "s3://model-storage/"
    # AWS credentials file location for boto3 (used by storage-initializer)
    - name: AWS_SHARED_CREDENTIALS_FILE
      value: "/var/run/secrets/s3/credentials"
    - name: AWS_CONFIG_FILE
      value: "/var/run/secrets/s3/config"
    {{- if .Values.objectStore.enabled }}
    - name: AWS_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: model-storage-config
          key: AWS_ACCESS_KEY_ID
    - name: AWS_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: model-storage-config
          key: AWS_SECRET_ACCESS_KEY
    - name: AWS_S3_ENDPOINT
      valueFrom:
        secretKeyRef:
          name: model-storage-config
          key: AWS_S3_ENDPOINT
    - name: AWS_DEFAULT_REGION
      valueFrom:
        secretKeyRef:
          name: model-storage-config
          key: AWS_DEFAULT_REGION
    - name: AWS_SSL_VERIFY
      value: "false"
    {{- end }}

  # Volume definitions for S3 credentials
  volumes:
  - name: s3-credentials
    secret:
      secretName: model-storage-config
      defaultMode: 0400

---
# KServe sklearn-pvc ServingRuntime (PVC-based, recommended)
# Enables serving of scikit-learn models through KServe with PVC storage
# RECOMMENDED for new deployments - simpler, faster, no credentials needed
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: sklearn-pvc-runtime
  namespace: {{ .Values.main.namespace }}
  labels:
    app.kubernetes.io/name: kserve-sklearn-pvc-runtime
    app.kubernetes.io/component: model-serving
    app.kubernetes.io/part-of: self-healing-platform
  annotations:
    description: "PVC-based sklearn runtime - recommended for new deployments"
spec:
  # Supported model formats for this runtime
  supportedModelFormats:
  - name: sklearn
    version: "1"
    autoSelect: true  # Prefer this runtime over S3-based

  # Container specification for sklearn model serving
  containers:
  - name: kserve-container
    image: {{ .Values.modelServing.sklearn.image | default "kserve/sklearnserver:latest" }}
    imagePullPolicy: IfNotPresent

    # Explicit args for PVC-based model loading
    args:
    - --model_dir=/mnt/models
    - --http_port=8080

    # Port configuration
    ports:
    - containerPort: 8080
      name: http1
      protocol: TCP

    # Resource requests and limits
    resources:
      requests:
        memory: {{ .Values.modelServing.sklearn.resources.requests.memory | default "256Mi" }}
        cpu: {{ .Values.modelServing.sklearn.resources.requests.cpu | default "100m" }}
      limits:
        memory: {{ .Values.modelServing.sklearn.resources.limits.memory | default "512Mi" }}
        cpu: {{ .Values.modelServing.sklearn.resources.limits.cpu | default "500m" }}

    # NOTE: volumeMounts NOT needed - KServe automatically mounts from storageUri
    # The InferenceService spec uses: storageUri: "pvc://model-storage-pvc/model-name"
    # KServe will create the mount automatically at /mnt/models using the volume definition below

    # Environment variables
    env:
    - name: LOG_LEVEL
      value: "INFO"

  # Volume definition for PVC
  volumes:
  - name: model-storage
    persistentVolumeClaim:
      claimName: model-storage-pvc

{{- end }}
